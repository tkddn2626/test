import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime, timedelta
from urllib.parse import urljoin, urlparse, quote
import logging
from typing import List, Dict, Optional, Tuple, Union
import asyncio
import json
from dataclasses import dataclass, field
import concurrent.futures
from collections import Counter
import time
from functools import lru_cache
import hashlib

# aiohttp ì„í¬íŠ¸ë¥¼ try-exceptë¡œ ë³´í˜¸
try:
    import aiohttp
    AIOHTTP_AVAILABLE = True
except ImportError:
    AIOHTTP_AVAILABLE = False
    aiohttp = None
    logging.warning("aiohttp ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install aiohttpë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# ================================
# ğŸ”¥ í™•ì¥ëœ Lemmy ì„¤ì • ë° ìƒìˆ˜
# ================================

# ì•ˆì •ì ì¸ ì¸ìŠ¤í„´ìŠ¤ ìš°ì„ ìˆœìœ„
STABILITY_TIER = {
    # Tier 1: ê°€ì¥ ì•ˆì •ì  (ìš°ì„  ì‹œë„)
    'tier1': ['lemmy.ml', 'lemmy.world', 'beehaw.org'],
    # Tier 2: ì•ˆì •ì  
    'tier2': ['sh.itjust.works', 'lemm.ee', 'programming.dev'],
    # Tier 3: ë³´í†µ
    'tier3': ['lemmy.one', 'feddit.de', 'reddthat.com', 'sopuli.xyz']
}

# ğŸ“¡ ì•Œë ¤ì§„ Lemmy ì¸ìŠ¤í„´ìŠ¤ (í™•ì¥ë¨)
KNOWN_LEMMY_INSTANCES = {
    # ë©”ì´ì € ì¸ìŠ¤í„´ìŠ¤
    'lemmy.world': {'users': 50000, 'type': 'general', 'region': 'US'},
    'lemmy.ml': {'users': 30000, 'type': 'general', 'region': 'EU'},
    'beehaw.org': {'users': 12000, 'type': 'community', 'region': 'US'},
    'sh.itjust.works': {'users': 15000, 'type': 'tech', 'region': 'CA'},
    'lemmy.one': {'users': 8000, 'type': 'general', 'region': 'US'},
    
    # ì§€ì—­ë³„ ì¸ìŠ¤í„´ìŠ¤
    'feddit.de': {'users': 10000, 'type': 'regional', 'region': 'DE'},
    'feddit.uk': {'users': 5000, 'type': 'regional', 'region': 'UK'},
    'lemmy.ca': {'users': 4000, 'type': 'regional', 'region': 'CA'},
    'aussie.zone': {'users': 3000, 'type': 'regional', 'region': 'AU'},
    'feddit.it': {'users': 2000, 'type': 'regional', 'region': 'IT'},
    'feddit.nl': {'users': 2500, 'type': 'regional', 'region': 'NL'},
    
    # íŠ¹í™” ì¸ìŠ¤í„´ìŠ¤
    'programming.dev': {'users': 8000, 'type': 'tech', 'region': 'US'},
    'discuss.tchncs.de': {'users': 6000, 'type': 'tech', 'region': 'DE'},
    'sopuli.xyz': {'users': 4000, 'type': 'general', 'region': 'FI'},
    'lemm.ee': {'users': 7000, 'type': 'general', 'region': 'EE'},
    'slrpnk.net': {'users': 3000, 'type': 'climate', 'region': 'US'},
    'lemmy.zip': {'users': 2000, 'type': 'general', 'region': 'US'},
    'midwest.social': {'users': 1500, 'type': 'regional', 'region': 'US'},
    'reddthat.com': {'users': 5000, 'type': 'general', 'region': 'CA'},
    'lemmy.dbzer0.com': {'users': 4000, 'type': 'piracy', 'region': 'US'},
    'startrek.website': {'users': 2000, 'type': 'fandom', 'region': 'US'},
    'lemmy.blahaj.zone': {'users': 3000, 'type': 'lgbtq', 'region': 'US'},
    'hexbear.net': {'users': 8000, 'type': 'leftist', 'region': 'US'},
    'lemmygrad.ml': {'users': 4000, 'type': 'leftist', 'region': 'ML'},
    'exploding-heads.com': {'users': 1000, 'type': 'rightist', 'region': 'US'},
    'lemmy.fmhy.ml': {'users': 3000, 'type': 'piracy', 'region': 'ML'},
    'burggit.moe': {'users': 1000, 'type': 'anime', 'region': 'US'},
    'ani.social': {'users': 2000, 'type': 'anime', 'region': 'US'},
    'ttrpg.network': {'users': 1500, 'type': 'gaming', 'region': 'US'},
    'pathfinder.social': {'users': 800, 'type': 'gaming', 'region': 'US'},
    'lemmy.studio': {'users': 1200, 'type': 'creative', 'region': 'US'},
    'mander.xyz': {'users': 2000, 'type': 'science', 'region': 'US'},
    'lemmy.eco.br': {'users': 1000, 'type': 'regional', 'region': 'BR'},
    'jlai.lu': {'users': 1500, 'type': 'regional', 'region': 'FR'},
    'lemmy.pt': {'users': 800, 'type': 'regional', 'region': 'PT'}
}

# ================================
# ğŸ”¥ ê¸°íšì„œ ê¸°ë°˜ ë„ë©”ì¸ íŒ¨í„´ ì¶”ê°€
# ================================

LEMMY_DOMAIN_PATTERNS = [
    # 1) lemmy.*
    r'^(?:[\w-]+\.)?lemmy\.[a-z]{2,}$',
    
    # 2) feddit.*
    r'^(?:[\w-]+\.)?feddit\.[a-z]{2,}$',
    
    # 3) beehaw.org
    r'^beehaw\.org$',
    
    # 4) social/zone/network TLD
    r'^[\w-]+\.(?:social|zone|network)$',
    
    # 5) dev/xyz/net/org TLD
    r'^[\w-]+\.(?:dev|xyz|net|org)$',
    
    # 6) êµ­ê°€ë³„ TLD
    r'^[\w-]+\.(?:ca|de|it|nl|uk|pt|br|lu|ee|cc|my)$',
    
    # 7) ê¸°íƒ€ ê³ ì • ë„ë©”ì¸
    r'^(?:sh\.itjust\.works|lemm\.ee|sopuli\.xyz)$',
    
    # 8) .onion í˜ë”ë ˆì´ì…˜
    r'^(?:[\w-]+)\.onion$',

]

# ì»´íŒŒì¼ëœ ì •ê·œí‘œí˜„ì‹ ìºì‹± (ì„±ëŠ¥ ìµœì í™”)
_compiled_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in LEMMY_DOMAIN_PATTERNS]

# ğŸ¯ Lemmy íŠ¹í™” ì„¤ì • (í–¥ìƒë¨)
LEMMY_CONFIG = {
    'api_timeout': 12, 
    'rss_timeout': 8, 
    'html_timeout': 15,
    'max_pages': 8,    
    'max_concurrent': 3,
    'retry_count': 2, 
    'retry_delay': 1.5, 
    'rate_limit_delay': 1.0,
    'user_agent': 'LemmyCrawler/2.0 (Enhanced Community Crawler)',
    'cache_ttl': 240,
}

# ğŸ”¥ Lemmy API ì—”ë“œí¬ì¸íŠ¸ ë§¤í•‘
LEMMY_API_ENDPOINTS = {
    'posts': '/api/v3/post/list',
    'communities': '/api/v3/community/list',
    'search': '/api/v3/search',
    'site': '/api/v3/site',
    'nodeinfo': '/.well-known/nodeinfo',
    'feeds': {
        'all': '/feeds/all.xml',
        'local': '/feeds/local.xml',
        'community': '/feeds/c/{community}.xml'
    }
}

# ğŸš€ Lemmy ì‹œê°„ í•„í„° ë§¤í•‘ ì¶”ê°€
LEMMY_TIME_FILTER_MAPPING = {
    'hour': 'TopHour',
    'day': 'TopDay',
    'week': 'TopWeek',
    'month': 'TopMonth',
    'year': 'TopYear',
    'all': 'TopAll'
}

# ğŸ¨ Lemmy ì •ë ¬ ë°©ì‹ ë§¤í•‘ (í™•ì¥ë¨)
LEMMY_SORT_MAPPING = {
    'hot': 'Hot',
    'popular': 'TopAll',
    'top': 'TopAll',
    'topall': 'TopAll',
    'topday': 'TopDay',
    'topweek': 'TopWeek',
    'topmonth': 'TopMonth',
    'topyear': 'TopYear',
    'new': 'New',
    'recent': 'New',
    'active': 'Active',
    'comments': 'MostComments',
    'mostcomments': 'MostComments',
    'newcomments': 'NewComments',
    'old': 'Old',
    'scaled': 'Scaled',
    'controversial': 'Controversial'
}

# ğŸ“Š ì¸ê¸° ì»¤ë®¤ë‹ˆí‹° í…œí”Œë¦¿
POPULAR_COMMUNITIES = {
    'technology': ['technology', 'tech', 'programming', 'linux', 'opensource'],
    'news': ['worldnews', 'news', 'globalnews', 'politics'],
    'gaming': ['gaming', 'games', 'pcgaming', 'nintendo', 'playstation'],
    'science': ['science', 'askscience', 'space', 'physics'],
    'entertainment': ['movies', 'television', 'music', 'books'],
    'lifestyle': ['cooking', 'food', 'fitness', 'travel'],
    'creative': ['art', 'photography', 'writing', 'diy'],
    'social': ['asklemmy', 'casualconversation', 'showerthoughts'],
    'memes': ['memes', 'dankmemes', 'funny'],
    'learning': ['todayilearned', 'explainlikeimfive', 'lifeprotips']
}

# ================================
# ğŸ”¥ í–¥ìƒëœ ë°ì´í„° í´ë˜ìŠ¤
# ================================

@dataclass
class LemmyInstance:
    """Lemmy ì¸ìŠ¤í„´ìŠ¤ ì •ë³´ (í™•ì¥ë¨)"""
    domain: str
    name: str
    description: str = ""
    users: int = 0
    communities: int = 0
    posts: int = 0
    comments: int = 0
    version: str = ""
    is_nsfw: bool = False
    registration_open: bool = True
    federation_enabled: bool = True
    region: str = ""
    type: str = "general"  # general, tech, regional, etc.
    status: str = "online"  # online, offline, slow
    response_time: float = 0.0
    last_checked: datetime = field(default_factory=datetime.now)
    

@dataclass
class LemmyCommunity:
    """Lemmy ì»¤ë®¤ë‹ˆí‹° ì •ë³´ (í™•ì¥ë¨)"""
    name: str
    display_name: str
    description: str
    subscribers: int = 0
    posts: int = 0
    comments: int = 0
    url: str = ""
    instance: str = ""
    nsfw: bool = False
    removed: bool = False
    posting_restricted: bool = False
    icon: str = ""
    banner: str = ""
    created: datetime = field(default_factory=datetime.now)
    last_active: datetime = field(default_factory=datetime.now)

@dataclass
class LemmyPost:
    """Lemmy ê²Œì‹œë¬¼ ì •ë³´"""
    id: int
    title: str
    url: str = ""
    body: str = ""
    score: int = 0
    upvotes: int = 0
    downvotes: int = 0
    comments: int = 0
    author: str = ""
    community: str = ""
    instance: str = ""
    published: datetime = field(default_factory=datetime.now)
    updated: datetime = field(default_factory=datetime.now)
    nsfw: bool = False
    locked: bool = False
    stickied: bool = False
    featured: bool = False
    thumbnail_url: str = ""
    embed_title: str = ""
    embed_description: str = ""

# ================================
# ğŸ”¥ ê³ ê¸‰ ìºì‹± ì‹œìŠ¤í…œ
# ================================

class LemmyCache:
    """Lemmy ì „ìš© ìºì‹± ì‹œìŠ¤í…œ"""
    
    def __init__(self, ttl: int = 300):
        self.cache = {}
        self.ttl = ttl
        self.failed_instances = set()  # ì‹¤íŒ¨í•œ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì 
        self.instance_success_time = {}  # ë§ˆì§€ë§‰ ì„±ê³µ ì‹œê°„
    
    def _generate_key(self, *args, **kwargs) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_data = str(args) + str(sorted(kwargs.items()))
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def is_instance_reliable(self, domain: str) -> bool:
        """ì¸ìŠ¤í„´ìŠ¤ ì‹ ë¢°ì„± í™•ì¸ (ìºì‹œ ê¸°ë°˜)"""
        if domain in self.failed_instances:
            last_success = self.instance_success_time.get(domain)
            if last_success and time.time() - last_success < self.ttl:
                return False
        return True

    def mark_instance_failed(self, domain: str):
        """ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‹¤íŒ¨ë¡œ ë§ˆí‚¹ (ìºì‹œ ê¸°ë°˜)"""
        self.failed_instances.add(domain)
        logger.warning(f"[Cache] ì¸ìŠ¤í„´ìŠ¤ ì‹¤íŒ¨ë¡œ ë§ˆí‚¹: {domain}")

    def mark_instance_success(self, domain: str):
        """ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì„±ê³µìœ¼ë¡œ ë³µêµ¬ (ìºì‹œ ê¸°ë°˜)"""
        self.failed_instances.discard(domain)
        self.instance_success_time[domain] = time.time()
        logger.info(f"[Cache] ì¸ìŠ¤í„´ìŠ¤ ì„±ê³µìœ¼ë¡œ ë³µêµ¬: {domain}")

    def get(self, *args, **kwargs) -> Optional[any]:
        """ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        key = self._generate_key(*args, **kwargs)
        if key in self.cache:
            data, timestamp = self.cache[key]
            if time.time() - timestamp < self.ttl:
                return data
            else:
                del self.cache[key]
        return None
    
    def set(self, data: any, *args, **kwargs):
        """ìºì‹œì— ë°ì´í„° ì €ì¥"""
        key = self._generate_key(*args, **kwargs)
        self.cache[key] = (data, time.time())
    
    def clear(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self.cache.clear()

# ================================
# ğŸ”¥ í–¥ìƒëœ ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬ì
# ================================

class EnhancedLemmyInstanceManager:
    """í–¥ìƒëœ Lemmy ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.instances = {}
        self.cache = LemmyCache(ttl=600)  # 10ë¶„ ìºì‹œ
        self.session = None
        if AIOHTTP_AVAILABLE:
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(
                    total=LEMMY_CONFIG['api_timeout'],
                    connect=3,  # ğŸ”¥ ì¶”ê°€: ì—°ê²° íƒ€ì„ì•„ì›ƒ
                    sock_read=5  # ğŸ”¥ ì¶”ê°€: ì½ê¸° íƒ€ì„ì•„ì›ƒ
                ),
                headers={'User-Agent': LEMMY_CONFIG['user_agent']},
                connector=aiohttp.TCPConnector(
                    limit=10,           # ğŸ”¥ ì¶”ê°€: ì „ì²´ ì—°ê²° ìˆ˜ ì œí•œ
                    limit_per_host=5    # ğŸ”¥ ì¶”ê°€: í˜¸ìŠ¤íŠ¸ë‹¹ ì—°ê²° ìˆ˜ ì œí•œ
                )
            )
    
    async def get_best_instance_for_community(self, community_name: str) -> str:
        """ì»¤ë®¤ë‹ˆí‹°ì— ê°€ì¥ ì í•©í•œ ì•ˆì •ì ì¸ ì¸ìŠ¤í„´ìŠ¤ ì„ íƒ"""
        # 1. ì»¤ë®¤ë‹ˆí‹°ë³„ ê¶Œì¥ ì¸ìŠ¤í„´ìŠ¤ í™•ì¸
        community_preferences = {
            'asklemmy': ['lemmy.ml', 'lemmy.world'],
            'technology': ['lemmy.world', 'programming.dev', 'lemmy.ml'],
            'programming': ['programming.dev', 'lemmy.ml', 'lemmy.world'],
            'worldnews': ['lemmy.ml', 'lemmy.world'],
            'linux': ['lemmy.ml', 'programming.dev'],
            'privacy': ['lemmy.ml', 'beehaw.org'],
        }
        
        if community_name.lower() in community_preferences:
            for instance in community_preferences[community_name.lower()]:
                if self.cache.is_instance_reliable(instance):
                    if await self._quick_health_check(instance):
                        return instance
        
        # 2. Tierë³„ë¡œ ì•ˆì •ì ì¸ ì¸ìŠ¤í„´ìŠ¤ ì‹œë„
        for tier in ['tier1', 'tier2', 'tier3']:
            for instance in STABILITY_TIER[tier]:
                if self.cache.is_instance_reliable(instance):
                    if await self._quick_health_check(instance):
                        return instance
        
        # 3. ê¸°ë³¸ê°’
        return 'lemmy.ml'
    
    async def _quick_health_check(self, domain: str) -> bool:
        """ë¹ ë¥¸ í—¬ìŠ¤ì²´í¬ (5ì´ˆ ì´ë‚´)"""
        if not AIOHTTP_AVAILABLE:
            return True  # aiohttp ì—†ìœ¼ë©´ ê¸°ë³¸ì ìœ¼ë¡œ í†µê³¼
            
        try:
            # ğŸ”¥ íƒ€ì„ì•„ì›ƒ ë‹¨ì¶• ë° ì¬ì‹œë„ ë¡œì§
            timeout = aiohttp.ClientTimeout(total=5, connect=2)
            
            async with aiohttp.ClientSession(timeout=timeout) as session:
                url = f"https://{domain}/api/v3/site"
                
                try:
                    async with session.get(url) as response:
                        is_healthy = response.status in [200, 301, 302]  # ë¦¬ë‹¤ì´ë ‰íŠ¸ë„ OK
                        
                        if is_healthy:
                            self.cache.mark_instance_success(domain)
                            return True
                except asyncio.TimeoutError:
                    logger.debug(f"í—¬ìŠ¤ì²´í¬ íƒ€ì„ì•„ì›ƒ: {domain}")
                except Exception as e:
                    logger.debug(f"í—¬ìŠ¤ì²´í¬ ì˜¤ë¥˜ {domain}: {e}")
            
            # ğŸ”¥ ì‹¤íŒ¨í•´ë„ ì™„ì „íˆ ì°¨ë‹¨í•˜ì§€ ì•Šê³  ê²½ê³ ë§Œ
            logger.warning(f"ì¸ìŠ¤í„´ìŠ¤ í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨í•˜ì§€ë§Œ í¬ë¡¤ë§ ì‹œë„: {domain}")
            return True  # â† ì—¬ê¸°ë¥¼ Trueë¡œ ë³€ê²½ (ê¸°ì¡´: False)
            
        except Exception as e:
            logger.debug(f"í—¬ìŠ¤ì²´í¬ ì˜ˆì™¸ {domain}: {e}")
            return True  # ğŸ”¥ ì˜ˆì™¸ ë°œìƒí•´ë„ ì‹œë„
    
    async def get_instance_info(self, domain: str) -> Optional[LemmyInstance]:
        """í–¥ìƒëœ ì¸ìŠ¤í„´ìŠ¤ ì •ë³´ ìˆ˜ì§‘"""
        if not AIOHTTP_AVAILABLE:
            # aiohttp ì—†ìœ¼ë©´ ê¸°ë³¸ ì •ë³´ ë°˜í™˜
            known_info = KNOWN_LEMMY_INSTANCES.get(domain, {})
            return LemmyInstance(
                domain=domain,
                name=known_info.get('name', domain),
                users=known_info.get('users', 0),
                region=known_info.get('region', ''),
                type=known_info.get('type', 'general')
            )
            
        # ì‹ ë¢°í•  ìˆ˜ ì—†ëŠ” ì¸ìŠ¤í„´ìŠ¤ëŠ” ê±´ë„ˆë›°ê¸°
        if not self.cache.is_instance_reliable(domain):
            logger.debug(f"ì‹ ë¢°í•  ìˆ˜ ì—†ëŠ” ì¸ìŠ¤í„´ìŠ¤ ê±´ë„ˆë›°ê¸°: {domain}")
            return None
        
        # ìºì‹œ í™•ì¸
        cached = self.cache.get('instance', domain)
        if cached:
            return cached
        
        try:
            start_time = time.time()
            
            # ğŸ”¥ ë” ì§§ì€ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì‹œë„
            instance = await self._get_site_info(domain)
            
            if instance:
                instance.response_time = time.time() - start_time
                instance.last_checked = datetime.now()
                
                # ìºì‹œì— ì €ì¥
                self.cache.set(instance, 'instance', domain)
                self.cache.mark_instance_success(domain)
                self.instances[domain] = instance
                
                return instance
            else:
                self.cache.mark_instance_failed(domain)
            
        except Exception as e:
            logger.debug(f"ì¸ìŠ¤í„´ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨ ({domain}): {e}")
            self.cache.mark_instance_failed(domain)
        
        return None
    
    async def _get_site_info(self, domain: str) -> Optional[LemmyInstance]:
        """Lemmy Site APIë¡œ ì •ë³´ ìˆ˜ì§‘"""
        if not self.session:
            return None
            
        try:
            url = f"https://{domain}/api/v3/site"
            async with self.session.get(url) as response:
                if response.status == 200:
                    data = await response.json()
                    return self._parse_site_info(domain, data)
        except Exception as e:
            logger.debug(f"Site API ì¡°íšŒ ì‹¤íŒ¨ ({domain}): {e}")
        
        return None
    
    def _parse_site_info(self, domain: str, data: Dict) -> LemmyInstance:
        """Site API ë°ì´í„° íŒŒì‹±"""
        site_view = data.get('site_view', {})
        site = site_view.get('site', {})
        counts = site_view.get('counts', {})
        
        known_info = KNOWN_LEMMY_INSTANCES.get(domain, {})
        
        return LemmyInstance(
            domain=domain,
            name=site.get('name', domain),
            description=site.get('description', ''),
            users=counts.get('users', 0),
            communities=counts.get('communities', 0),
            posts=counts.get('posts', 0),
            comments=counts.get('comments', 0),
            version=data.get('version', ''),
            is_nsfw=site.get('content_warning', False),
            region=known_info.get('region', ''),
            type=known_info.get('type', 'general')
        )
    
    def is_lemmy_instance(self, domain: str) -> bool:
        """ë„ë©”ì¸ì´ Lemmy ì¸ìŠ¤í„´ìŠ¤ì¸ì§€ í™•ì¸ (ê¸°íšì„œ íŒ¨í„´ ì ìš©)"""
        domain = domain.lower().strip()
        
        # 1. ì•Œë ¤ì§„ ì¸ìŠ¤í„´ìŠ¤ í™•ì¸ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        if domain in KNOWN_LEMMY_INSTANCES:
            return True
        
        # 2. ê¸°íšì„œ ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™•ì¸ (ìƒˆë¡œ ì¶”ê°€)
        for pattern in _compiled_patterns:
            if pattern.match(domain):
                logger.debug(f"Lemmy íŒ¨í„´ ë§¤ì¹­: {domain}")
                return True
        
        return False
    
    async def __aenter__(self):
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

class LemmyConditionChecker:
    """Lemmy ì „ìš© ì¡°ê±´ ê²€ì‚¬ê¸°"""
    
    def __init__(self, min_views: int = 0, min_likes: int = 0, 
                start_date: str = None, end_date: str = None):
        self.min_views = min_views
        self.min_likes = min_likes
        self.start_dt = self._parse_date(start_date)
        self.end_dt = self._parse_date(end_date)
        if self.end_dt:
            self.end_dt = self.end_dt.replace(hour=23, minute=59, second=59)
    
    def _parse_date(self, date_str: Optional[str]) -> Optional[datetime]:
        if not date_str:
            return None
        try:
            return datetime.strptime(date_str, '%Y-%m-%d')
        except Exception:
            return None
    
    def check_conditions(self, post: Dict) -> bool:
        """ê²Œì‹œë¬¼ ì¡°ê±´ ê²€ì‚¬"""
        # ë©”íŠ¸ë¦­ ê²€ì‚¬
        if post.get('ì¡°íšŒìˆ˜', 0) < self.min_views:
            return False
        if post.get('ì¶”ì²œìˆ˜', 0) < self.min_likes:
            return False
        
        # ë‚ ì§œ ê²€ì‚¬
        if self.start_dt and self.end_dt:
            post_date = self._extract_post_date(post)
            if post_date:
                if not (self.start_dt <= post_date <= self.end_dt):
                    return False
        
        return True
    
    def _extract_post_date(self, post: Dict) -> Optional[datetime]:
        date_str = post.get('ì‘ì„±ì¼', '')
        if not date_str:
            return None
        
        formats = ['%Y.%m.%d %H:%M', '%Y-%m-%d %H:%M', '%Y.%m.%d', '%Y-%m-%d']
        for fmt in formats:
            try:
                return datetime.strptime(date_str.strip(), fmt)
            except ValueError:
                continue
        return None

# ================================
# ğŸ”¥ ê³ ê¸‰ ì»¤ë®¤ë‹ˆí‹° ê²€ìƒ‰ê¸°
# ================================

class AdvancedLemmyCommunitySearcher:
    """ê³ ê¸‰ Lemmy ì»¤ë®¤ë‹ˆí‹° ê²€ìƒ‰ê¸°"""
    
    def __init__(self, instance_manager: EnhancedLemmyInstanceManager):
        self.instance_manager = instance_manager
        self.cache = LemmyCache(ttl=900)  # 15ë¶„ ìºì‹œ
        self.popular_cache = {}
    
    def resolve_community_url(self, community_input: str) -> Tuple[str, str, str]:
        """ì»¤ë®¤ë‹ˆí‹° ì…ë ¥ì„ URL, ì»¤ë®¤ë‹ˆí‹°ëª…, ì¸ìŠ¤í„´ìŠ¤ë¡œ ë¶„í•´ (ì™„ì „ ê°œì„ ë¨)"""
        community_input = community_input.strip()
        
        # 1. ì´ë¯¸ URLì¸ ê²½ìš° (ì™„ì „íˆ ê°œì„ ëœ ë¡œì§)
        if community_input.startswith('http'):
            parsed = urlparse(community_input)
            instance = parsed.netloc
            
            # Lemmy ì¸ìŠ¤í„´ìŠ¤ ê²€ì¦
            if not self.instance_manager.is_lemmy_instance(instance):
                logger.warning(f"Lemmy ì¸ìŠ¤í„´ìŠ¤ê°€ ì•„ë‹ ìˆ˜ ìˆìŒ: {instance}")
            
            # ğŸ”¥ ëª¨ë“  URL íŒ¨í„´ ê°ì§€ ë¡œì§
            path = parsed.path.strip('/')
            
            # íŒ¨í„´ 1: /c/ì»¤ë®¤ë‹ˆí‹° í˜•íƒœ
            if '/c/' in parsed.path:
                community_name = parsed.path.split('/c/')[1].split('/')[0]
                return community_input, community_name, instance
            
            # íŒ¨í„´ 2: /post/ë²ˆí˜¸ í˜•íƒœ - ê²Œì‹œë¬¼ URLì—ì„œ ì»¤ë®¤ë‹ˆí‹° ì¶”ì¶œ ì‹œë„
            elif '/post/' in parsed.path:
                # ê²Œì‹œë¬¼ URLì´ì§€ë§Œ ì¸ìŠ¤í„´ìŠ¤ëŠ” í™•ì •ì ì´ë¯€ë¡œ í¬ë¡¤ë§ ê°€ëŠ¥
                # ì¼ë°˜ì ìœ¼ë¡œ ì¸ìŠ¤í„´ìŠ¤ì˜ ì „ì²´ í”¼ë“œë¥¼ ê°€ì ¸ì˜´
                return community_input, "all", instance
            
            # íŒ¨í„´ 3: /comment/ë²ˆí˜¸ í˜•íƒœ - ëŒ“ê¸€ URL
            elif '/comment/' in parsed.path:
                # ëŒ“ê¸€ URLì´ì§€ë§Œ ì¸ìŠ¤í„´ìŠ¤ëŠ” í™•ì •ì 
                return community_input, "all", instance
            
            # íŒ¨í„´ 4: /u/ì‚¬ìš©ì í˜•íƒœ - ì‚¬ìš©ì í”„ë¡œí•„
            elif '/u/' in parsed.path:
                username = parsed.path.split('/u/')[1].split('/')[0]
                # ì‚¬ìš©ìë³„ ê²Œì‹œë¬¼ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ (êµ¬í˜„ ê°€ëŠ¥)
                return community_input, f"user:{username}", instance
            
            # íŒ¨í„´ 5: /communities í˜•íƒœ - ì»¤ë®¤ë‹ˆí‹° ëª©ë¡
            elif path in ['communities', 'communities/']:
                return community_input, "communities", instance
            
            # íŒ¨í„´ 6: ë£¨íŠ¸ ë„ë©”ì¸ (https://instance.com/ ë˜ëŠ” https://instance.com)
            elif not path or path == '/':
                # ì¸ìŠ¤í„´ìŠ¤ ë©”ì¸ í˜ì´ì§€ - ì „ì²´ í”¼ë“œ í¬ë¡¤ë§
                return community_input, "local", instance
            
            # íŒ¨í„´ 7: ê¸°íƒ€ ì•Œ ìˆ˜ ì—†ëŠ” ê²½ë¡œ
            else:
                # ê²½ë¡œê°€ ìˆì§€ë§Œ ì•Œ ìˆ˜ ì—†ëŠ” í˜•íƒœ - ì‹œë„í•´ë³¼ ê°€ì¹˜ê°€ ìˆìŒ
                logger.info(f"ì•Œ ìˆ˜ ì—†ëŠ” Lemmy URL íŒ¨í„´ì´ì§€ë§Œ ì‹œë„: {community_input}")
                return community_input, "unknown", instance
        
        # 2. !community@instance í˜•íƒœ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        if community_input.startswith('!') and '@' in community_input:
            parts = community_input[1:].split('@')
            if len(parts) == 2:
                community_name, instance = parts
                if self.instance_manager.is_lemmy_instance(instance):
                    url = f"https://{instance}/c/{community_name}"
                    return url, community_name, instance
                else:
                    logger.warning(f"Lemmy ì¸ìŠ¤í„´ìŠ¤ê°€ ì•„ë‹ ìˆ˜ ìˆìŒ: {instance}")
        
        # 3. community@instance í˜•íƒœ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        if '@' in community_input and not community_input.startswith('http'):
            parts = community_input.split('@')
            if len(parts) == 2:
                community_name, instance = parts
                if self.instance_manager.is_lemmy_instance(instance):
                    url = f"https://{instance}/c/{community_name}"
                    return url, community_name, instance
                else:
                    logger.info(f"ì•Œë ¤ì§€ì§€ ì•Šì€ ë„ë©”ì¸ì´ì§€ë§Œ ì‹œë„: {instance}")
                    url = f"https://{instance}/c/{community_name}"
                    return url, community_name, instance
        
        # 4. ì»¤ë®¤ë‹ˆí‹° ì´ë¦„ë§Œ ìˆëŠ” ê²½ìš° (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        if '/' not in community_input and '.' not in community_input:
            community_name = community_input
            instance = "lemmy.world"
            url = f"https://{instance}/c/{community_name}"
            return url, community_name, instance
        
        # 5. ë„ë©”ì¸ë§Œ ì…ë ¥ëœ ê²½ìš° (ê°œì„ ë¨)
        if '.' in community_input and '/' not in community_input and '@' not in community_input:
            if self.instance_manager.is_lemmy_instance(community_input):
                instance = community_input
                url = f"https://{instance}"
                return url, "local", instance  # ğŸ”¥ "local"ë¡œ ëª…í™•íˆ ì§€ì •
        
        # ê¸°ë³¸ê°’
        return community_input, "", ""
# ================================
# ğŸ”¥ í–¥ìƒëœ Lemmy í¬ë¡¤ëŸ¬
# ================================

class AdvancedLemmyCrawler:
    """í–¥ìƒëœ Lemmy ì „ìš© í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.instance_manager = EnhancedLemmyInstanceManager()
        self.community_searcher = AdvancedLemmyCommunitySearcher(self.instance_manager)
        self.cache = LemmyCache(ttl=LEMMY_CONFIG['cache_ttl'])
        self.rate_limiter = {}  # ì¸ìŠ¤í„´ìŠ¤ë³„ ë ˆì´íŠ¸ ë¦¬ë¯¸í„°
    
    async def crawl_lemmy_community(self, community_url: str, limit: int = 50, 
                                sort: str = "Hot", min_views: int = 0, 
                                min_likes: int = 0, start_date: str = None, 
                                end_date: str = None, websocket=None, 
                                enforce_date_limit: bool = False,
                                time_filter: str = "day") -> List[Dict]:
        """í–¥ìƒëœ ì—ëŸ¬ ì²˜ë¦¬ê°€ í¬í•¨ëœ Lemmy í¬ë¡¤ë§"""
        start_time = time.time()
        
        try:
            logger.info(f"í–¥ìƒëœ Lemmy í¬ë¡¤ë§ ì‹œì‘: {community_url}")
            
            # ğŸ”¥ ì…ë ¥ ê²€ì¦ ê°•í™”
            if not community_url or len(community_url.strip()) < 2:
                raise Exception("ì˜¬ë°”ë¥¸ Lemmy ì»¤ë®¤ë‹ˆí‹°ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”. ì˜ˆ: technology@lemmy.world")
            
            # URL íŒŒì‹± ë° ì •ê·œí™”
            url, community_name, instance = self.community_searcher.resolve_community_url(community_url)
            
            if not community_name or not instance:
                # ìë™ ë³´ì • ì‹œë„
                if '@' not in community_url and '.' not in community_url:
                    corrected_url = f"{community_url}@lemmy.world"
                    url, community_name, instance = self.community_searcher.resolve_community_url(corrected_url)
                else:
                    raise Exception(f"ì˜¬ë°”ë¥´ì§€ ì•Šì€ Lemmy ì»¤ë®¤ë‹ˆí‹° í˜•ì‹ì…ë‹ˆë‹¤: {community_url}\nì˜ˆì‹œ: technology@lemmy.world")
            
            # ğŸ”¥ ë” ìœ ì—°í•œ ì¸ìŠ¤í„´ìŠ¤ ìƒíƒœ í™•ì¸
            instance_info = await self.instance_manager.get_instance_info(instance)
            if not instance_info:
                logger.warning(f"ì¸ìŠ¤í„´ìŠ¤ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì§€ë§Œ í¬ë¡¤ë§ì„ ê³„ì† ì‹œë„í•©ë‹ˆë‹¤: {instance}")
                instance_info = type('obj', (object,), {
                    'name': instance,
                    'domain': instance,
                    'users': 0,
                    'status': 'unknown',
                    'region': '',
                    'type': 'general'
                })()
            
            await self._apply_rate_limit(instance)
            
            # ğŸ”¥ ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì‹œë„í•˜ëŠ” fallback ì²´ì¸
            posts = []
            attempted_methods = []
            
            try:
                # ë°©ë²• 1: í–¥ìƒëœ API í¬ë¡¤ë§
                posts = await self._try_api_crawling_with_fallback(
                    url, community_name, instance, limit, sort, time_filter, websocket
                )
                attempted_methods.append("API")
                
            except Exception as api_error:
                logger.debug(f"API í¬ë¡¤ë§ ì‹¤íŒ¨: {api_error}")
                attempted_methods.append("API(ì‹¤íŒ¨)")
            
            if not posts:
                try:
                    # ë°©ë²• 2: RSS í¬ë¡¤ë§
                    posts = await self._crawl_via_enhanced_rss(url, community_name, instance, limit)
                    attempted_methods.append("RSS")
                    
                except Exception as rss_error:
                    logger.debug(f"RSS í¬ë¡¤ë§ ì‹¤íŒ¨: {rss_error}")
                    attempted_methods.append("RSS(ì‹¤íŒ¨)")
            
            if not posts:
                try:
                    # ë°©ë²• 3: HTML í¬ë¡¤ë§
                    posts = await self._crawl_via_enhanced_html(url, instance, limit)
                    attempted_methods.append("HTML")
                    
                except Exception as html_error:
                    logger.debug(f"HTML í¬ë¡¤ë§ ì‹¤íŒ¨: {html_error}")
                    attempted_methods.append("HTML(ì‹¤íŒ¨)")
            
            # ğŸ”¥ ë” ìì„¸í•œ ì—ëŸ¬ ë©”ì‹œì§€ ì œê³µ
            if not posts:
                error_details = f"""
Lemmy ì»¤ë®¤ë‹ˆí‹° '{community_name}@{instance}'ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ì‹œë„í•œ ë°©ë²•ë“¤: {', '.join(attempted_methods)}

ê°€ëŠ¥í•œ í•´ê²°ì±…:
1. ì»¤ë®¤ë‹ˆí‹° ì´ë¦„ í™•ì¸:
â€¢ technology@lemmy.world âœ“
â€¢ asklemmy@lemmy.ml âœ“
â€¢ programming@programming.dev âœ“

2. ë‹¤ë¥¸ ì¸ìŠ¤í„´ìŠ¤ ì‹œë„:
â€¢ {community_name}@lemmy.ml
â€¢ {community_name}@beehaw.org
â€¢ {community_name}@sh.itjust.works

3. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„ (ì¸ìŠ¤í„´ìŠ¤ê°€ ì¼ì‹œì ìœ¼ë¡œ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ)

4. ì „ì²´ URL í˜•íƒœë¡œ ì‹œë„:
â€¢ https://{instance}/c/{community_name}
                """
                
                raise Exception(error_details.strip())
            
            # í•„í„°ë§ ì ìš©
            if min_views > 0 or min_likes > 0:
                posts = await self._apply_metric_filters(posts, min_views, min_likes)
            
            if start_date and end_date:
                posts = await self._apply_exact_date_filter(posts, start_date, end_date)
            
            # ë©”íƒ€ë°ì´í„° ë³´ê°•
            enhanced_posts = await self._enhance_post_metadata(posts, instance_info)
            
            logger.info(f"Lemmy í¬ë¡¤ë§ ì™„ë£Œ: {len(enhanced_posts)}ê°œ ê²Œì‹œë¬¼")
            return enhanced_posts
            
        except Exception as e:
            logger.error(f"Lemmy í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            raise

    async def _try_api_crawling_with_fallback(self, url: str, community_name: str, 
                                            instance: str, limit: int, sort: str,
                                            time_filter: str, websocket=None) -> List[Dict]:
        """ì—¬ëŸ¬ API ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‹œë„í•˜ëŠ” fallback í¬ë¡¤ë§"""
        
        if not AIOHTTP_AVAILABLE or not self.instance_manager.session:
            return []
        
        # ğŸ”¥ ì—¬ëŸ¬ API ì—”ë“œí¬ì¸íŠ¸ ì‹œë„
        api_attempts = [
            # 1. ì»¤ë®¤ë‹ˆí‹°ë³„ ê²Œì‹œë¬¼ ì¡°íšŒ
            {
                'url': f"https://{instance}/api/v3/post/list",
                'params': {
                    'limit': min(limit, 50),
                    'sort': LEMMY_SORT_MAPPING.get(sort.lower(), sort),
                    'type_': 'All',
                    'community_name': community_name
                }
            },
            # 2. ì „ì²´ ê²Œì‹œë¬¼ì—ì„œ ì»¤ë®¤ë‹ˆí‹° í•„í„°ë§
            {
                'url': f"https://{instance}/api/v3/post/list",
                'params': {
                    'limit': min(limit * 2, 100),
                    'sort': LEMMY_SORT_MAPPING.get(sort.lower(), sort),
                    'type_': 'Local'
                }
            },
            # 3. ê²€ìƒ‰ API ì‚¬ìš©
            {
                'url': f"https://{instance}/api/v3/search",
                'params': {
                    'q': community_name,
                    'type_': 'Posts',
                    'sort': LEMMY_SORT_MAPPING.get(sort.lower(), sort),
                    'limit': min(limit, 20)
                }
            }
        ]
        
        for attempt_idx, attempt in enumerate(api_attempts):
            try:
                async with self.instance_manager.session.get(
                    attempt['url'], 
                    params=attempt['params'],
                    timeout=aiohttp.ClientTimeout(total=20)  # ë” ê¸´ íƒ€ì„ì•„ì›ƒ
                ) as response:
                    
                    if response.status == 200:
                        data = await response.json()
                        
                        if attempt['url'].endswith('/search'):
                            # ê²€ìƒ‰ API ì‘ë‹µ ì²˜ë¦¬
                            posts = self._parse_search_api_response(data, instance, community_name)
                        else:
                            # ì¼ë°˜ API ì‘ë‹µ ì²˜ë¦¬
                            posts = self._parse_enhanced_api_response(data, instance)
                            
                            # ì „ì²´ ê²Œì‹œë¬¼ì—ì„œ ì»¤ë®¤ë‹ˆí‹° í•„í„°ë§
                            if attempt_idx == 1:  # ë‘ ë²ˆì§¸ ì‹œë„ì¸ ê²½ìš°
                                posts = [p for p in posts if community_name.lower() in p.get('ì»¤ë®¤ë‹ˆí‹°', '').lower()]
                        
                        if posts:
                            logger.info(f"API ì‹œë„ {attempt_idx + 1} ì„±ê³µ: {len(posts)}ê°œ ê²Œì‹œë¬¼")
                            return posts[:limit]
                        
                    else:
                        logger.debug(f"API ì‹œë„ {attempt_idx + 1} ì‹¤íŒ¨: HTTP {response.status}")
                        
            except asyncio.TimeoutError:
                logger.debug(f"API ì‹œë„ {attempt_idx + 1} íƒ€ì„ì•„ì›ƒ")
            except Exception as e:
                logger.debug(f"API ì‹œë„ {attempt_idx + 1} ì˜¤ë¥˜: {e}")
        
        return []
    
    def _parse_enhanced_api_response(self, data: Dict, instance: str) -> List[Dict]:
        """í–¥ìƒëœ API ì‘ë‹µ íŒŒì‹±"""
        posts = []
        
        try:
            post_list = data.get('posts', [])
            
            for idx, item in enumerate(post_list):
                try:
                    post_data = item.get('post', {})
                    counts = item.get('counts', {})
                    creator = item.get('creator', {})
                    community = item.get('community', {})
                    
                    # ê¸°ë³¸ ì •ë³´
                    title = post_data.get('name', f'Post {idx + 1}')
                    body = post_data.get('body', '')
                    url = post_data.get('ap_id', post_data.get('url', ''))
                    
                    # í†µê³„ ì •ë³´
                    score = counts.get('score', 0)
                    upvotes = counts.get('upvotes', 0)
                    downvotes = counts.get('downvotes', 0)
                    comments = counts.get('comments', 0)
                    
                    # ë©”íƒ€ ì •ë³´
                    author = creator.get('name', 'ìµëª…')
                    community_name = community.get('name', '')
                    published = post_data.get('published', '')
                    
                    # ì¸ë„¤ì¼ ë° ì„ë² ë“œ ì •ë³´
                    thumbnail_url = post_data.get('thumbnail_url', '')
                    embed_title = post_data.get('embed_title', '')
                    embed_description = post_data.get('embed_description', '')
                    
                    # í”Œë˜ê·¸
                    nsfw = post_data.get('nsfw', False)
                    locked = post_data.get('locked', False)
                    featured = post_data.get('featured_community', False) or post_data.get('featured_local', False)
                    
                    posts.append({
                        "ë²ˆí˜¸": idx + 1,
                        "ì›ì œëª©": title,
                        "ë²ˆì—­ì œëª©": None,
                        "ë§í¬": url,
                        "ì›ë¬¸URL": url,
                        "ì¸ë„¤ì¼ URL": thumbnail_url,
                        "ë³¸ë¬¸": body[:200] + "..." if len(body) > 200 else body,
                        "ì¡°íšŒìˆ˜": comments,  # LemmyëŠ” ì¡°íšŒìˆ˜ ëŒ€ì‹  ëŒ“ê¸€ìˆ˜ ì‚¬ìš©
                        "ì¶”ì²œìˆ˜": score,
                        "upvotes": upvotes,
                        "downvotes": downvotes,
                        "ëŒ“ê¸€ìˆ˜": comments,
                        "ì‘ì„±ì¼": self._format_lemmy_date(published),
                        "ì‘ì„±ì": author,
                        "ì»¤ë®¤ë‹ˆí‹°": community_name,
                        "ì¸ìŠ¤í„´ìŠ¤": instance,
                        "nsfw": nsfw,
                        "ì ê¹€": locked,
                        "ì¶”ì²œë¨": featured,
                        "ì„ë² ë“œì œëª©": embed_title,
                        "ì„ë² ë“œì„¤ëª…": embed_description,
                        "í¬ë¡¤ë§ë°©ì‹": "Lemmy-Enhanced-API"
                    })
                    
                except Exception as e:
                    logger.debug(f"API ê²Œì‹œë¬¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            return posts
            
        except Exception as e:
            logger.error(f"API ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return []
    
    def _parse_search_api_response(self, data: Dict, instance: str, community_name: str) -> List[Dict]:
        """ê²€ìƒ‰ API ì‘ë‹µ íŒŒì‹±"""
        posts = []
        
        try:
            post_list = data.get('posts', [])
            
            for idx, item in enumerate(post_list):
                try:
                    post_data = item.get('post', {})
                    counts = item.get('counts', {})
                    creator = item.get('creator', {})
                    community = item.get('community', {})
                    
                    # ì»¤ë®¤ë‹ˆí‹° ì´ë¦„ í™•ì¸
                    if community_name.lower() not in community.get('name', '').lower():
                        continue
                    
                    # ê¸°ë³¸ ì •ë³´
                    title = post_data.get('name', f'Search Result {idx + 1}')
                    body = post_data.get('body', '')
                    url = post_data.get('ap_id', post_data.get('url', ''))
                    
                    posts.append({
                        "ë²ˆí˜¸": idx + 1,
                        "ì›ì œëª©": title,
                        "ë²ˆì—­ì œëª©": None,
                        "ë§í¬": url,
                        "ì›ë¬¸URL": url,
                        "ë³¸ë¬¸": body[:200] + "..." if len(body) > 200 else body,
                        "ì¡°íšŒìˆ˜": counts.get('comments', 0),
                        "ì¶”ì²œìˆ˜": counts.get('score', 0),
                        "ëŒ“ê¸€ìˆ˜": counts.get('comments', 0),
                        "ì‘ì„±ì¼": self._format_lemmy_date(post_data.get('published', '')),
                        "ì‘ì„±ì": creator.get('name', 'ìµëª…'),
                        "ì»¤ë®¤ë‹ˆí‹°": community.get('name', ''),
                        "ì¸ìŠ¤í„´ìŠ¤": instance,
                        "í¬ë¡¤ë§ë°©ì‹": "Lemmy-Search-API"
                    })
                    
                except Exception as e:
                    logger.debug(f"ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            return posts
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ API ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return []
    
    async def _apply_exact_date_filter(self, posts: List[Dict], start_date: str, end_date: str) -> List[Dict]:
        """ì •í™•í•œ ë‚ ì§œ í•„í„°ë§"""
        if not start_date or not end_date:
            return posts
            
        try:
            start_dt = datetime.strptime(start_date, '%Y-%m-%d')
            end_dt = datetime.strptime(end_date, '%Y-%m-%d')
            end_dt = end_dt.replace(hour=23, minute=59, second=59)
            
            filtered_posts = []
            for post in posts:
                post_date_str = post.get('ì‘ì„±ì¼', '')
                post_date = self._parse_post_date_flexible(post_date_str)  # ğŸ”¥ ìœ ì—°í•œ íŒŒì‹±
                
                if post_date:
                    if start_dt <= post_date <= end_dt:
                        filtered_posts.append(post)
                    else:
                        # ğŸ”¥ ë””ë²„ê¹…ì„ ìœ„í•œ ë¡œê·¸
                        logger.debug(f"ë‚ ì§œ í•„í„° ì œì™¸: {post_date_str} ({post_date}) not in {start_date}~{end_date}")
                else:
                    # ğŸ”¥ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨í•œ ê²Œì‹œë¬¼ë„ í¬í•¨ (ë„ˆë¬´ ì—„ê²©í•˜ì§€ ì•Šê²Œ)
                    logger.debug(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ë¡œ í¬í•¨: {post_date_str}")
                    filtered_posts.append(post)
            
            logger.info(f"ë‚ ì§œ í•„í„°ë§: {len(posts)} -> {len(filtered_posts)}ê°œ (ì œì™¸ëœ ì´ìœ  í™•ì¸ í•„ìš”)")
            return filtered_posts
            
        except Exception as e:
            logger.error(f"ë‚ ì§œ í•„í„°ë§ ì˜¤ë¥˜: {e}")
            return posts  # ğŸ”¥ ì˜¤ë¥˜ì‹œ ì›ë³¸ ë°˜í™˜
    
    def _parse_post_date_flexible(self, date_str: str) -> Optional[datetime]:
        """ìœ ì—°í•œ ê²Œì‹œë¬¼ ë‚ ì§œ íŒŒì‹±"""
        if not date_str:
            return None
        
        # ğŸ”¥ Lemmy íŠ¹í™” ë‚ ì§œ í˜•ì‹ë“¤ ì‹œë„
        formats = [
            '%Y.%m.%d %H:%M',          # í•œêµ­ í˜•ì‹
            '%Y-%m-%d %H:%M',          # ì¼ë°˜ í˜•ì‹
            '%Y.%m.%d',                # ë‚ ì§œë§Œ
            '%Y-%m-%d',                # ë‚ ì§œë§Œ
            '%Y-%m-%dT%H:%M:%SZ',      # ISO with Z
            '%Y-%m-%dT%H:%M:%S.%fZ',   # ISO with microseconds
        ]
        
        for fmt in formats:
            try:
                return datetime.strptime(date_str.strip(), fmt)
            except ValueError:
                continue
        
        # ğŸ”¥ ISO í˜•ì‹ íŠ¹ë³„ ì²˜ë¦¬
        if 'T' in date_str:
            try:
                cleaned = date_str.replace('Z', '+00:00')
                return datetime.fromisoformat(cleaned)
            except Exception:
                pass
        
        logger.debug(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_str}")
        return None

    async def _apply_metric_filters(self, posts: List[Dict], min_views: int, min_likes: int) -> List[Dict]:
        """ë©”íŠ¸ë¦­ í•„í„°ë§ (ìµœì†Œ ì¡°íšŒìˆ˜, ì¶”ì²œìˆ˜)"""
        if min_views <= 0 and min_likes <= 0:
            return posts
        
        filtered_posts = [
            post for post in posts
            if post.get('ì¡°íšŒìˆ˜', 0) >= min_views and post.get('ì¶”ì²œìˆ˜', 0) >= min_likes
        ]
        
        logger.info(f"ë©”íŠ¸ë¦­ í•„í„°ë§: {len(posts)} -> {len(filtered_posts)}ê°œ")
        return filtered_posts

    async def _crawl_via_enhanced_rss(self, url: str, community_name: str, 
                                    instance: str, limit: int) -> List[Dict]:
        """í–¥ìƒëœ RSS í¬ë¡¤ë§"""
        if not AIOHTTP_AVAILABLE or not self.instance_manager.session:
            return []
            
        try:
            rss_urls = []
            
            # ì»¤ë®¤ë‹ˆí‹°ë³„ RSS
            if community_name:
                rss_urls.append(f"https://{instance}/feeds/c/{community_name}.xml")
            
            # ì „ì²´ RSS
            rss_urls.extend([
                f"https://{instance}/feeds/all.xml",
                f"https://{instance}/feeds/local.xml"
            ])
            
            for rss_url in rss_urls:
                try:
                    async with self.instance_manager.session.get(rss_url) as response:
                        if response.status == 200:
                            content = await response.text()
                            posts = self._parse_enhanced_rss(content, instance)
                            
                            if posts:
                                logger.info(f"RSS í¬ë¡¤ë§ ì„±ê³µ: {len(posts)}ê°œ ê²Œì‹œë¬¼")
                                return posts[:limit]
                                
                except Exception as e:
                    logger.debug(f"RSS URL ì‹¤íŒ¨ ({rss_url}): {e}")
                    continue
                    
        except Exception as e:
            logger.debug(f"RSS í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        
        return []
    
    async def _crawl_via_enhanced_html(self, url: str, instance: str, limit: int) -> List[Dict]:
        """í–¥ìƒëœ HTML í¬ë¡¤ë§"""
        if not AIOHTTP_AVAILABLE or not self.instance_manager.session:
            return []
            
        try:
            async with self.instance_manager.session.get(url) as response:
                if response.status == 200:
                    content = await response.text()
                    soup = BeautifulSoup(content, 'html.parser')
                    posts = self._parse_enhanced_html(soup, url, instance)
                    
                    if posts:
                        logger.info(f"HTML í¬ë¡¤ë§ ì„±ê³µ: {len(posts)}ê°œ ê²Œì‹œë¬¼")
                        return posts[:limit]
                        
        except Exception as e:
            logger.debug(f"HTML í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        
        return []
    
    def _parse_enhanced_rss(self, content: str, instance: str) -> List[Dict]:
        """í–¥ìƒëœ RSS íŒŒì‹±"""
        posts = []
        
        try:
            soup = BeautifulSoup(content, 'xml') or BeautifulSoup(content, 'html.parser')
            items = soup.find_all(['item', 'entry'])
            
            for idx, item in enumerate(items):
                try:
                    # ê¸°ë³¸ ì •ë³´
                    title_elem = item.find('title')
                    link_elem = item.find('link')
                    description_elem = item.find('description') or item.find('summary') or item.find('content')
                    pub_date_elem = item.find('pubDate') or item.find('published') or item.find('updated')
                    author_elem = item.find('author') or item.find('dc:creator')
                    
                    title = title_elem.get_text(strip=True) if title_elem else f'RSS Item {idx + 1}'
                    link = link_elem.get_text(strip=True) if link_elem else ''
                    description = description_elem.get_text(strip=True) if description_elem else ''
                    pub_date = pub_date_elem.get_text(strip=True) if pub_date_elem else ''
                    author = author_elem.get_text(strip=True) if author_elem else 'RSS'
                    
                    # RSS íŠ¹í™” ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                    guid = item.find('guid')
                    category = item.find('category')
                    
                    post_id = guid.get_text(strip=True) if guid else f"rss_{idx}"
                    community = category.get_text(strip=True) if category else ""
                    
                    posts.append({
                        "ë²ˆí˜¸": idx + 1,
                        "ì›ì œëª©": title,
                        "ë²ˆì—­ì œëª©": None,
                        "ë§í¬": link,
                        "ì›ë¬¸URL": link,
                        "ì¸ë„¤ì¼ URL": None,
                        "ë³¸ë¬¸": description[:200] + "..." if len(description) > 200 else description,
                        "ì¡°íšŒìˆ˜": 0,
                        "ì¶”ì²œìˆ˜": 0,
                        "ëŒ“ê¸€ìˆ˜": 0,
                        "ì‘ì„±ì¼": self._format_rss_date(pub_date),
                        "ì‘ì„±ì": author,
                        "ì»¤ë®¤ë‹ˆí‹°": community,
                        "ì¸ìŠ¤í„´ìŠ¤": instance,
                        "ê²Œì‹œë¬¼ID": post_id,
                        "í¬ë¡¤ë§ë°©ì‹": "Lemmy-Enhanced-RSS"
                    })
                    
                except Exception as e:
                    logger.debug(f"RSS í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            return posts
            
        except Exception as e:
            logger.error(f"RSS íŒŒì‹± ì˜¤ë¥˜: {e}")
            return []
    
    def _parse_enhanced_html(self, soup: BeautifulSoup, base_url: str, instance: str) -> List[Dict]:
        """í–¥ìƒëœ HTML íŒŒì‹±"""
        posts = []
        
        try:
            # Lemmy UI êµ¬ì¡°ì— íŠ¹í™”ëœ ì„ íƒìë“¤ (í™•ì¥ë¨)
            post_selectors = [
                'article.post-listing',
                '.post-listing',
                'div[class*="post-listing"]',
                'article[class*="post"]',
                '.post',
                '.community-link',
                '.post-link',
                'div[data-testid*="post"]',
                'div[role="article"]',
                '.thread-listing',
                '.item[data-post-id]',
                '.lemmy-post',
                '.feed-item'
            ]
            
            found_elements = []
            for selector in post_selectors:
                elements = soup.select(selector)
                if elements:
                    logger.info(f"HTML ì„ íƒì '{selector}'ë¡œ {len(elements)}ê°œ ìš”ì†Œ ë°œê²¬")
                    found_elements.extend(elements)
                    break  # ì²« ë²ˆì§¸ë¡œ ê²°ê³¼ê°€ ë‚˜ì˜¨ ì„ íƒì ì‚¬ìš©
            
            for idx, element in enumerate(found_elements[:50]):
                try:
                    post_data = self._extract_enhanced_post_from_element(element, idx, base_url, instance)
                    if post_data:
                        posts.append(post_data)
                except Exception as e:
                    logger.debug(f"HTML ìš”ì†Œ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            return posts
            
        except Exception as e:
            logger.error(f"HTML íŒŒì‹± ì˜¤ë¥˜: {e}")
            return []
    
    def _extract_enhanced_post_from_element(self, element, idx: int, base_url: str, instance: str) -> Optional[Dict]:
        """í–¥ìƒëœ HTML ìš”ì†Œì—ì„œ ê²Œì‹œë¬¼ ë°ì´í„° ì¶”ì¶œ"""
        try:
            # ì œëª© ì¶”ì¶œ (í™•ì¥ëœ ì„ íƒì)
            title_selectors = [
                '.post-title', '.post-name', 'h1', 'h2', 'h3',
                '.title', 'a[href*="/post/"]', '.lemmy-post-title',
                '.post-listing-title', '[data-testid="post-title"]'
            ]
            
            title = ""
            link = ""
            
            for selector in title_selectors:
                title_elem = element.select_one(selector)
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    if title_elem.name == 'a' and title_elem.get('href'):
                        link = urljoin(base_url, title_elem.get('href'))
                    if len(title) > 5:
                        break
            
            if not title:
                return None
            
            # ë§í¬ ì¶”ì¶œ (ì œëª©ì—ì„œ ëª» ì°¾ì€ ê²½ìš°)
            if not link:
                link_selectors = [
                    'a[href*="/post/"]', 'a[href*="/posts/"]',
                    '.post-link', '.lemmy-link', '.permalink'
                ]
                for selector in link_selectors:
                    link_elem = element.select_one(selector)
                    if link_elem and link_elem.get('href'):
                        link = urljoin(base_url, link_elem.get('href'))
                        break
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (í™•ì¥ë¨)
            score = self._extract_score(element)
            comments = self._extract_comments(element)
            author = self._extract_author(element)
            community = self._extract_community(element)
            date_str = self._extract_date(element)
            
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            nsfw = self._check_nsfw(element)
            locked = self._check_locked(element)
            featured = self._check_featured(element)
            
            return {
                "ë²ˆí˜¸": idx + 1,
                "ì›ì œëª©": title,
                "ë²ˆì—­ì œëª©": None,
                "ë§í¬": link or base_url,
                "ì›ë¬¸URL": link or base_url,
                "ì¸ë„¤ì¼ URL": self._extract_thumbnail(element, base_url),
                "ë³¸ë¬¸": f"Lemmy ê²Œì‹œë¬¼: {title}",
                "ì¡°íšŒìˆ˜": comments,  # ëŒ“ê¸€ìˆ˜ë¥¼ ì¡°íšŒìˆ˜ë¡œ ì‚¬ìš©
                "ì¶”ì²œìˆ˜": score,
                "ëŒ“ê¸€ìˆ˜": comments,
                "ì‘ì„±ì¼": self._format_lemmy_date(date_str),
                "ì‘ì„±ì": author,
                "ì»¤ë®¤ë‹ˆí‹°": community,
                "ì¸ìŠ¤í„´ìŠ¤": instance,
                "nsfw": nsfw,
                "ì ê¹€": locked,
                "ì¶”ì²œë¨": featured,
                "í¬ë¡¤ë§ë°©ì‹": "Lemmy-Enhanced-HTML"
            }
            
        except Exception as e:
            logger.debug(f"HTML ìš”ì†Œ ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None
    
    def _extract_score(self, element) -> int:
        """ì ìˆ˜ ì¶”ì¶œ (í™•ì¥ë¨)"""
        score_selectors = [
            '.score', '.points', '.upvotes', '.vote-score',
            '.post-score', '.karma', '[data-testid="score"]',
            '.lemmy-score', '.post-rating'
        ]
        
        for selector in score_selectors:
            score_elem = element.select_one(selector)
            if score_elem:
                score_text = score_elem.get_text(strip=True)
                numbers = re.findall(r'-?\d+', score_text)
                if numbers:
                    return int(numbers[0])
        
        return 0
    
    def _extract_comments(self, element) -> int:
        """ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ (í™•ì¥ë¨)"""
        comment_selectors = [
            '.comments', '.replies', '.comment-count',
            '.reply-count', '[data-testid="comments"]',
            '.lemmy-comments', '.post-comments', '.num-comments'
        ]
        
        for selector in comment_selectors:
            comment_elem = element.select_one(selector)
            if comment_elem:
                comment_text = comment_elem.get_text(strip=True)
                numbers = re.findall(r'\d+', comment_text)
                if numbers:
                    return int(numbers[0])
        
        return 0
    
    def _extract_author(self, element) -> str:
        """ì‘ì„±ì ì¶”ì¶œ (í™•ì¥ë¨)"""
        author_selectors = [
            '.author', '.creator', '.username', '.user',
            '.post-author', '.by-author', '[data-testid="author"]',
            '.lemmy-author', '.posted-by'
        ]
        
        for selector in author_selectors:
            author_elem = element.select_one(selector)
            if author_elem:
                author = author_elem.get_text(strip=True)
                if author and len(author) > 0:
                    return author
        
        return "ìµëª…"
    
    def _extract_community(self, element) -> str:
        """ì»¤ë®¤ë‹ˆí‹° ì¶”ì¶œ"""
        community_selectors = [
            '.community', '.community-name', '.subreddit',
            '.community-link', '[data-testid="community"]',
            '.lemmy-community', '.in-community'
        ]
        
        for selector in community_selectors:
            community_elem = element.select_one(selector)
            if community_elem:
                community = community_elem.get_text(strip=True)
                if community:
                    return community
        
        return ""
    
    def _extract_date(self, element) -> str:
        """ë‚ ì§œ ì¶”ì¶œ (í™•ì¥ë¨)"""
        date_selectors = [
            '.date', '.time', '.timestamp', '.created',
            '.post-date', '.published', '[data-testid="date"]',
            '.lemmy-date', '.posted-time', 'time'
        ]
        
        for selector in date_selectors:
            date_elem = element.select_one(selector)
            if date_elem:
                # datetime ì†ì„± ìš°ì„  í™•ì¸
                if date_elem.get('datetime'):
                    return date_elem.get('datetime')
                
                date_text = date_elem.get_text(strip=True)
                if date_text:
                    return date_text
        
        return ""
    
    def _extract_thumbnail(self, element, base_url: str) -> Optional[str]:
        """ì¸ë„¤ì¼ ì¶”ì¶œ (í™•ì¥ë¨)"""
        img_selectors = [
            'img', '.thumbnail', '.post-thumbnail',
            '.lemmy-thumbnail', '[data-testid="thumbnail"]'
        ]
        
        for selector in img_selectors:
            img_elem = element.select_one(selector)
            if img_elem:
                src = (img_elem.get('src') or
                      img_elem.get('data-src') or
                      img_elem.get('data-original'))
                if src:
                    if src.startswith('http'):
                        return src
                    else:
                        return urljoin(base_url, src)
        
        return None
    
    def _check_nsfw(self, element) -> bool:
        """NSFW ì—¬ë¶€ í™•ì¸"""
        nsfw_indicators = [
            '.nsfw', '.adult', '[data-nsfw="true"]',
            '.lemmy-nsfw', '.content-warning'
        ]
        
        for indicator in nsfw_indicators:
            if element.select_one(indicator):
                return True
        
        # í…ìŠ¤íŠ¸ì—ì„œ NSFW í™•ì¸
        element_text = element.get_text().lower()
        if 'nsfw' in element_text or 'adult' in element_text:
            return True
        
        return False
    
    def _check_locked(self, element) -> bool:
        """ì ê¹€ ì—¬ë¶€ í™•ì¸"""
        locked_indicators = [
            '.locked', '.closed', '[data-locked="true"]',
            '.lemmy-locked', '.post-locked'
        ]
        
        for indicator in locked_indicators:
            if element.select_one(indicator):
                return True
        
        return False
    
    def _check_featured(self, element) -> bool:
        """ì¶”ì²œ ì—¬ë¶€ í™•ì¸"""
        featured_indicators = [
            '.featured', '.pinned', '.sticky', '.stickied',
            '.lemmy-featured', '.post-featured', '[data-featured="true"]'
        ]
        
        for indicator in featured_indicators:
            if element.select_one(indicator):
                return True
        
        return False
    
    async def _apply_rate_limit(self, instance: str):
        """ì¸ìŠ¤í„´ìŠ¤ë³„ ë ˆì´íŠ¸ ë¦¬ë¯¸í„° ì ìš©"""
        now = time.time()
        if instance in self.rate_limiter:
            last_request = self.rate_limiter[instance]
            elapsed = now - last_request
            if elapsed < LEMMY_CONFIG['rate_limit_delay']:
                await asyncio.sleep(LEMMY_CONFIG['rate_limit_delay'] - elapsed)
        
        self.rate_limiter[instance] = now
    
    def _apply_lemmy_sorting(self, posts: List[Dict], sort: str) -> List[Dict]:
        """Lemmy íŠ¹í™” ì •ë ¬ ì ìš©"""
        if not posts:
            return posts
        
        try:
            sort_lower = sort.lower()
            
            if sort_lower in ["hot", "active"]:
                # ë³µí•© ì ìˆ˜ ê¸°ë°˜ ì •ë ¬ (ì ìˆ˜ + ëŒ“ê¸€ + ì‹œê°„)
                def hot_score(post):
                    score = post.get('ì¶”ì²œìˆ˜', 0)
                    comments = post.get('ëŒ“ê¸€ìˆ˜', 0)
                    # ì‹œê°„ ê°€ì¤‘ì¹˜ (ìµœê·¼ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
                    try:
                        post_date = self._parse_post_date(post.get('ì‘ì„±ì¼', ''))
                        if post_date:
                            hours_ago = (datetime.now() - post_date).total_seconds() / 3600
                            time_weight = max(0, 1 - (hours_ago / 168))  # ì¼ì£¼ì¼ ê¸°ì¤€
                        else:
                            time_weight = 0.5
                    except:
                        time_weight = 0.5
                    
                    return (score * 1.0 + comments * 0.5) * (1 + time_weight)
                
                return sorted(posts, key=hot_score, reverse=True)
                
            elif sort_lower in ["top", "topall", "popular"]:
                return sorted(posts, key=lambda x: x.get('ì¶”ì²œìˆ˜', 0), reverse=True)
                
            elif sort_lower in ["new", "recent"]:
                return sorted(posts, key=lambda x: x.get('ì‘ì„±ì¼', ''), reverse=True)
                
            elif sort_lower in ["comments", "mostcomments"]:
                return sorted(posts, key=lambda x: x.get('ëŒ“ê¸€ìˆ˜', 0), reverse=True)
                
            elif sort_lower == "controversial":
                # ë…¼ë€ì˜ ì—¬ì§€ê°€ ìˆëŠ” ê²Œì‹œë¬¼ (upvotesì™€ downvotes ë¹„ìœ¨)
                def controversial_score(post):
                    upvotes = post.get('upvotes', post.get('ì¶”ì²œìˆ˜', 0))
                    downvotes = post.get('downvotes', 0)
                    total = upvotes + downvotes
                    if total == 0:
                        return 0
                    return min(upvotes, downvotes) * total
                
                return sorted(posts, key=controversial_score, reverse=True)
                
            else:
                return posts
                
        except Exception as e:
            logger.error(f"ì •ë ¬ ì˜¤ë¥˜: {e}")
            return posts
    
    async def _enhance_post_metadata(self, posts: List[Dict], instance_info: LemmyInstance) -> List[Dict]:
        """ê²Œì‹œë¬¼ ë©”íƒ€ë°ì´í„° ë³´ê°•"""
        for post in posts:
            # ì¸ìŠ¤í„´ìŠ¤ ì •ë³´ ì¶”ê°€
            post['ì¸ìŠ¤í„´ìŠ¤ì •ë³´'] = {
                'ì´ë¦„': instance_info.name,
                'ì‚¬ìš©ììˆ˜': instance_info.users,
                'ì§€ì—­': instance_info.region,
                'íƒ€ì…': instance_info.type
            }
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            post['í’ˆì§ˆì ìˆ˜'] = self._calculate_post_quality(post)
            
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            post['í¬ë¡¤ë§ì‹œê°„'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            post['í”Œë«í¼'] = 'Lemmy'
            
            # URL ì •ê·œí™”
            if post.get('ë§í¬') and not post['ë§í¬'].startswith('http'):
                post['ë§í¬'] = f"https://{instance_info.domain}{post['ë§í¬']}"
            
        return posts
    
    def _calculate_post_quality(self, post: Dict) -> float:
        """ê²Œì‹œë¬¼ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # ì œëª© í’ˆì§ˆ
        title = post.get('ì›ì œëª©', '')
        if 10 <= len(title) <= 100:
            score += 2.0
        elif 5 <= len(title) <= 200:
            score += 1.0
        
        # ì°¸ì—¬ë„
        upvotes = post.get('ì¶”ì²œìˆ˜', 0)
        comments = post.get('ëŒ“ê¸€ìˆ˜', 0)
        
        if upvotes > 0:
            score += min(2.0, upvotes / 10)
        if comments > 0:
            score += min(2.0, comments / 5)
        
        # ì½˜í…ì¸  ìœ í˜•
        if post.get('ì¸ë„¤ì¼ URL'):
            score += 0.5
        if post.get('ë³¸ë¬¸') and len(post['ë³¸ë¬¸']) > 50:
            score += 1.0
        
        # ë©”íƒ€ë°ì´í„° ì™„ì„±ë„
        if post.get('ì‘ì„±ì') != 'ìµëª…':
            score += 0.5
        if post.get('ì»¤ë®¤ë‹ˆí‹°'):
            score += 0.5
        
        return min(10.0, score)
    
    def _format_lemmy_date(self, date_str: str) -> str:
        """Lemmy ë‚ ì§œ í˜•ì‹ ë³€í™˜ (í–¥ìƒë¨)"""
        if not date_str:
            return datetime.now().strftime('%Y.%m.%d')
        
        try:
            # ğŸ”¥ ISO í˜•ì‹ ì²˜ë¦¬ ê°œì„ 
            if 'T' in date_str:
                # Z ë˜ëŠ” +00:00 í˜•íƒœ ëª¨ë‘ ì²˜ë¦¬
                cleaned = date_str.replace('Z', '+00:00')
                if not cleaned.endswith(('+', '-')):
                    cleaned += '+00:00'
                dt = datetime.fromisoformat(cleaned.replace('Z', '+00:00'))
                return dt.strftime('%Y.%m.%d %H:%M')
            
            # ğŸ”¥ ì¶”ê°€: Lemmy íŠ¹í™” ë‚ ì§œ í˜•ì‹ë“¤
            formats = [
                '%Y-%m-%dT%H:%M:%S.%fZ',     # ë§ˆì´í¬ë¡œì´ˆ í¬í•¨
                '%Y-%m-%dT%H:%M:%SZ',        # ê¸°ë³¸ ISO
                '%Y-%m-%d %H:%M:%S',         # ì¼ë°˜ í˜•ì‹
                '%Y.%m.%d %H:%M',            # í•œêµ­ í˜•ì‹
            ]
            
            for fmt in formats:
                try:
                    dt = datetime.strptime(date_str, fmt)
                    return dt.strftime('%Y.%m.%d %H:%M')
                except ValueError:
                    continue
                    
            return date_str  # íŒŒì‹± ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜
            
        except Exception as e:
            logger.debug(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_str} -> {e}")
            return datetime.now().strftime('%Y.%m.%d')
    
    def _format_rss_date(self, date_str: str) -> str:
        """RSS ë‚ ì§œ í˜•ì‹ ë³€í™˜ (í–¥ìƒë¨)"""
        if not date_str:
            return datetime.now().strftime('%Y.%m.%d')
        
        try:
            # ë‹¤ì–‘í•œ RSS ë‚ ì§œ í˜•ì‹ ì§€ì›
            formats = [
                '%a, %d %b %Y %H:%M:%S %z',
                '%a, %d %b %Y %H:%M:%S %Z',
                '%Y-%m-%dT%H:%M:%S%z',
                '%Y-%m-%dT%H:%M:%SZ',
                '%Y-%m-%d %H:%M:%S',
                '%Y-%m-%d'
            ]
            
            for fmt in formats:
                try:
                    dt = datetime.strptime(date_str.strip(), fmt)
                    return dt.strftime('%Y.%m.%d %H:%M')
                except ValueError:
                    continue
            
            return datetime.now().strftime('%Y.%m.%d')
            
        except Exception:
            return datetime.now().strftime('%Y.%m.%d')
    
    def _parse_post_date(self, date_str: str) -> Optional[datetime]:
        """ê²Œì‹œë¬¼ ë‚ ì§œ íŒŒì‹± (í–¥ìƒë¨)"""
        if not date_str:
            return None
        
        try:
            # ë‹¤ì–‘í•œ í˜•ì‹ ì‹œë„
            formats = [
                '%Y.%m.%d %H:%M',
                '%Y-%m-%d %H:%M',
                '%Y.%m.%d',
                '%Y-%m-%d',
                '%m.%d %H:%M',
                '%m-%d %H:%M'
            ]
            
            for fmt in formats:
                try:
                    return datetime.strptime(date_str.strip(), fmt)
                except ValueError:
                    continue
            
            # ISO í˜•ì‹ ì‹œë„
            if 'T' in date_str:
                return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
            
            return None
            
        except Exception:
            return None
    
    async def __aenter__(self):
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.instance_manager.__aexit__(exc_type, exc_val, exc_tb)

# ================================
# ğŸ”¥ ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜ (í–¥ìƒë¨)
# ================================

async def crawl_lemmy_board(community_input: str, limit: int = 50, sort: str = "Hot",
                           min_views: int = 0, min_likes: int = 0, 
                           time_filter: str = "day", start_date: str = None, 
                           end_date: str = None, websocket=None, 
                           enforce_date_limit: bool = False,
                           start_index: int = 1, end_index: int = 20) -> List[Dict]:
    """í–¥ìƒëœ Lemmy ì»¤ë®¤ë‹ˆí‹° í¬ë¡¤ë§ - ì •í™•í•œ ë²”ìœ„ ì§€ì›"""
    
    # aiohttp ê°€ìš©ì„± í™•ì¸
    if not AIOHTTP_AVAILABLE:
        error_msg = "Lemmy í¬ë¡¤ëŸ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. aiohttp ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: pip install aiohttp"
        logger.error(error_msg)
        raise Exception(error_msg)
    
    async with AdvancedLemmyCrawler() as crawler:
        try:
            logger.info(f"Lemmy í¬ë¡¤ë§ ì‹œì‘: {community_input} (ë²”ìœ„: {start_index}-{end_index})")
            
            # URL íŒŒì‹± ë° ì •ê·œí™”
            url, community_name, instance = crawler.community_searcher.resolve_community_url(community_input)
            
            if not community_name or not instance:
                if '@' not in community_input and '.' not in community_input:
                    corrected_url = f"{community_input}@lemmy.world"
                    url, community_name, instance = crawler.community_searcher.resolve_community_url(corrected_url)
                else:
                    raise Exception(f"ì˜¬ë°”ë¥´ì§€ ì•Šì€ Lemmy ì»¤ë®¤ë‹ˆí‹° í˜•ì‹: {community_input}")
            
            # ì¸ìŠ¤í„´ìŠ¤ ìƒíƒœ í™•ì¸
            instance_info = await crawler.instance_manager.get_instance_info(instance)
            if not instance_info:
                logger.warning(f"ì¸ìŠ¤í„´ìŠ¤ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì§€ë§Œ í¬ë¡¤ë§ì„ ê³„ì† ì‹œë„í•©ë‹ˆë‹¤: {instance}")
                instance_info = type('obj', (object,), {
                    'name': instance,
                    'domain': instance,
                    'users': 0,
                    'status': 'unknown',
                    'region': '',
                    'type': 'general'
                })()
            
            await crawler._apply_rate_limit(instance)
            
            # ğŸ”¥ í•µì‹¬ ê°œì„ : ì •í™•í•œ ê°œìˆ˜ë§Œ í¬ë¡¤ë§
            if start_date and end_date:
                # ë‚ ì§œ í•„í„°ë§ ëª¨ë“œ: ì¶©ë¶„í•œ ì–‘ ìˆ˜ì§‘ í›„ í•„í„°ë§
                posts = await crawler.crawl_lemmy_community(
                    community_input, limit * 2, sort, min_views, min_likes,
                    start_date, end_date, websocket, enforce_date_limit, time_filter
                )
                
                # ë‚ ì§œ í•„í„°ë§ í›„ ë²”ìœ„ ì ìš©
                if posts and len(posts) >= end_index:
                    posts = posts[start_index-1:end_index]
                
            else:
                # ğŸ”¥ ì¼ë°˜ ëª¨ë“œ: ì •í™•í•œ ë²”ìœ„ë§Œ í¬ë¡¤ë§
                # ì•½ê°„ì˜ ì—¬ìœ ë¥¼ ë‘ê³  í¬ë¡¤ë§ (API í˜ì´ì§€ë„¤ì´ì…˜ ê³ ë ¤)
                actual_limit = min(end_index + 10, 100)
                
                posts = await crawler.crawl_lemmy_community(
                    community_input, actual_limit, sort, min_views, min_likes,
                    start_date, end_date, websocket, enforce_date_limit, time_filter
                )
                
                # ğŸ”¥ ì •í™•í•œ ë²”ìœ„ë¡œ ìë¥´ê¸°
                if posts:
                    posts = posts[start_index-1:end_index]
            
            if not posts:
                error_details = f"""
Lemmy ì»¤ë®¤ë‹ˆí‹° '{community_name}@{instance}'ì—ì„œ {start_index}-{end_index}ìœ„ ê²Œì‹œë¬¼ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ê°€ëŠ¥í•œ ì›ì¸:
1. ì»¤ë®¤ë‹ˆí‹°ì— ê²Œì‹œë¬¼ì´ {end_index}ê°œ ë¯¸ë§Œ
2. ì„¤ì •í•œ ì •ë ¬ ê¸°ì¤€ì— ë§ëŠ” ê²Œì‹œë¬¼ì´ ë¶€ì¡±
3. ì¸ìŠ¤í„´ìŠ¤ê°€ ì¼ì‹œì ìœ¼ë¡œ ë¶ˆì•ˆì •
                """
                raise Exception(error_details.strip())
            
            # ğŸ”¥ ë©”íƒ€ë°ì´í„° ë³´ê°•
            enhanced_posts = await crawler._enhance_post_metadata(posts, instance_info)
            
            # ğŸ”¥ ë²ˆí˜¸ë¥¼ start_indexë¶€í„° ì •í™•íˆ ë¶€ì—¬
            for idx, post in enumerate(enhanced_posts):
                post['ë²ˆí˜¸'] = start_index + idx
            
            logger.info(f"Lemmy í¬ë¡¤ë§ ì™„ë£Œ: {len(enhanced_posts)}ê°œ ê²Œì‹œë¬¼ ({start_index}-{start_index+len(enhanced_posts)-1}ìœ„)")
            return enhanced_posts
            
        except Exception as e:
            logger.error(f"Lemmy í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            raise

# ================================
# ğŸ”¥ ë ˆê±°ì‹œ í˜¸í™˜ì„± í•¨ìˆ˜ë“¤
# ================================

def resolve_lemmy_community_id(community_input: str) -> str:
    """ì»¤ë®¤ë‹ˆí‹° ì…ë ¥ì„ í‘œì¤€ URLë¡œ ë³€í™˜ (ë™ê¸° ë²„ì „)"""
    manager = EnhancedLemmyInstanceManager()
    searcher = AdvancedLemmyCommunitySearcher(manager)
    url, _, _ = searcher.resolve_community_url(community_input)
    return url

def filter_posts_by_date(posts: List[dict], start_date: Optional[str], end_date: Optional[str]) -> List[dict]:
    """ë‚ ì§œë³„ ê²Œì‹œë¬¼ í•„í„°ë§ (ë ˆê±°ì‹œ í˜¸í™˜ì„±)"""
    if not start_date or not end_date:
        return posts

    from datetime import datetime
    start_dt = datetime.strptime(start_date, "%Y-%m-%d")
    end_dt = datetime.strptime(end_date, "%Y-%m-%d")
    end_dt = end_dt.replace(hour=23, minute=59, second=59)

    filtered = []
    for post in posts:
        post_time = post.get('ì‘ì„±ì¼') or post.get('published')  # datetime ê°ì²´ or ë¬¸ìì—´
        if isinstance(post_time, str):
            try:
                post_time = datetime.strptime(post_time, "%Y-%m-%d %H:%M:%S")
            except:
                continue
        if start_dt <= post_time <= end_dt:
            filtered.append(post)
    
    return filtered

# ================================
# ğŸ”¥ ì¶”ê°€ í—¬í¼ í•¨ìˆ˜ (ê¸°ì¡´ ì½”ë“œ ì˜í–¥ ì—†ìŒ)
# ================================

def is_lemmy_instance(domain: str) -> bool:
    """ì „ì—­ í•¨ìˆ˜ë¡œ Lemmy ì¸ìŠ¤í„´ìŠ¤ í™•ì¸ (ëª¨ë“ˆ í˜¸í™˜ì„±)"""
    domain = domain.lower().strip()
    
    # 1. ì•Œë ¤ì§„ ì¸ìŠ¤í„´ìŠ¤ í™•ì¸
    if domain in KNOWN_LEMMY_INSTANCES:
        return True
    
    # 2. ê¸°íšì„œ ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™•ì¸
    for pattern in _compiled_patterns:
        if pattern.match(domain):
            return True
    
    return False

# ëª¨ë“ˆ ì •ë³´ (ë™ì  íƒì§€ë¥¼ ìœ„í•œ ë©”íƒ€ë°ì´í„°)
DISPLAY_NAME = "Lemmy Crawler"
DESCRIPTION = "Lemmy ì»¤ë®¤ë‹ˆí‹° í¬ë¡¤ëŸ¬"
VERSION = "2.0.0"
SUPPORTED_DOMAINS = ["lemmy.world", "lemmy.ml", "beehaw.org", "sh.itjust.works", "programming.dev"]
KEYWORDS = ["lemmy", "fediverse", "@lemmy"]

# ëª¨ë“ˆ ë¡œë“œ ì‹œ ì´ˆê¸°í™”
logger.info("ğŸ”¥ í–¥ìƒëœ Lemmy í¬ë¡¤ëŸ¬ v2.0 ë¡œë“œ ì™„ë£Œ")
logger.info(f"ğŸ“Š ì§€ì› ì¸ìŠ¤í„´ìŠ¤: {len(KNOWN_LEMMY_INSTANCES)}ê°œ")
logger.info(f"ğŸ¯ ì•ˆì •ì„± í‹°ì–´: {sum(len(v) for v in STABILITY_TIER.values())}ê°œ")
logger.info(f"âš™ï¸ ì„¤ì •: {LEMMY_CONFIG['api_timeout']}s timeout, {LEMMY_CONFIG['cache_ttl']}s cache")
logger.info(f"ğŸ” ë„ë©”ì¸ íŒ¨í„´: {len(LEMMY_DOMAIN_PATTERNS)}ê°œ ì •ê·œí‘œí˜„ì‹ ì ìš©")
if not AIOHTTP_AVAILABLE:
    logger.warning("âš ï¸ aiohttpê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Lemmy í¬ë¡¤ëŸ¬ê°€ ì œí•œì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.")