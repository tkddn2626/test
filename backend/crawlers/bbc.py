# bbc.py - ìˆœìˆ˜ í¬ë¡¤ë§ ë¡œì§ë§Œ (ë©”ì‹œì§€ ì²˜ë¦¬ëŠ” main.pyì—ì„œ)

import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime, timedelta
from urllib.parse import urljoin, urlparse
import logging
from typing import List, Dict, Optional, Tuple
import asyncio
import time
import hashlib
from dataclasses import dataclass, field

# aiohttp ì„í¬íŠ¸ë¥¼ try-exceptë¡œ ë³´í˜¸
try:
    import aiohttp
    AIOHTTP_AVAILABLE = True
except ImportError:
    AIOHTTP_AVAILABLE = False
    aiohttp = None
    logging.warning("aiohttp ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install aiohttpë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ================================
# ğŸ¯ BBC URL ìë™ ê°ì§€ ì‹œìŠ¤í…œ
# ================================

def detect_bbc_url_and_extract_info(input_text: str) -> dict:
    """BBC URLì„ ê°ì§€í•˜ê³  ê´€ë ¨ ì •ë³´ë¥¼ ì¶”ì¶œ"""
    
    if not input_text or not input_text.strip():
        return {"is_bbc": False}
    
    input_text = input_text.strip()
    
    # BBC URL íŒ¨í„´ë“¤
    bbc_patterns = [
        r'https?://(?:www\.)?bbc\.com/.*',
        r'https?://(?:www\.)?bbc\.co\.uk/.*',
        r'bbc\.com/.*',
        r'bbc\.co\.uk/.*'
    ]
    
    is_bbc_url = False
    for pattern in bbc_patterns:
        if re.match(pattern, input_text, re.IGNORECASE):
            is_bbc_url = True
            break
    
    if not is_bbc_url:
        return {"is_bbc": False}
    
    # URL ì •ê·œí™”
    if not input_text.startswith('http'):
        if input_text.startswith('bbc.'):
            normalized_url = f"https://www.{input_text}"
        else:
            normalized_url = f"https://www.bbc.com/{input_text.lstrip('/')}"
    else:
        normalized_url = input_text
    
    # URLì—ì„œ ì •ë³´ ì¶”ì¶œ
    try:
        parsed = urlparse(normalized_url)
        path_parts = [p for p in parsed.path.split('/') if p]
        
        # ì„¹ì…˜ ì •ë³´ ì¶”ì¶œ
        section_info = analyze_bbc_url_section(normalized_url, path_parts)
        
        return {
            "is_bbc": True,
            "original_input": input_text,
            "normalized_url": normalized_url,
            "detected_site": "bbc",
            "board_name": section_info["display_name"],
            "section": section_info["section"],
            "subsection": section_info["subsection"],
            "description": section_info["description"],
            "auto_detected": True,
            "switch_message": f"ğŸ¯ BBC {section_info['display_name']} ì„¹ì…˜ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤!"
        }
        
    except Exception as e:
        logger.error(f"BBC URL ë¶„ì„ ì˜¤ë¥˜: {e}")
        return {
            "is_bbc": True,
            "original_input": input_text,
            "normalized_url": normalized_url,
            "detected_site": "bbc",
            "board_name": "BBC News",
            "section": "general",
            "subsection": "",
            "description": "BBC ë‰´ìŠ¤ ë° ì½˜í…ì¸ ",
            "auto_detected": True,
            "switch_message": "ğŸ¯ BBC ì‚¬ì´íŠ¸ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤!"
        }

def analyze_bbc_url_section(url: str, path_parts: list) -> dict:
    """BBC URLì˜ ì„¹ì…˜ ì •ë³´ë¥¼ ë¶„ì„"""
    
    # BBC ì„¹ì…˜ë³„ ì •ë³´ (í™•ì¥ë¨)
    bbc_section_map = {
        # ì£¼ìš” ì„¹ì…˜
        "news": {"display_name": "BBC News", "description": "BBC ë‰´ìŠ¤ - ì„¸ê³„ ë° ì˜êµ­ ë‰´ìŠ¤"},
        "sport": {"display_name": "BBC Sport", "description": "BBC ìŠ¤í¬ì¸  - ëª¨ë“  ìŠ¤í¬ì¸  ë‰´ìŠ¤"},
        "business": {"display_name": "BBC Business", "description": "BBC ë¹„ì¦ˆë‹ˆìŠ¤ - ê²½ì œ ë° ê¸ˆìœµ ë‰´ìŠ¤"},
        "technology": {"display_name": "BBC Technology", "description": "BBC ê¸°ìˆ  - ê³¼í•™ê¸°ìˆ  ë‰´ìŠ¤"},
        "health": {"display_name": "BBC Health", "description": "BBC ê±´ê°• - ì˜ë£Œ ë° ê±´ê°• ë‰´ìŠ¤"},
        "science": {"display_name": "BBC Science", "description": "BBC ê³¼í•™ - ê³¼í•™ ì—°êµ¬ ë° ë°œê²¬"},
        "entertainment": {"display_name": "BBC Entertainment", "description": "BBC ì—°ì˜ˆ - ë¬¸í™” ë° ì—°ì˜ˆ ë‰´ìŠ¤"},
        
        # ìŠ¤í¬ì¸  ì„¸ë¶€ ì„¹ì…˜ (í™•ì¥ë¨)
        "football": {"display_name": "BBC Football", "description": "BBC ì¶•êµ¬ - í”„ë¦¬ë¯¸ì–´ë¦¬ê·¸, ì±”í”¼ì–¸ìŠ¤ë¦¬ê·¸ ë“±"},
        "rugby-union": {"display_name": "BBC Rugby Union", "description": "BBC ëŸ­ë¹„ ìœ ë‹ˆì˜¨ - 6ë„¤ì´ì…˜ìŠ¤, ì›”ë“œì»µ ë“±"},
        "rugby-league": {"display_name": "BBC Rugby League", "description": "BBC ëŸ­ë¹„ ë¦¬ê·¸ - ìŠˆí¼ë¦¬ê·¸ ë“±"},
        "cricket": {"display_name": "BBC Cricket", "description": "BBC í¬ë¦¬ì¼“ - í…ŒìŠ¤íŠ¸, T20, ì›”ë“œì»µ"},
        "tennis": {"display_name": "BBC Tennis", "description": "BBC í…Œë‹ˆìŠ¤ - ìœ”ë¸”ë˜, ê·¸ëœë“œìŠ¬ë¨"},
        "golf": {"display_name": "BBC Golf", "description": "BBC ê³¨í”„ - ë§ˆìŠ¤í„°ìŠ¤, ë©”ì´ì € ëŒ€íšŒ"},
        "formula1": {"display_name": "BBC Formula 1", "description": "BBC F1 - í¬ë®¬ëŸ¬ì› ë‰´ìŠ¤"},
        "boxing": {"display_name": "BBC Boxing", "description": "BBC ë³µì‹± - ì›”ë“œ íƒ€ì´í‹€ ë§¤ì¹˜"},
        "athletics": {"display_name": "BBC Athletics", "description": "BBC ìœ¡ìƒ - ì˜¬ë¦¼í”½, ì„¸ê³„ì„ ìˆ˜ê¶Œ"},
        "swimming": {"display_name": "BBC Swimming", "description": "BBC ìˆ˜ì˜ - ì˜¬ë¦¼í”½, ì„¸ê³„ì„ ìˆ˜ê¶Œ"},
        "cycling": {"display_name": "BBC Cycling", "description": "BBC ì‚¬ì´í´ë§ - íˆ¬ë¥´ ë“œ í”„ë‘ìŠ¤"},
        "motorsport": {"display_name": "BBC Motorsport", "description": "BBC ëª¨í„°ìŠ¤í¬ì¸  - F1, MotoGP"},
        "winter-sports": {"display_name": "BBC Winter Sports", "description": "BBC ìœˆí„°ìŠ¤í¬ì¸  - ìŠ¤í‚¤, ìŠ¤ì¼€ì´íŒ…"},
        "horse-racing": {"display_name": "BBC Horse Racing", "description": "BBC ê²½ë§ˆ - ê·¸ëœë“œë‚´ì…”ë„"},
        "snooker": {"display_name": "BBC Snooker", "description": "BBC ìŠ¤ëˆ„ì»¤ - ì›”ë“œì±”í”¼ì–¸ì‹­"},
        "darts": {"display_name": "BBC Darts", "description": "BBC ë‹¤íŠ¸ - PDC ì›”ë“œì±”í”¼ì–¸ì‹­"},
        
        # ë‰´ìŠ¤ ì„¸ë¶€ ì„¹ì…˜
        "world": {"display_name": "BBC World News", "description": "BBC ì„¸ê³„ë‰´ìŠ¤ - êµ­ì œ ë‰´ìŠ¤"},
        "uk": {"display_name": "BBC UK News", "description": "BBC ì˜êµ­ë‰´ìŠ¤ - ì˜êµ­ êµ­ë‚´ ë‰´ìŠ¤"},
        "politics": {"display_name": "BBC Politics", "description": "BBC ì •ì¹˜ - ì˜êµ­ ë° ì„¸ê³„ ì •ì¹˜"},
        "education": {"display_name": "BBC Education", "description": "BBC êµìœ¡ - êµìœ¡ ì •ì±… ë° ë‰´ìŠ¤"},
        "science-environment": {"display_name": "BBC Science & Environment", "description": "BBC ê³¼í•™í™˜ê²½ - ê¸°í›„ë³€í™”, í™˜ê²½"},
        "entertainment-arts": {"display_name": "BBC Entertainment & Arts", "description": "BBC ì—°ì˜ˆì˜ˆìˆ  - ë¬¸í™”, ì˜ˆìˆ "},
        "disability": {"display_name": "BBC Disability", "description": "BBC ì¥ì•  - ì¥ì• ì¸ ê´€ë ¨ ë‰´ìŠ¤"},
        
        # ì§€ì—­ë³„ ë‰´ìŠ¤
        "england": {"display_name": "BBC England", "description": "BBC ì‰ê¸€ëœë“œ - ì‰ê¸€ëœë“œ ì§€ì—­ ë‰´ìŠ¤"},
        "scotland": {"display_name": "BBC Scotland", "description": "BBC ìŠ¤ì½”í‹€ëœë“œ - ìŠ¤ì½”í‹€ëœë“œ ë‰´ìŠ¤"},
        "wales": {"display_name": "BBC Wales", "description": "BBC ì›¨ì¼ìŠ¤ - ì›¨ì¼ìŠ¤ ë‰´ìŠ¤"},
        "northern-ireland": {"display_name": "BBC Northern Ireland", "description": "BBC ë¶ì•„ì¼ëœë“œ - ë¶ì•„ì¼ëœë“œ ë‰´ìŠ¤"}
    }
    
    main_section = ""
    subsection = ""
    
    if len(path_parts) >= 1:
        main_section = path_parts[0].lower()
        
    if len(path_parts) >= 2:
        subsection = path_parts[1].lower()
    
    # ì£¼ ì„¹ì…˜ê³¼ ì„œë¸Œì„¹ì…˜ ì¡°í•©ìœ¼ë¡œ ì°¾ê¸°
    combined_key = f"{main_section}-{subsection}" if subsection else main_section
    
    if combined_key in bbc_section_map:
        section_info = bbc_section_map[combined_key]
        return {
            "section": main_section,
            "subsection": subsection,
            "display_name": section_info["display_name"],
            "description": section_info["description"]
        }
    elif main_section in bbc_section_map:
        section_info = bbc_section_map[main_section]
        return {
            "section": main_section,
            "subsection": subsection,
            "display_name": section_info["display_name"],
            "description": section_info["description"]
        }
    else:
        # ì•Œ ìˆ˜ ì—†ëŠ” ì„¹ì…˜
        display_name = f"BBC {main_section.title()}" if main_section else "BBC News"
        if subsection:
            display_name += f" - {subsection.title()}"
            
        return {
            "section": main_section or "general",
            "subsection": subsection,
            "display_name": display_name,
            "description": f"BBC {main_section} ì„¹ì…˜" if main_section else "BBC ì½˜í…ì¸ "
        }

def parse_relative_time(relative_str: str) -> str:
    """ìƒëŒ€ ì‹œê°„ íŒŒì‹± ('2 hours ago' ë“±)"""
    try:
        import re
        now = datetime.now()
        
        # "X hours ago", "X minutes ago" ë“± íŒŒì‹±
        match = re.search(r'(\d+)\s*(hour|minute|day|week)s?\s*ago', relative_str.lower())
        if match:
            value = int(match.group(1))
            unit = match.group(2)
            
            if unit == 'minute':
                result_time = now - timedelta(minutes=value)
            elif unit == 'hour':
                result_time = now - timedelta(hours=value)
            elif unit == 'day':
                result_time = now - timedelta(days=value)
            elif unit == 'week':
                result_time = now - timedelta(weeks=value)
            else:
                result_time = now
                
            return result_time.strftime('%Y.%m.%d %H:%M')
        
        return datetime.now().strftime('%Y.%m.%d %H:%M')
    except:
        return datetime.now().strftime('%Y.%m.%d %H:%M')

def is_bbc_domain(url: str) -> bool:
    """URLì´ BBC ë„ë©”ì¸ì¸ì§€ ê°„ë‹¨ í™•ì¸"""
    
    if not url:
        return False
    
    url_lower = url.lower()
    return 'bbc.com' in url_lower or 'bbc.co.uk' in url_lower

# ================================
# ğŸ›¡ï¸ ì•ˆì •ì„± ìš°ì„  BBC ì„¤ì •
# ================================

# ğŸ”¥ Fallback ì„ íƒì (ì•ˆì •ì„± ìš°ì„ )
BBC_STABLE_SELECTORS = {
    # Level 1: ìµœì‹  BBC ì»´í¬ë„ŒíŠ¸ (ì‹œë„í•´ë³¼ ê°€ì¹˜ ìˆìŒ)
    'level1_primary': [
        '[data-testid="liverpool-card"]',
        '[data-testid="edinburgh-card"]', 
        '.gs-c-promo',
    ],
    
    # Level 2: ê²€ì¦ëœ BBC ì„ íƒì (ì‹ ë¢°ë„ ë†’ìŒ)
    'level2_reliable': [
        '.media',
        'article[class*="promo"]',
        '.gel-layout__item article',
        '.qa-heading-link',
    ],
    
    # Level 3: ì¼ë°˜ì ì¸ êµ¬ì¡° (ê±°ì˜ í•­ìƒ ì‘ë™)
    'level3_general': [
        'article',
        'h2 a[href*="/"]',
        'h3 a[href*="/"]', 
        '.story-promo',
    ],
    
    # Level 4: ë§í¬ ê¸°ë°˜ (ë§¤ìš° ê´€ëŒ€í•¨)
    'level4_links': [
        'a[href*="/news/"]',
        'a[href*="/sport/"]',
        'a[href*="/business/"]',
        'a[href*="/technology/"]',
    ],
    
    # Level 5: ìµœí›„ì˜ ìˆ˜ë‹¨ (ëª¨ë“  ë§í¬)
    'level5_emergency': [
        'a[href]',
    ]
}

# ğŸ¯ BBC ì„¹ì…˜ë³„ íŠ¹í™” ì„¤ì • (ë‹¨ìˆœí™”ë¨)
BBC_SECTION_CONFIG = {
    'news': {
        'expected_count': 15,
        'sub_sections': ['world', 'uk', 'politics', 'health', 'education'],
        'quality_threshold': 0.3,  # ë” ê´€ëŒ€í•¨
    },
    'sport': {
        'expected_count': 12,
        'sub_sections': ['football', 'cricket', 'tennis', 'golf', 'darts', 'rugby'],
        'quality_threshold': 0.2,  # ë§¤ìš° ê´€ëŒ€í•¨
    },
    'business': {
        'expected_count': 8,
        'sub_sections': ['economy', 'companies', 'markets'],
        'quality_threshold': 0.3,
    },
    'technology': {
        'expected_count': 6,
        'sub_sections': ['science', 'health'],
        'quality_threshold': 0.3,
    }
}

# ğŸš« ìµœì†Œí•œì˜ í•„í„°ë§ë§Œ (ì•ˆì •ì„± ìš°ì„ )
BBC_MINIMAL_FILTERS = {
    'min_title_length': 8,  # ë” ê´€ëŒ€í•¨
    'max_title_length': 300,  # ë” ê´€ëŒ€í•¨
    'exclude_exact_matches': [  # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²ƒë§Œ ì œì™¸
        'BBC', 'Home', 'Menu', 'Search', 'Sign in', 'Sport', 'News',
        'More', 'Live', 'Video', 'Audio', 'Weather', 'Travel'
    ]
}

# BBC URL íŒ¨í„´ ì •ì˜
BBC_URL_PATTERNS = {
    'main_sections': [
        r'bbc\.com/news',
        r'bbc\.com/sport', 
        r'bbc\.com/business',
        r'bbc\.com/technology',
        r'bbc\.co\.uk/news',
        r'bbc\.co\.uk/sport'
    ],
    'sport_subsections': [
        r'bbc\.com/sport/football',
        r'bbc\.com/sport/cricket', 
        r'bbc\.com/sport/tennis',
        r'bbc\.com/sport/golf',
        r'bbc\.com/sport/rugby-union',
        r'bbc\.com/sport/rugby-league',
        r'bbc\.com/sport/formula1',
        r'bbc\.com/sport/athletics',
        r'bbc\.com/sport/cycling',
        r'bbc\.com/sport/boxing',
        r'bbc\.com/sport/darts',
        r'bbc\.com/sport/snooker'
    ],
    'news_subsections': [
        r'bbc\.com/news/world',
        r'bbc\.com/news/uk', 
        r'bbc\.com/news/politics',
        r'bbc\.com/news/business',
        r'bbc\.com/news/health',
        r'bbc\.com/news/education',
        r'bbc\.com/news/science-environment',
        r'bbc\.com/news/technology',
        r'bbc\.com/news/entertainment-arts'
    ]
}

# ================================
# ğŸ›¡ï¸ ì•ˆì •ì„± ìš°ì„  BBC í¬ë¡¤ëŸ¬
# ================================

class StableBBCCrawler:
    """ì•ˆì •ì„±ì„ ìµœìš°ì„ ìœ¼ë¡œ í•˜ëŠ” BBC í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.session = None
        if AIOHTTP_AVAILABLE:
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=25),  # ë” ì—¬ìœ ë¡œìš´ íƒ€ì„ì•„ì›ƒ
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Connection': 'keep-alive',
                }
            )
        self.seen_titles = set()
        self.seen_urls = set()
        self.fallback_stats = {'level1': 0, 'level2': 0, 'level3': 0, 'level4': 0, 'level5': 0}
    
    async def crawl_with_maximum_stability(self, board_url: str, limit: int = 50,
                                         start_index: int = 1, end_index: int = 20,
                                         websocket=None) -> List[Dict]:
        """ìµœëŒ€ ì•ˆì •ì„± ë³´ì¥ í¬ë¡¤ë§"""
        
        start_time = time.time()
        all_articles = []
        
        # aiohttp ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸
        if not AIOHTTP_AVAILABLE or not self.session:
            error_msg = "BBC í¬ë¡¤ëŸ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. aiohttp ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: pip install aiohttp"
            logger.error(error_msg)
            raise Exception(error_msg)
        
        try:
            logger.info(f"ğŸ›¡ï¸ ì•ˆì •ì„± ìš°ì„  BBC í¬ë¡¤ë§ ì‹œì‘: {board_url}")
            
            # ğŸ”¥ 1ë‹¨ê³„: ë©”ì¸ í˜ì´ì§€ í¬ë¡¤ë§
            main_articles = await self._crawl_with_fallback_levels(board_url, websocket)
            all_articles.extend(main_articles)
            
            # ğŸ”¥ 2ë‹¨ê³„: ì„¸ë¶€ ì„¹ì…˜ ìë™ íƒì§€ ë° í¬ë¡¤ë§
            if len(all_articles) < (end_index - start_index + 1) * 2:  # ë¶€ì¡±í•˜ë©´ ì„¸ë¶€ ì„¹ì…˜ íƒìƒ‰
                sub_articles = await self._auto_discover_and_crawl_subsections(board_url, websocket)
                all_articles.extend(sub_articles)
            
            # ğŸ”¥ 3ë‹¨ê³„: ê´€ëŒ€í•œ í•„í„°ë§ ì ìš©
            filtered_articles = self._apply_minimal_filtering(all_articles)
            
            # ğŸ”¥ 4ë‹¨ê³„: ì •ë ¬ (í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œ)
            sorted_articles = self._simple_client_side_sort(filtered_articles)
            
            # ğŸ”¥ 5ë‹¨ê³„: ë²”ìœ„ ì ìš© ë° ë²ˆí˜¸ ë¶€ì—¬
            final_articles = self._apply_range_safely(sorted_articles, start_index, end_index)
            
            logger.info(f"âœ… ì•ˆì •ì„± ìš°ì„  í¬ë¡¤ë§ ì™„ë£Œ: {len(final_articles)}ê°œ")
            return final_articles
            
        except Exception as e:
            logger.error(f"ì•ˆì •ì„± í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            # ğŸš¨ ìµœí›„ì˜ ì‘ê¸‰ í¬ë¡¤ë§
            return await self._emergency_crawl(board_url, start_index, end_index)
    
    async def _crawl_with_fallback_levels(self, url: str, websocket=None) -> List[Dict]:
        """5ë‹¨ê³„ Fallback í¬ë¡¤ë§"""
        
        try:
            async with self.session.get(url) as response:
                if response.status != 200:
                    raise Exception(f"HTTP {response.status}")
                
                content = await response.text()
                soup = BeautifulSoup(content, 'html.parser')
                
                # Level 1: ìµœì‹  BBC ì»´í¬ë„ŒíŠ¸ ì‹œë„
                articles = await self._try_level1_extraction(soup, url)
                if len(articles) >= 3:
                    self.fallback_stats['level1'] = len(articles)
                    logger.info(f"âœ… Level 1 ì„±ê³µ: {len(articles)}ê°œ")
                    return articles
                
                # Level 2: ê²€ì¦ëœ ì„ íƒì
                articles = await self._try_level2_extraction(soup, url)
                if len(articles) >= 3:
                    self.fallback_stats['level2'] = len(articles)
                    logger.info(f"âœ… Level 2 ì„±ê³µ: {len(articles)}ê°œ")
                    return articles
                
                # Level 3: ì¼ë°˜ì ì¸ êµ¬ì¡°
                articles = await self._try_level3_extraction(soup, url)
                if len(articles) >= 2:
                    self.fallback_stats['level3'] = len(articles)
                    logger.info(f"âœ… Level 3 ì„±ê³µ: {len(articles)}ê°œ")
                    return articles
                
                # Level 4: ë§í¬ ê¸°ë°˜
                articles = await self._try_level4_extraction(soup, url)
                if len(articles) >= 1:
                    self.fallback_stats['level4'] = len(articles)
                    logger.info(f"âœ… Level 4 ì„±ê³µ: {len(articles)}ê°œ")
                    return articles
                
                # Level 5: ì‘ê¸‰ ëª¨ë“œ
                articles = await self._try_level5_extraction(soup, url)
                self.fallback_stats['level5'] = len(articles)
                logger.info(f"ğŸš¨ Level 5 ì‘ê¸‰ëª¨ë“œ: {len(articles)}ê°œ")
                return articles
                
        except Exception as e:
            logger.error(f"Fallback í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return []
    
    async def _try_level1_extraction(self, soup, base_url: str) -> List[Dict]:
        """Level 1: ìµœì‹  BBC ì»´í¬ë„ŒíŠ¸"""
        articles = []
        
        for selector in BBC_STABLE_SELECTORS['level1_primary']:
            try:
                containers = soup.select(selector)
                for container in containers[:15]:  # ì ë‹¹í•œ ì œí•œ
                    article = self._extract_from_container_safe(container, base_url, "Level1")
                    if article:
                        articles.append(article)
                        
                if len(articles) >= 5:  # ì¶©ë¶„íˆ ì°¾ì•˜ìœ¼ë©´ ì¤‘ë‹¨
                    break
                    
            except Exception as e:
                logger.debug(f"Level 1 ì„ íƒì '{selector}' ì‹¤íŒ¨: {e}")
                continue
        
        return articles
    
    async def _try_level2_extraction(self, soup, base_url: str) -> List[Dict]:
        """Level 2: ê²€ì¦ëœ ì„ íƒì"""
        articles = []
        
        for selector in BBC_STABLE_SELECTORS['level2_reliable']:
            try:
                containers = soup.select(selector)
                for container in containers[:20]:
                    article = self._extract_from_container_safe(container, base_url, "Level2")
                    if article:
                        articles.append(article)
                        
                if len(articles) >= 8:
                    break
                    
            except Exception as e:
                logger.debug(f"Level 2 ì„ íƒì '{selector}' ì‹¤íŒ¨: {e}")
                continue
        
        return articles
    
    async def _try_level3_extraction(self, soup, base_url: str) -> List[Dict]:
        """Level 3: ì¼ë°˜ì ì¸ êµ¬ì¡°"""
        articles = []
        
        for selector in BBC_STABLE_SELECTORS['level3_general']:
            try:
                containers = soup.select(selector)
                for container in containers[:30]:
                    article = self._extract_from_container_safe(container, base_url, "Level3")
                    if article:
                        articles.append(article)
                        
                if len(articles) >= 10:
                    break
                    
            except Exception as e:
                logger.debug(f"Level 3 ì„ íƒì '{selector}' ì‹¤íŒ¨: {e}")
                continue
        
        return articles
    
    async def _try_level4_extraction(self, soup, base_url: str) -> List[Dict]:
        """Level 4: ë§í¬ ê¸°ë°˜ (ê´€ëŒ€í•¨)"""
        articles = []
        
        for selector in BBC_STABLE_SELECTORS['level4_links']:
            try:
                links = soup.select(selector)
                for link in links[:50]:
                    title = link.get_text(strip=True)
                    href = link.get('href', '')
                    
                    if title and len(title) > BBC_MINIMAL_FILTERS['min_title_length']:
                        article = self._create_article_safe(title, href, base_url, "Level4")
                        if article:
                            articles.append(article)
                            
                if len(articles) >= 5:
                    break
                    
            except Exception as e:
                logger.debug(f"Level 4 ì„ íƒì '{selector}' ì‹¤íŒ¨: {e}")
                continue
        
        return articles
    
    async def _try_level5_extraction(self, soup, base_url: str) -> List[Dict]:
        """Level 5: ìµœí›„ì˜ ìˆ˜ë‹¨ (ëª¨ë“  ë§í¬)"""
        articles = []
        
        try:
            all_links = soup.find_all('a', href=True)
            
            for link in all_links[:100]:  # ìµœëŒ€ 100ê°œê¹Œì§€ë§Œ
                title = link.get_text(strip=True)
                href = link.get('href', '')
                
                # ë§¤ìš° ê¸°ë³¸ì ì¸ í•„í„°ë§ë§Œ
                if (title and 
                    len(title) >= BBC_MINIMAL_FILTERS['min_title_length'] and 
                    len(title) <= BBC_MINIMAL_FILTERS['max_title_length'] and
                    title not in BBC_MINIMAL_FILTERS['exclude_exact_matches']):
                    
                    article = self._create_article_safe(title, href, base_url, "Level5-Emergency")
                    if article:
                        articles.append(article)
                        
                if len(articles) >= 3:  # ìµœì†Œí•œë§Œ í™•ë³´
                    break
                    
        except Exception as e:
            logger.error(f"Level 5 ì‘ê¸‰ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return articles
    
    def _extract_from_container_safe(self, container, base_url: str, method: str) -> Optional[Dict]:
        """ì•ˆì „í•œ ì»¨í…Œì´ë„ˆ ì¶”ì¶œ"""
        try:
            # ì œëª© ì°¾ê¸° (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
            title = ""
            url = ""
            
            # ë°©ë²• 1: ë§í¬ í…ìŠ¤íŠ¸
            link = container.find('a', href=True)
            if link:
                title = link.get_text(strip=True)
                url = urljoin(base_url, link.get('href', ''))
            
            # ë°©ë²• 2: í—¤ë”© íƒœê·¸
            if not title:
                for tag in ['h1', 'h2', 'h3', 'h4', 'h5']:
                    heading = container.find(tag)
                    if heading:
                        title = heading.get_text(strip=True)
                        break
            
            # ë°©ë²• 3: ì»¨í…Œì´ë„ˆ ì „ì²´ í…ìŠ¤íŠ¸ (ì¶•ì•½)
            if not title:
                full_text = container.get_text(strip=True)
                if full_text:
                    title = full_text[:100] + "..." if len(full_text) > 100 else full_text
            
            if title:
                return self._create_article_safe(title, url, base_url, method, container)
                
        except Exception as e:
            logger.debug(f"ì»¨í…Œì´ë„ˆ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return None
    
    def _extract_bbc_datetime(self, container, base_url: str) -> str:
        """BBC íŠ¹í™” ë‚ ì§œ/ì‹œê°„ ì¶”ì¶œ í•¨ìˆ˜"""
        try:
            # BBC ë‚ ì§œ ì„ íƒìë“¤ (ìš°ì„ ìˆœìœ„ ìˆœ)
            date_selectors = [
                '[data-testid="timestamp"]',
                'time[datetime]',
                '.date',
                '.timestamp',
                '[datetime]',
                '.gel-body-copy time'
            ]
            
            for selector in date_selectors:
                date_elem = container.select_one(selector)
                if date_elem:
                    # datetime ì†ì„± ìš°ì„  í™•ì¸
                    if date_elem.get('datetime'):
                        return self._parse_bbc_datetime(date_elem.get('datetime'))
                    # í…ìŠ¤íŠ¸ ë‚´ìš© íŒŒì‹±
                    elif date_elem.get_text(strip=True):
                        return self._parse_bbc_datetime(date_elem.get_text(strip=True))
            
            # ê¸°ë³¸ê°’: í˜„ì¬ ì‹œê°„
            return datetime.now().strftime('%Y.%m.%d %H:%M')
        
        except Exception as e:
            logger.debug(f"BBC ë‚ ì§œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return datetime.now().strftime('%Y.%m.%d %H:%M')

    def _parse_bbc_datetime(self, date_str: str) -> str:
        """BBC ë‚ ì§œ í˜•ì‹ íŒŒì‹±"""
        try:
            # BBC ì¼ë°˜ì ì¸ í˜•ì‹ë“¤
            formats = [
                '%Y-%m-%dT%H:%M:%S.%fZ',  # ISO format
                '%Y-%m-%dT%H:%M:%SZ',     # ISO without microseconds
                '%d %B %Y',               # "11 June 2025"
                '%B %d, %Y',              # "June 11, 2025"
                '%d %b %Y',               # "11 Jun 2025"
                '%Y-%m-%d',               # "2025-06-11"
            ]
            
            for fmt in formats:
                try:
                    dt = datetime.strptime(date_str.strip(), fmt)
                    return dt.strftime('%Y.%m.%d %H:%M')
                except ValueError:
                    continue
            
            # ìƒëŒ€ ì‹œê°„ ì²˜ë¦¬ ("2 hours ago", "1 day ago" ë“±)
            if 'ago' in date_str.lower():
                return parse_relative_time(date_str)
            
            # íŒŒì‹± ì‹¤íŒ¨ì‹œ í˜„ì¬ ì‹œê°„
            return datetime.now().strftime('%Y.%m.%d %H:%M')
        
        except Exception:
            return datetime.now().strftime('%Y.%m.%d %H:%M')

    def _create_article_safe(self, title: str, url: str, base_url: str, method: str, container = None) -> Optional[Dict]:
        """ì•ˆì „í•œ ê¸°ì‚¬ ê°ì²´ ìƒì„±"""
        try:
            # ê¸°ë³¸ ê²€ì¦
            if not title or len(title) < BBC_MINIMAL_FILTERS['min_title_length']:
                return None
            
            if title in BBC_MINIMAL_FILTERS['exclude_exact_matches']:
                return None
            
            # ì¤‘ë³µ ê²€ì‚¬ (í•´ì‹œ ê¸°ë°˜)
            title_hash = hashlib.md5(title.encode()).hexdigest()
            if title_hash in self.seen_titles:
                return None
            self.seen_titles.add(title_hash)
            
            # URL ì •ê·œí™”
            if url:
                # ìƒëŒ€ URL ì²˜ë¦¬
                if url.startswith('/'):
                    parsed_base = urlparse(base_url)
                    url = f"{parsed_base.scheme}://{parsed_base.netloc}{url}"
                # í”„ë¡œí† ì½œ ì—†ëŠ” URL ì²˜ë¦¬
                elif not url.startswith('http'):
                    url = urljoin(base_url, url)
                # ì´ë¯¸ ì™„ì „í•œ URLì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
            else:
                # URLì´ ì—†ìœ¼ë©´ ê¸°ì‚¬ ì œëª©ìœ¼ë¡œ BBC ê²€ìƒ‰ URL ìƒì„±
                search_query = title.replace(' ', '+')
                url = f"https://www.bbc.com/search?q={search_query}"
            
            date_info = self._extract_bbc_datetime(container, base_url) if container else datetime.now().strftime('%Y.%m.%d %H:%M')
            
            # ê¸°ì‚¬ ê°ì²´ ìƒì„±
            article = {
                "ë²ˆí˜¸": 0,  # ë‚˜ì¤‘ì— ë¶€ì—¬
                "ì›ì œëª©": title,
                "ë²ˆì—­ì œëª©": None,  # ë²ˆì—­ì€ ë³„ë„ ì²˜ë¦¬
                "ë§í¬": url,
                "ì›ë¬¸URL": url,
                "ì¸ë„¤ì¼ URL": "",
                "ë³¸ë¬¸": "",
                "ì¡°íšŒìˆ˜": 0,
                "ì¶”ì²œìˆ˜": 0,
                "ëŒ“ê¸€ìˆ˜": 0,
                "ì‘ì„±ì¼": date_info,
                "ì‘ì„±ì": "BBC News",
                "ì‚¬ì´íŠ¸": "bbc.com",
                "ì½˜í…ì¸ íƒ€ì…": "news",
                "ì„¹ì…˜": self._detect_section_from_url(url or base_url),
                "í’ˆì§ˆì ìˆ˜": 5.0,  # ê¸°ë³¸ ì ìˆ˜
                "ì¶”ì¶œë°©ë²•": method,
                "í¬ë¡¤ë§ë°©ì‹": "BBC-Stable-MultiLevel",
                "ë¶„ë¥˜ì‹ ë¢°ë„": 0.9,
                "í‚¤ì›Œë“œ": ["bbc", "news"],
                "ê°ì •": "neutral"
            }
            
            return article
            
        except Exception as e:
            logger.debug(f"ê¸°ì‚¬ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    async def _auto_discover_and_crawl_subsections(self, main_url: str, websocket=None) -> List[Dict]:
        """ì„¸ë¶€ ì„¹ì…˜ ìë™ íƒì§€ ë° í¬ë¡¤ë§"""
        
        subsection_articles = []
        
        try:
            # URLì—ì„œ ì„¹ì…˜ ê°ì§€
            section = self._detect_section_from_url(main_url)
            section_config = BBC_SECTION_CONFIG.get(section, {})
            sub_sections = section_config.get('sub_sections', [])
            
            # ê° ì„¸ë¶€ ì„¹ì…˜ í¬ë¡¤ë§ ì‹œë„
            for idx, sub_section in enumerate(sub_sections[:3]):  # ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ
                try:
                    sub_url = self._construct_subsection_url(main_url, sub_section)
                    logger.info(f"ğŸ” ì„¸ë¶€ì„¹ì…˜ í¬ë¡¤ë§: {sub_url}")
                    
                    sub_articles = await self._crawl_with_fallback_levels(sub_url)
                    
                    # ì„¸ë¶€ì„¹ì…˜ í‘œì‹œ ì¶”ê°€
                    for article in sub_articles:
                        article['ì„¹ì…˜'] = f"{section}-{sub_section}"
                        article['ì¶”ì¶œë°©ë²•'] += f"-SubSection({sub_section})"
                    
                    subsection_articles.extend(sub_articles[:5])  # ê° ì„¹ì…˜ì—ì„œ ìµœëŒ€ 5ê°œ
                    
                    # ë„ˆë¬´ ë§ì´ ìˆ˜ì§‘í–ˆìœ¼ë©´ ì¤‘ë‹¨
                    if len(subsection_articles) >= 15:
                        break
                        
                except Exception as e:
                    logger.debug(f"ì„¸ë¶€ì„¹ì…˜ '{sub_section}' í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"ì„¸ë¶€ì„¹ì…˜ íƒìƒ‰ ì˜¤ë¥˜: {e}")
        
        logger.info(f"ì„¸ë¶€ì„¹ì…˜ í¬ë¡¤ë§ ì™„ë£Œ: {len(subsection_articles)}ê°œ")
        return subsection_articles
    
    def _construct_subsection_url(self, main_url: str, sub_section: str) -> str:
        """ì„¸ë¶€ì„¹ì…˜ URL ìƒì„±"""
        try:
            parsed = urlparse(main_url)
            base = f"{parsed.scheme}://{parsed.netloc}"
            
            # BBC URL íŒ¨í„´ì— ë”°ë¥¸ ì„¸ë¶€ì„¹ì…˜ URL ìƒì„±
            if '/sport' in main_url:
                return f"{base}/sport/{sub_section}"
            elif '/news' in main_url:
                return f"{base}/news/{sub_section}"
            elif '/business' in main_url:
                return f"{base}/business/{sub_section}"
            elif '/technology' in main_url:
                return f"{base}/technology"  # ê¸°ìˆ ì€ ì„¸ë¶„í™” ì•ˆë¨
            else:
                return f"{base}/{sub_section}"
                
        except Exception as e:
            logger.debug(f"ì„¸ë¶€ì„¹ì…˜ URL ìƒì„± ì‹¤íŒ¨: {e}")
            return main_url
    
    def _detect_section_from_url(self, url: str) -> str:
        """URLì—ì„œ ì„¹ì…˜ ê°ì§€"""
        if not url:
            return 'general'
        
        url_lower = url.lower()
        if '/sport' in url_lower:
            return 'sport'
        elif '/news' in url_lower:
            return 'news'
        elif '/business' in url_lower:
            return 'business'
        elif '/technology' in url_lower:
            return 'technology'
        else:
            return 'general'
    
    def _apply_minimal_filtering(self, articles: List[Dict]) -> List[Dict]:
        """ìµœì†Œí•œì˜ í•„í„°ë§ë§Œ ì ìš© (ì•ˆì •ì„± ìš°ì„ )"""
        
        filtered = []
        seen_titles = set()
        
        for article in articles:
            title = article.get('ì›ì œëª©', '')
            
            # ë§¤ìš° ê¸°ë³¸ì ì¸ í•„í„°ë§ë§Œ
            if (title and 
                len(title) >= BBC_MINIMAL_FILTERS['min_title_length'] and
                len(title) <= BBC_MINIMAL_FILTERS['max_title_length'] and
                title not in BBC_MINIMAL_FILTERS['exclude_exact_matches']):
                
                # ì¤‘ë³µ ì œê±°
                title_key = title.lower().strip()
                if title_key not in seen_titles:
                    seen_titles.add(title_key)
                    filtered.append(article)
        
        logger.info(f"ìµœì†Œ í•„í„°ë§: {len(articles)} â†’ {len(filtered)} ê¸°ì‚¬")
        return filtered
    
    def _simple_client_side_sort(self, articles: List[Dict]) -> List[Dict]:
        """ê°„ë‹¨í•œ í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œ ì •ë ¬ (BBCëŠ” ì‹¤ì œ ì •ë ¬ API ì—†ìŒ)"""
        
        try:
            # í’ˆì§ˆì ìˆ˜ì™€ ì¶”ì¶œë°©ë²• ê¸°ì¤€ìœ¼ë¡œ ì•ˆì •ì  ì •ë ¬
            def sort_key(article):
                quality = article.get('í’ˆì§ˆì ìˆ˜', 0)
                method = article.get('ì¶”ì¶œë°©ë²•', '')
                
                # ì¶”ì¶œë°©ë²•ë³„ ê°€ì¤‘ì¹˜
                method_weight = 0
                if 'Level1' in method:
                    method_weight = 5
                elif 'Level2' in method:
                    method_weight = 4
                elif 'Level3' in method:
                    method_weight = 3
                elif 'Level4' in method:
                    method_weight = 2
                else:
                    method_weight = 1
                
                return quality + method_weight
            
            sorted_articles = sorted(articles, key=sort_key, reverse=True)
            logger.info(f"í´ë¼ì´ì–¸íŠ¸ ì •ë ¬ ì™„ë£Œ: {len(sorted_articles)}ê°œ")
            
            return sorted_articles
            
        except Exception as e:
            logger.error(f"ì •ë ¬ ì˜¤ë¥˜: {e}")
            return articles  # ì •ë ¬ ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜
    
    def _apply_range_safely(self, articles: List[Dict], start_index: int, end_index: int) -> List[Dict]:
        """ì•ˆì „í•œ ë²”ìœ„ ì ìš©"""
        
        try:
            # ë²”ìœ„ ë³´ì •
            start_idx = max(0, start_index - 1)
            end_idx = min(len(articles), end_index)
            
            if start_idx >= len(articles):
                # ì‹œì‘ ì¸ë±ìŠ¤ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ ë§ˆì§€ë§‰ ëª‡ ê°œë¼ë„ ë°˜í™˜
                final_articles = articles[-3:] if len(articles) >= 3 else articles
            else:
                final_articles = articles[start_idx:end_idx]
            
            # ë²ˆí˜¸ ë¶€ì—¬
            for idx, article in enumerate(final_articles):
                article['ë²ˆí˜¸'] = start_index + idx
            
            logger.info(f"ë²”ìœ„ ì ìš©: {start_index}-{end_index} â†’ {len(final_articles)}ê°œ")
            return final_articles
            
        except Exception as e:
            logger.error(f"ë²”ìœ„ ì ìš© ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œ ì²˜ìŒ ëª‡ ê°œë¼ë„ ë°˜í™˜
            safe_articles = articles[:5] if len(articles) >= 5 else articles
            for idx, article in enumerate(safe_articles):
                article['ë²ˆí˜¸'] = idx + 1
            return safe_articles
    
    async def _emergency_crawl(self, url: str, start_index: int, end_index: int) -> List[Dict]:
        """ğŸš¨ ìµœí›„ì˜ ì‘ê¸‰ í¬ë¡¤ë§ (ë¬´ì¡°ê±´ ì„±ê³µ)"""
        
        emergency_articles = []
        
        try:
            logger.warning("ğŸš¨ ì‘ê¸‰ í¬ë¡¤ë§ ëª¨ë“œ í™œì„±í™”")
            
            # ë§¤ìš° ê°„ë‹¨í•œ ìš”ì²­ìœ¼ë¡œ ê¸°ë³¸ ì •ë³´ë¼ë„ ì¶”ì¶œ
            async with self.session.get(url) as response:
                if response.status == 200:
                    content = await response.text()
                    
                    # HTMLì—ì„œ title íƒœê·¸ë¼ë„ ì¶”ì¶œ
                    soup = BeautifulSoup(content, 'html.parser')
                    page_title = soup.find('title')
                    if page_title:
                        main_title = page_title.get_text(strip=True)
                        
                        # ê¸°ë³¸ ê¸°ì‚¬ ê°ì²´ ìƒì„±
                        emergency_article = {
                            "ë²ˆí˜¸": 1,
                            "ì›ì œëª©": f"BBC í˜ì´ì§€: {main_title}",
                            "ë²ˆì—­ì œëª©": None,
                            "ë§í¬": url,
                            "ì›ë¬¸URL": url,
                            "ì¸ë„¤ì¼ URL": "",
                            "ë³¸ë¬¸": "ìƒì„¸ ë‚´ìš©ì„ ë³´ë ¤ë©´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”.",
                            "ì¡°íšŒìˆ˜": 0,
                            "ì¶”ì²œìˆ˜": 0,
                            "ëŒ“ê¸€ìˆ˜": 0,
                            "ì‘ì„±ì¼": datetime.now().strftime('%Y.%m.%d %H:%M'),
                            "ì‘ì„±ì": "BBC News",
                            "ì‚¬ì´íŠ¸": "bbc.com",
                            "ì½˜í…ì¸ íƒ€ì…": "news",
                            "ì„¹ì…˜": self._detect_section_from_url(url),
                            "í’ˆì§ˆì ìˆ˜": 3.0,
                            "ì¶”ì¶œë°©ë²•": "Emergency-Mode",
                            "í¬ë¡¤ë§ë°©ì‹": "BBC-Emergency-Fallback",
                            "ë¶„ë¥˜ì‹ ë¢°ë„": 0.7,
                            "í‚¤ì›Œë“œ": ["bbc", "news", "emergency"],
                            "ê°ì •": "neutral"
                        }
                        
                        emergency_articles.append(emergency_article)
            
            # ìµœì†Œí•œì´ë¼ë„ ë°˜í™˜
            if not emergency_articles:
                # ì •ë§ ìµœí›„ì˜ ìˆ˜ë‹¨: í•˜ë“œì½”ë”©ëœ ê¸°ë³¸ ê¸°ì‚¬
                default_article = {
                    "ë²ˆí˜¸": 1,
                    "ì›ì œëª©": "BBC News - ì½˜í…ì¸ ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    "ë²ˆì—­ì œëª©": None,
                    "ë§í¬": url,
                    "ì›ë¬¸URL": url,
                    "ì¸ë„¤ì¼ URL": "",
                    "ë³¸ë¬¸": "í˜„ì¬ BBC ë‰´ìŠ¤ ì½˜í…ì¸ ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§ì ‘ ë§í¬ë¥¼ ë°©ë¬¸í•´ ì£¼ì„¸ìš”.",
                    "ì¡°íšŒìˆ˜": 0,
                    "ì¶”ì²œìˆ˜": 0,
                    "ëŒ“ê¸€ìˆ˜": 0,
                    "ì‘ì„±ì¼": datetime.now().strftime('%Y.%m.%d %H:%M'),
                    "ì‘ì„±ì": "BBC News",
                    "ì‚¬ì´íŠ¸": "bbc.com",
                    "ì½˜í…ì¸ íƒ€ì…": "news",
                    "ì„¹ì…˜": "general",
                    "í’ˆì§ˆì ìˆ˜": 1.0,
                    "ì¶”ì¶œë°©ë²•": "Hardcoded-Fallback",
                    "í¬ë¡¤ë§ë°©ì‹": "BBC-Absolute-Emergency",
                    "ë¶„ë¥˜ì‹ ë¢°ë„": 0.5,
                    "í‚¤ì›Œë“œ": ["bbc", "news", "fallback"],
                    "ê°ì •": "neutral"
                }
                emergency_articles.append(default_article)
            
            logger.warning(f"ğŸš¨ ì‘ê¸‰ í¬ë¡¤ë§ ì™„ë£Œ: {len(emergency_articles)}ê°œ")
            return emergency_articles
            
        except Exception as e:
            logger.error(f"ì‘ê¸‰ í¬ë¡¤ë§ë§ˆì € ì‹¤íŒ¨: {e}")
            # ì •ë§ ìµœí›„ì˜ ìˆ˜ë‹¨: ë¹ˆ ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹  ê¸°ë³¸ ë©”ì‹œì§€
            return [{
                "ë²ˆí˜¸": 1,
                "ì›ì œëª©": "í¬ë¡¤ë§ ì‹¤íŒ¨ - BBC ì‚¬ì´íŠ¸ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                "ë²ˆì—­ì œëª©": None,
                "ë§í¬": url,
                "ì›ë¬¸URL": url,
                "ì¸ë„¤ì¼ URL": "",
                "ë³¸ë¬¸": f"ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ BBC ì‚¬ì´íŠ¸ í¬ë¡¤ë§ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {str(e)}",
                "ì¡°íšŒìˆ˜": 0,
                "ì¶”ì²œìˆ˜": 0,
                "ëŒ“ê¸€ìˆ˜": 0,
                "ì‘ì„±ì¼": datetime.now().strftime('%Y.%m.%d %H:%M'),
                "ì‘ì„±ì": "System",
                "ì‚¬ì´íŠ¸": "bbc.com",
                "ì½˜í…ì¸ íƒ€ì…": "error",
                "ì„¹ì…˜": "error",
                "í’ˆì§ˆì ìˆ˜": 0.0,
                "ì¶”ì¶œë°©ë²•": "Error-Fallback",
                "í¬ë¡¤ë§ë°©ì‹": "BBC-Error-Handler",
                "ë¶„ë¥˜ì‹ ë¢°ë„": 0.1,
                "í‚¤ì›Œë“œ": ["error", "fallback"],
                "ê°ì •": "neutral"
            }]
    
    async def __aenter__(self):
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

# ================================
# ğŸ›¡ï¸ ë©”ì¸ í•¨ìˆ˜ - ì•ˆì •ì„± ê·¹ëŒ€í™”
# ================================

async def crawl_bbc_board(board_url: str, limit: int = 50, sort: str = "recent",
                         min_views: int = 0, min_likes: int = 0, min_comments: int = 0,
                         time_filter: str = "all", start_date: str = None, 
                         end_date: str = None, websocket=None, board_name: str = "",
                         enforce_date_limit: bool = False, start_index: int = 1, 
                         end_index: int = 20) -> List[Dict]:
    """ì•ˆì •ì„± ê·¹ëŒ€í™” BBC í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜"""
    
    start_time = time.time()
    
    # aiohttp ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸
    if not AIOHTTP_AVAILABLE:
        error_msg = "BBC í¬ë¡¤ëŸ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. aiohttp ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: pip install aiohttp"
        logger.error(error_msg)
        raise Exception(error_msg)
    
    async with StableBBCCrawler() as crawler:
        try:
            logger.info(f"ğŸ›¡ï¸ ì•ˆì •ì„± ìš°ì„  BBC í¬ë¡¤ë§ ì‹œì‘: {board_url}")
            
            # BBC ì‚¬ì´íŠ¸ í™•ì¸ (ê´€ëŒ€í•˜ê²Œ)
            if 'bbc' not in board_url.lower():
                logger.warning("BBC ì‚¬ì´íŠ¸ê°€ ì•„ë‹ ìˆ˜ ìˆì§€ë§Œ í¬ë¡¤ë§ ì‹œë„")
            
            # ğŸ›¡ï¸ ìµœëŒ€ ì•ˆì •ì„± í¬ë¡¤ë§ ì‹¤í–‰
            articles = await crawler.crawl_with_maximum_stability(
                board_url, limit, start_index, end_index, websocket
            )
            
            # ğŸ“… ë‚ ì§œ í•„í„°ë§ (ì„ íƒì , ì‹¤íŒ¨í•´ë„ ê³„ì†)
            if start_date and end_date and articles:
                try:
                    original_count = len(articles)
                    articles = filter_bbc_by_date_safe(articles, start_date, end_date)
                        
                except Exception as e:
                    logger.warning(f"ë‚ ì§œ í•„í„°ë§ ì‹¤íŒ¨í•˜ì§€ë§Œ ê³„ì† ì§„í–‰: {e}")
                    # ë‚ ì§œ í•„í„°ë§ ì‹¤íŒ¨í•´ë„ ì›ë³¸ articles ìœ ì§€
            
            logger.info(f"âœ… BBC ì•ˆì •ì„± í¬ë¡¤ë§ ì„±ê³µ: {len(articles)}ê°œ")
            return articles
            
        except Exception as e:
            logger.error(f"BBC í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            
            # ğŸš¨ ì˜¤ë¥˜ ë°œìƒì‹œì—ë„ ìµœì†Œí•œì˜ ê²°ê³¼ ì œê³µ
            emergency_result = await crawler._emergency_crawl(board_url, start_index, end_index)
            
            return emergency_result

def filter_bbc_by_date_safe(articles: List[Dict], start_date: str, end_date: str) -> List[Dict]:
    """ì•ˆì „í•œ BBC ë‚ ì§œ í•„í„°ë§ (ì‹¤íŒ¨í•´ë„ ì›ë³¸ ë°˜í™˜)"""
    
    try:
        from datetime import datetime
        
        # ê´€ëŒ€í•œ ë‚ ì§œ íŒŒì‹±
        start_dt = datetime.strptime(start_date, '%Y-%m-%d')
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')
        end_dt = end_dt.replace(hour=23, minute=59, second=59)
        
        filtered = []
        
        for article in articles:
            try:
                date_str = article.get('ì‘ì„±ì¼', '')
                
                # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ì‹œ í¬í•¨ (ê´€ëŒ€í•¨)
                if not date_str:
                    filtered.append(article)
                    continue
                
                # ê°„ë‹¨í•œ ë‚ ì§œ í˜•ì‹ë§Œ ì²˜ë¦¬
                if re.match(r'\d{4}\.\d{2}\.\d{2}', date_str):
                    date_part = date_str.split()[0]
                    article_date = datetime.strptime(date_part, '%Y.%m.%d')
                    
                    if start_dt <= article_date <= end_dt:
                        filtered.append(article)
                else:
                    # íŒŒì‹± ë¶ˆê°€ëŠ¥í•˜ë©´ í¬í•¨ (ì•ˆì „)
                    filtered.append(article)
                    
            except Exception:
                # ê°œë³„ ê¸°ì‚¬ ë‚ ì§œ ì²˜ë¦¬ ì‹¤íŒ¨ì‹œ í¬í•¨
                filtered.append(article)
                continue
        
        logger.info(f"ì•ˆì „ ë‚ ì§œ í•„í„°ë§: {len(articles)} â†’ {len(filtered)}")
        return filtered if filtered else articles  # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
        
    except Exception as e:
        logger.warning(f"ë‚ ì§œ í•„í„°ë§ ì™„ì „ ì‹¤íŒ¨, ì›ë³¸ ë°˜í™˜: {e}")
        return articles

# ================================
# ğŸ”§ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ë‹¨ìˆœí™”)
# ================================

def parse_bbc_article(url: str) -> Dict:
    """ê°„ë‹¨í•œ BBC ê¸°ì‚¬ íŒŒì‹± (ì‹¤íŒ¨ ë°©ì§€)"""
    try:
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        response = session.get(url, timeout=20)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë§¤ìš° ê°„ë‹¨í•œ ì œëª© ì¶”ì¶œ
        title = ""
        title_selectors = ['h1', 'title', '[data-testid="headline"]']
        for selector in title_selectors:
            elem = soup.select_one(selector)
            if elem:
                title = elem.get_text(strip=True)
                break
        
        # ë§¤ìš° ê°„ë‹¨í•œ ë³¸ë¬¸ ì¶”ì¶œ
        content = ""
        content_selectors = ['[data-component="text-block"]', 'p', '.story-body']
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                content = '\n'.join([elem.get_text(strip=True) for elem in elements[:3]])
                break
        
        return {
            "title": title or "BBC ê¸°ì‚¬",
            "content": content or "ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "meta": {"author": "BBC News"},
            "url": url,
            "source": "BBC News Stable"
        }
        
    except Exception as e:
        logger.error(f"BBC ê¸°ì‚¬ íŒŒì‹± ì‹¤íŒ¨: {e}")
        return {
            "title": "íŒŒì‹± ì‹¤íŒ¨",
            "content": f"ê¸°ì‚¬ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§ì ‘ ë§í¬ë¥¼ ë°©ë¬¸í•´ ì£¼ì„¸ìš”: {url}",
            "meta": {"error": str(e)},
            "url": url,
            "source": "BBC News Stable"
        }

def is_bbc_url(url: str) -> bool:
    """BBC URL ì—¬ë¶€ í™•ì¸ (í–¥ìƒëœ ê°ì§€)"""
    if not url:
        return False
    
    url_lower = url.lower()
    
    # ê¸°ë³¸ BBC ë„ë©”ì¸ ì²´í¬
    if 'bbc.com' in url_lower or 'bbc.co.uk' in url_lower:
        return True
    
    # íŒ¨í„´ ê¸°ë°˜ ì²´í¬
    all_patterns = (BBC_URL_PATTERNS['main_sections'] + 
                   BBC_URL_PATTERNS['sport_subsections'] + 
                   BBC_URL_PATTERNS['news_subsections'])
    
    for pattern in all_patterns:
        if re.search(pattern, url_lower):
            return True
    
    return False

# ëª¨ë“ˆ ì •ë³´ (ë™ì  íƒì§€ë¥¼ ìœ„í•œ ë©”íƒ€ë°ì´í„°)
DISPLAY_NAME = "BBC Crawler"
DESCRIPTION = "BBC ë‰´ìŠ¤ ë° ìŠ¤í¬ì¸  í¬ë¡¤ëŸ¬"
VERSION = "2.0.0"
SUPPORTED_DOMAINS = ["bbc.com", "www.bbc.com", "bbc.co.uk", "www.bbc.co.uk"]
KEYWORDS = ["bbc", "british broadcasting"]

# ëª¨ë“ˆ ë¡œë“œ ì‹œ ì´ˆê¸°í™”
logger.info("ğŸ›¡ï¸ ì•ˆì •ì„± ìš°ì„  BBC í¬ë¡¤ëŸ¬ v2.0 ë¡œë“œ ì™„ë£Œ")
logger.info(f"ğŸ“Š ì§€ì› ì„¹ì…˜: {len(BBC_SECTION_CONFIG)}ê°œ")
logger.info(f"ğŸ¯ Fallback ë ˆë²¨: {len(BBC_STABLE_SELECTORS)}ë‹¨ê³„")
if not AIOHTTP_AVAILABLE:
    logger.warning("âš ï¸ aiohttpê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. BBC í¬ë¡¤ëŸ¬ê°€ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")