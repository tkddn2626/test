# reddit.py - ìˆœìˆ˜ í¬ë¡¤ë§ ë¡œì§ë§Œ (ë©”ì‹œì§€ ì²˜ë¦¬ëŠ” main.pyì—ì„œ)

import time
import logging
import requests
from datetime import datetime
import os
import re
from typing import Tuple, Optional
from dotenv import load_dotenv

# praw ì„í¬íŠ¸ë¥¼ try-exceptë¡œ ë³´í˜¸
try:
    import praw
    PRAW_AVAILABLE = True
except ImportError:
    PRAW_AVAILABLE = False
    praw = None
    logging.warning("praw ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install prawë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê·¸ ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

# Reddit API ì„¤ì •
reddit = None
if PRAW_AVAILABLE:
    try:
        reddit = praw.Reddit(
            client_id=os.getenv("REDDIT_CLIENT_ID"),
            client_secret=os.getenv("REDDIT_CLIENT_SECRET"),
            user_agent=os.getenv("REDDIT_USER_AGENT", "Community Crawler/1.0"),
        )
        reddit.user.me()
        logging.info("Reddit API ì—°ê²° ì„±ê³µ")
    except Exception as e:
        logging.error(f"Reddit API ì—°ê²° ì‹¤íŒ¨: {e}")
        print("âš ï¸  Reddit API ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”. .env íŒŒì¼ì— ì˜¬ë°”ë¥¸ API í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        try:
            reddit = praw.Reddit(
                client_id="CKTo0QVk-WwjAFuqn4s4eA",
                client_secret="xTHOniOa516bvOnyvbluzCl7Xff-3g",
                user_agent="TIL Excel Crawler/0.1 by u/PerspectivePutrid665",
            )
        except Exception as fallback_error:
            logging.error(f"í´ë°± Reddit API ì„¤ì •ë„ ì‹¤íŒ¨: {fallback_error}")
            reddit = None

def handle_reddit_errors(e, subreddit_name):
    """Reddit íŠ¹í™” ì—ëŸ¬ ì²˜ë¦¬"""
    msg = str(e)
    if "received 403 HTTP response" in msg:
        raise Exception(f"ì„œë¸Œë ˆë”§ r/{subreddit_name}ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹„ê³µê°œì´ê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    elif "received 404 HTTP response" in msg:
        raise Exception(f"ì„œë¸Œë ˆë”§ r/{subreddit_name}ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    elif "Redirect to login page" in msg:
        raise Exception("Reddit API ì¸ì¦ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. .env íŒŒì¼ì˜ API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    else:
        raise Exception(f"Reddit í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {msg}")

class RedditConditionChecker:
    def __init__(self, min_views: int = 0, min_likes: int = 0, min_comments: int = 0,
                 start_date: str = None, end_date: str = None):
        self.min_views = min_views
        self.min_likes = min_likes
        self.min_comments = min_comments
        self.start_dt = self._parse_date(start_date)
        self.end_dt = self._parse_date(end_date)

    def _parse_date(self, date_str: str):
        if not date_str:
            return None
        try:
            return datetime.strptime(date_str, '%Y-%m-%d')
        except Exception:
            return None

    def check_post_conditions(self, post) -> Tuple[bool, str]:
        """Reddit ê²Œì‹œë¬¼ ì¡°ê±´ ê²€ì‚¬"""
        views = post.num_comments
        likes = max(0, post.score)
        comments = post.num_comments

        if views < self.min_views:
            return False, f"ì¡°íšŒìˆ˜ ë¶€ì¡±: {views} < {self.min_views}"
        if likes < self.min_likes:
            return False, f"ì¶”ì²œìˆ˜ ë¶€ì¡±: {likes} < {self.min_likes}"
        if comments < self.min_comments:
            return False, f"ëŒ“ê¸€ìˆ˜ ë¶€ì¡±: {comments} < {self.min_comments}"

        # ë‚ ì§œ ê²€ì‚¬
        if self.start_dt and self.end_dt:
            post_date = datetime.fromtimestamp(post.created_utc)
            if not (self.start_dt <= post_date <= (self.end_dt.replace(hour=23, minute=59, second=59))):
                return False, "ë‚ ì§œ ë²”ìœ„ ë²—ì–´ë‚¨"

        return True, "ì¡°ê±´ ë§Œì¡±"

def format_reddit_date(timestamp):
    """Reddit íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ë³´ê¸° ì¢‹ì€ ë‚ ì§œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    try:
        dt = datetime.fromtimestamp(timestamp)
        return dt.strftime('%Y.%m.%d %H:%M')
    except Exception as e:
        logging.error(f"Date formatting error: {e}")
        return ""

def extract_reddit_media_urls(post) -> Tuple[Optional[str], Optional[str]]:
    """
    Reddit ê²Œì‹œë¬¼ì—ì„œ ì¸ë„¤ì¼ URLê³¼ ì›ë³¸ ì´ë¯¸ì§€/ë¹„ë””ì˜¤ URLì„ ì¶”ì¶œ
    4chan.py ìŠ¤íƒ€ì¼ë¡œ êµ¬í˜„ - ì¸ë„¤ì¼ê³¼ ì›ë³¸ì„ ë¶„ë¦¬
    
    Returns:
        Tuple[ì¸ë„¤ì¼_URL, ì›ë³¸_ì´ë¯¸ì§€_URL]
    """
    thumbnail_url = None
    original_image_url = None
    
    try:
        # 1. ì¸ë„¤ì¼ URL ì¶”ì¶œ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
        if hasattr(post, 'thumbnail') and post.thumbnail.startswith('http'):
            thumbnail_url = post.thumbnail
        
        # 2. ì›ë³¸ ì´ë¯¸ì§€/ë¹„ë””ì˜¤ URL ì¶”ì¶œ (ìƒˆë¡œìš´ ë°©ì‹)
        
        # 2-1. Reddit í˜¸ìŠ¤íŒ… ì´ë¯¸ì§€ (i.redd.it)
        if hasattr(post, 'url') and post.url:
            url = post.url
            
            # Reddit ì´ë¯¸ì§€ ë„ë©”ì¸ë“¤
            reddit_image_domains = ['i.redd.it', 'i.imgur.com', 'imgur.com']
            reddit_video_domains = ['v.redd.it']
            
            # ì§ì ‘ ì´ë¯¸ì§€ URLì¸ ê²½ìš°
            if any(domain in url for domain in reddit_image_domains):
                if url.endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp')):
                    original_image_url = url
                elif 'imgur.com' in url and not url.endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp')):
                    # Imgur ë§í¬ë¥¼ ì§ì ‘ ì´ë¯¸ì§€ URLë¡œ ë³€í™˜
                    if '/a/' not in url:  # ì•¨ë²”ì´ ì•„ë‹Œ ê²½ìš°
                        img_id = url.split('/')[-1].split('.')[0]
                        original_image_url = f"https://i.imgur.com/{img_id}.jpg"
            
            # Reddit ë¹„ë””ì˜¤ì¸ ê²½ìš°
            elif any(domain in url for domain in reddit_video_domains):
                original_image_url = url  # ë¹„ë””ì˜¤ URL ê·¸ëŒ€ë¡œ ì‚¬ìš©
        
        # 2-2. Preview ì´ë¯¸ì§€ì—ì„œ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ì¶”ì¶œ
        if not original_image_url and hasattr(post, 'preview'):
            try:
                preview_images = post.preview.get('images', [])
                if preview_images:
                    # ê°€ì¥ ë†’ì€ í•´ìƒë„ì˜ ì´ë¯¸ì§€ ì„ íƒ
                    resolutions = preview_images[0].get('resolutions', [])
                    source = preview_images[0].get('source', {})
                    
                    # ì›ë³¸ ì´ë¯¸ì§€ URL (ìµœê³  í•´ìƒë„)
                    if source and 'url' in source:
                        original_image_url = source['url'].replace('&amp;', '&')
                    # í•´ìƒë„ë³„ ì´ë¯¸ì§€ ì¤‘ ê°€ì¥ í° ê²ƒ ì„ íƒ
                    elif resolutions:
                        highest_res = max(resolutions, key=lambda x: x.get('width', 0) * x.get('height', 0))
                        original_image_url = highest_res['url'].replace('&amp;', '&')
            except Exception as e:
                logging.debug(f"Preview ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # 2-3. Gallery ì´ë¯¸ì§€ ì²˜ë¦¬ (Reddit ê°¤ëŸ¬ë¦¬)
        if not original_image_url and hasattr(post, 'is_gallery') and post.is_gallery:
            try:
                if hasattr(post, 'media_metadata'):
                    # ì²« ë²ˆì§¸ ê°¤ëŸ¬ë¦¬ ì´ë¯¸ì§€ ì‚¬ìš©
                    for media_id, media_data in post.media_metadata.items():
                        if 's' in media_data and 'u' in media_data['s']:
                            original_image_url = media_data['s']['u'].replace('&amp;', '&')
                            break
            except Exception as e:
                logging.debug(f"ê°¤ëŸ¬ë¦¬ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # 2-4. Reddit ë¹„ë””ì˜¤ ì²˜ë¦¬
        if not original_image_url and hasattr(post, 'is_video') and post.is_video:
            try:
                if hasattr(post, 'media') and post.media:
                    reddit_video = post.media.get('reddit_video', {})
                    if 'fallback_url' in reddit_video:
                        original_image_url = reddit_video['fallback_url']
                    elif 'dash_url' in reddit_video:
                        original_image_url = reddit_video['dash_url']
            except Exception as e:
                logging.debug(f"Reddit ë¹„ë””ì˜¤ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # 3. ì¸ë„¤ì¼ì´ ì—†ëŠ” ê²½ìš° ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ì¸ë„¤ì¼ë¡œ ì‚¬ìš© (4chan ë°©ì‹)
        if not thumbnail_url and original_image_url:
            # ì´ë¯¸ì§€ì¸ ê²½ìš°ì—ë§Œ ì¸ë„¤ì¼ë¡œ ì‚¬ìš©
            if any(ext in original_image_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                thumbnail_url = original_image_url
        
        # 4. ë¡œê¹… (ë””ë²„ê¹…ìš©)
        if original_image_url:
            logging.debug(f"ì›ë³¸ ì´ë¯¸ì§€ URL ì¶”ì¶œ ì„±ê³µ: {original_image_url}")
        if thumbnail_url:
            logging.debug(f"ì¸ë„¤ì¼ URL ì¶”ì¶œ ì„±ê³µ: {thumbnail_url}")
            
    except Exception as e:
        logging.error(f"ë¯¸ë””ì–´ URL ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    return thumbnail_url, original_image_url

def get_reddit_media_type(post) -> str:
    """Reddit ê²Œì‹œë¬¼ì˜ ë¯¸ë””ì–´ íƒ€ì… íŒë³„"""
    try:
        if hasattr(post, 'is_video') and post.is_video:
            return "video"
        elif hasattr(post, 'is_gallery') and post.is_gallery:
            return "gallery"
        elif hasattr(post, 'url') and post.url:
            url = post.url.lower()
            if any(ext in url for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                return "image"
            elif any(ext in url for ext in ['.mp4', '.webm', '.mov']):
                return "video"
            elif 'v.redd.it' in url:
                return "video"
            elif any(domain in url for domain in ['imgur.com', 'i.imgur.com']):
                return "image"
        return "text"
    except Exception:
        return "text"

async def fetch_posts(subreddit_name, limit=50, sort='top', time_filter='day', websocket=None, 
                     cancel_event=None, min_views=0, min_likes=0, min_comments=0,
                     start_date=None, end_date=None, enforce_date_limit=False, 
                     start_index=1, end_index=20):
    """
    Reddit ê²Œì‹œë¬¼ í¬ë¡¤ë§ í•¨ìˆ˜ - ìˆœìˆ˜ í¬ë¡¤ë§ ë¡œì§ë§Œ
    """
    start_time = time.time()
    
    # praw ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸
    if not PRAW_AVAILABLE or reddit is None:
        error_msg = "Reddit í¬ë¡¤ëŸ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. praw ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ê³  API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”."
        logging.error(error_msg)
        raise Exception(error_msg)
    
    try:
        logging.info(f"Reddit í¬ë¡¤ë§ ì‹œì‘: r/{subreddit_name} (ë²”ìœ„: {start_index}-{end_index})")

        subreddit = reddit.subreddit(subreddit_name)

        # ìˆ˜ì§‘ ì „ëµ ê²°ì •
        if enforce_date_limit and start_date and end_date:
            fetch_limit = 2000
            actual_time_filter = 'all'
        else:
            if min_views > 0 or min_likes > 0 or min_comments > 0:
                fetch_limit = end_index * 3
            else:
                fetch_limit = end_index + 10
            actual_time_filter = time_filter

        # Reddit API í˜¸ì¶œ
        if sort == 'top':
            posts_generator = subreddit.top(time_filter=actual_time_filter, limit=fetch_limit)
        elif sort == 'hot':
            posts_generator = subreddit.hot(limit=fetch_limit)
        elif sort == 'new':
            posts_generator = subreddit.new(limit=fetch_limit)
        elif sort == 'rising':
            posts_generator = subreddit.rising(limit=fetch_limit)
        elif sort == 'best':
            posts_generator = subreddit.best(limit=fetch_limit)
        else:
            posts_generator = subreddit.top(time_filter=actual_time_filter, limit=fetch_limit)

        all_posts = list(posts_generator)
        logging.info(f"ì´ˆê¸° ìˆ˜ì§‘: {len(all_posts)}ê°œ from r/{subreddit_name}")

        # ì¡°ê±´ ê²€ì‚¬ê¸° ìƒì„±
        condition_checker = RedditConditionChecker(
            min_views=min_views,
            min_likes=min_likes,
            min_comments=min_comments,
            start_date=start_date,
            end_date=end_date
        )

        matched_posts = []
        consecutive_fails = 0
        target_count = end_index - start_index + 1

        for i, post in enumerate(all_posts):
            if cancel_event and cancel_event.is_set():
                break

            is_valid, reason = condition_checker.check_post_conditions(post)
            if is_valid:
                matched_posts.append(post)
                consecutive_fails = 0
                if len(matched_posts) >= target_count and not enforce_date_limit:
                    break
            else:
                consecutive_fails += 1
                if consecutive_fails >= 20:
                    break

        logging.info(f"ì¡°ê±´ í•„í„°ë§ í›„: {len(matched_posts)}ê°œ ë§¤ì¹­ë¨")

        # ìµœì¢… ë²”ìœ„ ì ìš©
        if not enforce_date_limit:
            final_posts = matched_posts[start_index-1:end_index] if len(matched_posts) >= start_index else []
        else:
            final_posts = matched_posts

        data = []
        total_posts = len(final_posts)

        if total_posts == 0:
            logging.warning("í•„í„°ë§ í›„ ê²Œì‹œë¬¼ì´ ì—†ìŒ")
            return []

        # ê²Œì‹œë¬¼ ë°ì´í„° êµ¬ì„±
        for idx, post in enumerate(final_posts, start=1):
            if cancel_event and cancel_event.is_set():
                break
            
            try:
                title = post.title
                original_url = post.url if post.url.startswith("http") else f"https://reddit.com{post.permalink}"
                reddit_url = f"https://reddit.com{post.permalink}"

                # ğŸ”¥ í–¥ìƒëœ ë¯¸ë””ì–´ URL ì¶”ì¶œ (4chan ë°©ì‹ ì ìš©)
                thumbnail_url, original_image_url = extract_reddit_media_urls(post)
                
                # ë¯¸ë””ì–´ íƒ€ì… íŒë³„
                media_type = get_reddit_media_type(post)

                created_date = format_reddit_date(post.created_utc)
                score = max(0, post.score)
                num_comments = post.num_comments

                # ì‘ì„±ì ì •ë³´
                try:
                    author = str(post.author) if post.author else "[deleted]"
                except:
                    author = "[deleted]"

                # ê²Œì‹œë¬¼ í”Œë ˆì–´ ì¶”ì¶œ
                flair = ""
                if hasattr(post, 'link_flair_text') and post.link_flair_text:
                    flair = post.link_flair_text

                post_id = post.id

                # ë³¸ë¬¸ ë‚´ìš© êµ¬ì„± - ì‘ì„±ìì™€ ë¯¸ë””ì–´ íƒ€ì… í‘œì‹œ
                content_parts = [f"Author: {author}"]
                if media_type != "text":
                    content_parts.append(f"Media: {media_type}")

                is_external_link = original_url != reddit_url and not original_url.startswith('https://reddit.com')

                # íŒŒì¼ ì •ë³´ ì¶”ê°€ (4chan ìŠ¤íƒ€ì¼)
                file_info = {}
                if original_image_url:
                    file_info.update({
                        "íŒŒì¼URL": original_image_url,
                        "ë¯¸ë””ì–´íƒ€ì…": media_type
                    })

                data_entry = {
                    "ë²ˆí˜¸": start_index + idx - 1,
                    "ì›ì œëª©": title,
                    "ë²ˆì—­ì œëª©": None,
                    "ë§í¬": reddit_url,
                    "ì›ë¬¸URL": original_url if is_external_link else reddit_url,
                    "ì¸ë„¤ì¼ URL": thumbnail_url,
                    "ì´ë¯¸ì§€ URL": original_image_url,  # ğŸ”¥ 4chan ìŠ¤íƒ€ì¼ ì›ë³¸ ì´ë¯¸ì§€ URL
                    "ë³¸ë¬¸": " | ".join(content_parts),
                    "ì¡°íšŒìˆ˜": num_comments,
                    "ì¶”ì²œìˆ˜": score,
                    "ëŒ“ê¸€ìˆ˜": num_comments,
                    "ì‘ì„±ì¼": created_date,
                    "ì‘ì„±ì": author,
                    "í”Œë ˆì–´": flair,
                    "ê²Œì‹œë¬¼ID": post_id,
                    "ì„œë¸Œë ˆë”§": subreddit_name,
                    "upvotes": post.ups if hasattr(post, 'ups') else score,
                    "downvotes": post.downs if hasattr(post, 'downs') else 0,
                    "upvote_ratio": post.upvote_ratio if hasattr(post, 'upvote_ratio') else 0,
                    "nsfw": post.over_18 if hasattr(post, 'over_18') else False,
                    "stickied": post.stickied if hasattr(post, 'stickied') else False,
                    "ë¯¸ë””ì–´íƒ€ì…": media_type,  # ğŸ”¥ ìƒˆë¡œìš´ í•„ë“œ
                    "ì •ë ¬ë°©ì‹": sort,
                    "ì‹œê°„í•„í„°": time_filter,
                    "í¬ë¡¤ë§ë°©ì‹": "Reddit-Enhanced-API-v2",  # ğŸ”¥ ë²„ì „ ì—…ê·¸ë ˆì´ë“œ
                    "í”Œë«í¼": "reddit"  # ğŸ”¥ 4chan ìŠ¤íƒ€ì¼ í”Œë«í¼ í‘œì‹œ
                }

                # íŒŒì¼ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€
                data_entry.update(file_info)

                data.append(data_entry)

                time.sleep(0.1)

            except Exception as e:
                logging.error(f"ê²Œì‹œë¬¼ ì²˜ë¦¬ ì˜¤ë¥˜ {idx}: {e}")
                continue

        elapsed_time = time.time() - start_time
        logging.info(f"Reddit í¬ë¡¤ë§ ì™„ë£Œ: {len(data)}ê°œ ê²Œì‹œë¬¼ ({start_index}-{start_index+len(data)-1}ìœ„)")

        return data

    except Exception as e:
        logging.error(f"Reddit í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        handle_reddit_errors(e, subreddit_name)

# ================================
# ğŸ”¥ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (4chan ìŠ¤íƒ€ì¼)
# ================================

def detect_reddit_url_and_extract_info(url: str) -> dict:
    """Reddit URL ê°ì§€ ë° ì •ë³´ ì¶”ì¶œ (4chan ìŠ¤íƒ€ì¼)"""
    reddit_patterns = [
        r'^(?:https?://)?(?:www\.|old\.|new\.)?reddit\.com/r/([^/]+)/?$',
        r'^(?:https?://)?(?:www\.|old\.|new\.)?reddit\.com/r/([^/]+)/.*$',
        r'^r/([^/]+)/?$',
        r'^([a-zA-Z][a-zA-Z0-9_]{2,20})$'  # ì„œë¸Œë ˆë”§ ì´ë¦„ë§Œ
    ]
    
    for pattern in reddit_patterns:
        match = re.match(pattern, url.strip(), re.IGNORECASE)
        if match:
            subreddit_name = match.group(1)
            return {
                "is_reddit": True,
                "subreddit_name": subreddit_name,
                "parsed_url": f"https://reddit.com/r/{subreddit_name}",
                "input_type": "subreddit"
            }
    
    return {"is_reddit": False}

def is_reddit_url(url: str) -> bool:
    """Reddit URLì¸ì§€ í™•ì¸"""
    if not url:
        return False
    return detect_reddit_url_and_extract_info(url).get("is_reddit", False)

# ëª¨ë“ˆ ì •ë³´ (ë™ì  íƒì§€ë¥¼ ìœ„í•œ ë©”íƒ€ë°ì´í„°)
DISPLAY_NAME = "Reddit Crawler Enhanced"
DESCRIPTION = "Reddit ì„œë¸Œë ˆë”§ í¬ë¡¤ëŸ¬ (ì´ë¯¸ì§€/ë¹„ë””ì˜¤ ì§€ì›)"
VERSION = "2.1.0"  # ğŸ”¥ ë²„ì „ ì—…ê·¸ë ˆì´ë“œ
SUPPORTED_DOMAINS = ["reddit.com", "www.reddit.com", "old.reddit.com", "new.reddit.com"]
KEYWORDS = ["reddit", "subreddit", "/r/"]

# ëª¨ë“ˆ ë¡œë“œ í™•ì¸
logging.info("ğŸ”¥ Reddit í¬ë¡¤ëŸ¬ Enhanced v2.1 ë¡œë“œ ì™„ë£Œ")
logging.info("ğŸ“Š ì§€ì› ë¯¸ë””ì–´: ì´ë¯¸ì§€, ë¹„ë””ì˜¤, ê°¤ëŸ¬ë¦¬")
logging.info("ğŸ¯ API: Reddit PRAW + í–¥ìƒëœ ë¯¸ë””ì–´ ì¶”ì¶œ")
logging.info("ğŸ” URL ì§€ì›: ì¸ë„¤ì¼ + ì›ë³¸ ì´ë¯¸ì§€ ë¶„ë¦¬")
logging.info("ğŸ–¼ï¸ 4chan ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ ì ìš© ì™„ë£Œ")