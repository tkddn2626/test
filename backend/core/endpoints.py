# core/simple_endpoints.py - ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—†ëŠ” ë‹¨ìˆœ ë²„ì „

import asyncio
import importlib
import inspect
import logging
import os
import re
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, Callable
from fastapi import WebSocket
from datetime import datetime
from urllib.parse import urlparse

logger = logging.getLogger(__name__)

# endpoints.py ìƒë‹¨ì— ì¶”ê°€
import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

class SimpleEndpointManager:
    def __init__(self, app, crawl_manager):
        self.app = app
        self.crawl_manager = crawl_manager
        self.crawlers = {}
        
        # ê¸°ì¡´ í¬ë¡¤ëŸ¬ ëª…ì‹œì  ë“±ë¡
        self._register_explicit_crawlers()
        
        # ì—”ë“œí¬ì¸íŠ¸ ìƒì„±
        self._create_endpoints()
    
    def _register_explicit_crawlers(self):
        """ê¸°ì¡´ í¬ë¡¤ëŸ¬ë“¤ì„ ëª…ì‹œì ìœ¼ë¡œ ë“±ë¡"""
        explicit_crawlers = {
            'reddit': {'module': 'reddit', 'function': 'fetch_posts'},
            'dcinside': {'module': 'dcinside', 'function': 'crawl_dcinside_board'},
            'blind': {'module': 'blind', 'function': 'crawl_blind_board'},
            'bbc': {'module': 'bbc', 'function': 'crawl_bbc_board'},
            'lemmy': {'module': 'lemmy', 'function': 'crawl_lemmy_board'},
            'universal': {'module': 'universal', 'function': 'crawl_universal_board'}
        }
        
        for site_type, info in explicit_crawlers.items():
            try:
                module = __import__(info['module'])
                crawl_func = getattr(module, info['function'])
                
                self.crawlers[site_type] = {
                    "crawl_function": crawl_func,
                    "metadata": {"module_name": info['module']}
                }
                logger.info(f"âœ… ëª…ì‹œì  í¬ë¡¤ëŸ¬ ë“±ë¡: {site_type}")
                
            except Exception as e:
                logger.warning(f"í¬ë¡¤ëŸ¬ ë“±ë¡ ì‹¤íŒ¨ {site_type}: {e}")

# ==================== ğŸ”¥ ë‹¨ìˆœí•œ ìë™ ì—”ë“œí¬ì¸íŠ¸ ë§¤ë‹ˆì € ====================

class SimpleEndpointManager:
    """ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—†ëŠ” ë‹¨ìˆœ ì—”ë“œí¬ì¸íŠ¸ ë§¤ë‹ˆì €"""
    
    def __init__(self, app, crawl_manager):
        self.app = app
        self.crawl_manager = crawl_manager
        self.crawlers = {}  # {site_type: {crawl_func, detect_func, metadata}}
        self.origin_validator = OriginValidator()
        
        # í¬ë¡¤ëŸ¬ ìë™ ë°œê²¬
        self._discover_crawlers()
        
        # ì—”ë“œí¬ì¸íŠ¸ ìƒì„±
        self._create_endpoints()
        
        logger.info(f"ğŸš€ ë‹¨ìˆœ ì—”ë“œí¬ì¸íŠ¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”: {len(self.crawlers)}ê°œ í¬ë¡¤ëŸ¬")
    
    def _discover_crawlers(self):
        """í¬ë¡¤ëŸ¬ íŒŒì¼ë“¤ì„ ìë™ìœ¼ë¡œ ë°œê²¬í•˜ê³  ë“±ë¡"""
        crawler_dirs = [
            Path(__file__).parent.parent,  # í”„ë¡œì íŠ¸ ë£¨íŠ¸
            Path(__file__).parent / "sites",  # sites ë””ë ‰í† ë¦¬
        ]
        
        for crawler_dir in crawler_dirs:
            if not crawler_dir.exists():
                continue
                
            for py_file in crawler_dir.glob("*.py"):
                if py_file.stem.startswith("_") or py_file.stem == "__init__":
                    continue
                
                try:
                    self._register_crawler_from_file(py_file)
                except Exception as e:
                    logger.warning(f"í¬ë¡¤ëŸ¬ ë“±ë¡ ì‹¤íŒ¨ {py_file}: {e}")
    
    def _register_crawler_from_file(self, py_file: Path):
        """íŒŒì´ì¬ íŒŒì¼ì—ì„œ í¬ë¡¤ëŸ¬ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  ë“±ë¡"""
        module_name = py_file.stem
        
        try:
            # ë™ì  ëª¨ë“ˆ ì„í¬íŠ¸
            spec = importlib.util.spec_from_file_location(module_name, py_file)
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)
            
            # ì‚¬ì´íŠ¸ íƒ€ì… ì¶”ì¶œ
            site_type = getattr(module, 'SITE_TYPE', module_name.lower())
            
            # í¬ë¡¤ë§ í•¨ìˆ˜ ì°¾ê¸°
            crawl_function = self._find_crawl_function(module, module_name, site_type)
            if not crawl_function:
                return
            
            # ê°ì§€ í•¨ìˆ˜ ì°¾ê¸° (ì„ íƒì‚¬í•­)
            detect_function = self._find_detect_function(module, module_name, site_type)
            
            # ë©”íƒ€ë°ì´í„° êµ¬ì„±
            metadata = {
                "module_name": module_name,
                "display_name": getattr(module, 'DISPLAY_NAME', site_type.title()),
                "description": getattr(module, 'DESCRIPTION', f"{site_type} í¬ë¡¤ëŸ¬"),
                "version": getattr(module, 'VERSION', "1.0.0"),
            }
            
            # í¬ë¡¤ëŸ¬ ë“±ë¡
            self.crawlers[site_type] = {
                "crawl_function": crawl_function,
                "detect_function": detect_function,
                "metadata": metadata
            }
            
            logger.info(f"âœ… í¬ë¡¤ëŸ¬ ë“±ë¡: {site_type} ({module_name})")
            
        except Exception as e:
            logger.debug(f"ëª¨ë“ˆ {module_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _find_crawl_function(self, module, module_name: str, site_type: str):
        """í¬ë¡¤ë§ í•¨ìˆ˜ ì°¾ê¸°"""
        patterns = [
            f"crawl_{module_name}_board",
            f"crawl_{site_type}_board",
            "crawl_board", 
            "crawl",
            "fetch_posts"
        ]
        
        for pattern in patterns:
            if hasattr(module, pattern):
                return getattr(module, pattern)
        return None
    
    def _find_detect_function(self, module, module_name: str, site_type: str):
        """ê°ì§€ í•¨ìˆ˜ ì°¾ê¸°"""
        patterns = [
            f"detect_{module_name}_url_and_extract_info",
            f"detect_{site_type}_url_and_extract_info",
            "detect_url_and_extract_info"
        ]
        
        for pattern in patterns:
            if hasattr(module, pattern):
                return getattr(module, pattern)
        return None
    
    def _create_endpoints(self):
        """ì—”ë“œí¬ì¸íŠ¸ë“¤ ìƒì„±"""
        
        # 1. í†µí•© í¬ë¡¤ë§ ì—”ë“œí¬ì¸íŠ¸
        @self.app.websocket("/ws/crawl")
        async def unified_crawl_endpoint(websocket: WebSocket):
            await self._handle_unified_crawl(websocket)
        
        # 2. ì‚¬ì´íŠ¸ ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸  
        @self.app.websocket("/ws/analyze")
        async def analyze_endpoint(websocket: WebSocket):
            await self._handle_site_analysis(websocket)
        
        # 3. ê°œë³„ ì‚¬ì´íŠ¸ ì—”ë“œí¬ì¸íŠ¸ë“¤ (í•˜ìœ„ í˜¸í™˜ì„±)
        for site_type in self.crawlers.keys():
            self._create_site_endpoint(site_type)
    
    def _create_site_endpoint(self, site_type: str):
        """ê°œë³„ ì‚¬ì´íŠ¸ ì—”ë“œí¬ì¸íŠ¸ ìƒì„±"""
        endpoint_path = f"/ws/{site_type}-crawl"
        
        @self.app.websocket(endpoint_path)
        async def site_endpoint(websocket: WebSocket):
            await self._handle_site_crawl(websocket, site_type)
        
        logger.info(f"ğŸ“¡ ì—”ë“œí¬ì¸íŠ¸ ìƒì„±: {endpoint_path}")
    
    async def _handle_unified_crawl(self, websocket: WebSocket):
        """í†µí•© í¬ë¡¤ë§ ì²˜ë¦¬"""
        if not await self.origin_validator.validate_origin(websocket):
            return
        
        await websocket.accept()
        crawl_id = f"unified_{id(websocket)}_{int(time.time())}"
        
        try:
            config = await websocket.receive_json()
            input_data = config.get("input", "")
            
            # ì‚¬ì´íŠ¸ ìë™ ê°ì§€
            detected_site = await self._detect_site(input_data)
            if not detected_site:
                raise Exception("ì§€ì›ë˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸ì…ë‹ˆë‹¤.")
            
            # ê°ì§€ëœ ì‚¬ì´íŠ¸ ì •ë³´ ì „ì†¡
            await websocket.send_json({
                "detected_site": detected_site["site_type"],
                "board_identifier": detected_site.get("board_identifier"),
                "auto_detected": True,
                "progress": 20
            })
            
            # í•´ë‹¹ í¬ë¡¤ëŸ¬ë¡œ í¬ë¡¤ë§ ì‹¤í–‰
            results = await self._execute_crawl(detected_site["site_type"], input_data, websocket, config)
            
            # ê²°ê³¼ ì „ì†¡
            await websocket.send_json({
                "done": True,
                "data": results,
                "summary": f"í¬ë¡¤ë§ ì™„ë£Œ: {len(results)}ê°œ ê²Œì‹œë¬¼"
            })
            
        except Exception as e:
            logger.error(f"âŒ í†µí•© í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            await websocket.send_json({"error": str(e)})
        finally:
            await websocket.close()
    
    async def _detect_site(self, input_data: str):
        """ì‚¬ì´íŠ¸ ê°ì§€"""
        # 1. ë“±ë¡ëœ í¬ë¡¤ëŸ¬ë“¤ì˜ ê°ì§€ í•¨ìˆ˜ ì‹œë„
        for site_type, crawler_info in self.crawlers.items():
            detect_func = crawler_info.get("detect_function")
            if detect_func:
                try:
                    if asyncio.iscoroutinefunction(detect_func):
                        result = await detect_func(input_data)
                    else:
                        result = detect_func(input_data)
                    
                    if result and (result.get(f"is_{site_type}") or result.get("detected_site") == site_type):
                        return {
                            "site_type": site_type,
                            "board_identifier": result.get("board_identifier"),
                            "confidence": result.get("confidence", 1.0)
                        }
                except Exception as e:
                    logger.debug(f"ê°ì§€ ì‹¤íŒ¨ ({site_type}): {e}")
        
        # 2. í´ë°± ê°ì§€ (ê°„ë‹¨í•œ URL/í‚¤ì›Œë“œ ë§¤ì¹­)
        return self._fallback_detect(input_data)
    
    def _fallback_detect(self, input_data: str):
        """í´ë°± ì‚¬ì´íŠ¸ ê°ì§€"""
        input_lower = input_data.lower()
        
        # ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­
        patterns = {
            "reddit": ["reddit.com", "r/", "/r/"],
            "dcinside": ["dcinside.com", "ê°¤ëŸ¬ë¦¬", "dc"],
            "blind": ["teamblind.com", "blind", "ë¸”ë¼ì¸ë“œ"],
            "bbc": ["bbc.com", "bbc.co.uk"],
            "lemmy": ["lemmy.", "beehaw.org", "sh.itjust.works"]
        }
        
        for site_type, keywords in patterns.items():
            for keyword in keywords:
                if keyword in input_lower:
                    return {
                        "site_type": site_type,
                        "confidence": 0.7
                    }
        
        return None
    
    async def _execute_crawl(self, site_type: str, input_data: str, websocket: WebSocket, config: dict):
        """í¬ë¡¤ë§ ì‹¤í–‰"""
        crawler_info = self.crawlers.get(site_type)
        if not crawler_info:
            raise Exception(f"í¬ë¡¤ëŸ¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {site_type}")
        
        crawl_func = crawler_info["crawl_function"]
        
        # í•¨ìˆ˜ ë§¤ê°œë³€ìˆ˜ ë§¤í•‘
        sig = inspect.signature(crawl_func)
        params = {}
        
        # ë§¤ê°œë³€ìˆ˜ ìë™ ë§¤í•‘
        param_mapping = {
            'subreddit_name': input_data,  # Reddit
            'board_input': input_data,     # DCInside/Blind
            'board_url': input_data,       # BBC/Universal
            'websocket': websocket,
            'start_index': config.get('start', 1),
            'end_index': config.get('end', 20),
            'limit': config.get('limit', 20),
            'sort': config.get('sort', 'recent'),
            'min_views': config.get('min_views', 0),
            'min_likes': config.get('min_likes', 0),
            'time_filter': config.get('time_filter', 'day'),
        }
        
        for param_name in sig.parameters:
            if param_name in param_mapping:
                params[param_name] = param_mapping[param_name]
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        if asyncio.iscoroutinefunction(crawl_func):
            return await crawl_func(**params)
        else:
            return crawl_func(**params)
    
    async def _handle_site_analysis(self, websocket: WebSocket):
        """ì‚¬ì´íŠ¸ ë¶„ì„ ì²˜ë¦¬"""
        if not await self.origin_validator.validate_origin(websocket):
            return
        
        await websocket.accept()
        
        try:
            data = await websocket.receive_json()
            input_data = data.get("input", "")
            
            detected = await self._detect_site(input_data)
            
            await websocket.send_json({
                "analysis_complete": True,
                "detected_site": detected["site_type"] if detected else "unknown",
                "board_identifier": detected.get("board_identifier") if detected else None
            })
        except Exception as e:
            await websocket.send_json({"error": str(e)})
        finally:
            await websocket.close()
    
    async def _handle_site_crawl(self, websocket: WebSocket, site_type: str):
        """ê°œë³„ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì²˜ë¦¬"""
        if not await self.origin_validator.validate_origin(websocket):
            return
        
        await websocket.accept()
        
        try:
            config = await websocket.receive_json()
            input_data = config.get("board", "") or config.get("input", "")
            
            results = await self._execute_crawl(site_type, input_data, websocket, config)
            
            await websocket.send_json({
                "done": True,
                "data": results,
                "summary": f"{site_type} í¬ë¡¤ë§ ì™„ë£Œ: {len(results)}ê°œ ê²Œì‹œë¬¼"
            })
        except Exception as e:
            await websocket.send_json({"error": str(e)})
        finally:
            await websocket.close()
    
    def get_endpoint_info(self):
        """ì—”ë“œí¬ì¸íŠ¸ ì •ë³´ ë°˜í™˜"""
        return {
            "crawlers": {
                site: info["metadata"]
                for site, info in self.crawlers.items()
            },
            "endpoints": {
                "unified": "/ws/crawl",
                "analyze": "/ws/analyze",
                "individual": {
                    site: f"/ws/{site}-crawl"
                    for site in self.crawlers.keys()
                }
            }
        }

# ==================== Origin ê²€ì¦ê¸° ====================
class OriginValidator:
    def __init__(self):
        self.app_env = os.getenv("APP_ENV", "development")
    
    async def validate_origin(self, websocket: WebSocket) -> bool:
        origin = websocket.headers.get("origin", "")
        
        if self.app_env == "production":
            allowed = any(pattern in origin for pattern in ["netlify.app", "onrender.com"])
        else:
            allowed = True  # ê°œë°œ í™˜ê²½ì—ì„œëŠ” ëª¨ë“  origin í—ˆìš©
        
        if not allowed:
            await websocket.close(code=1008, reason="Invalid origin")
            return False
        
        return True

# ==================== íŒ©í† ë¦¬ í•¨ìˆ˜ ====================
def create_simple_endpoint_manager(app, crawl_manager):
    """ë‹¨ìˆœ ì—”ë“œí¬ì¸íŠ¸ ë§¤ë‹ˆì € ìƒì„±"""
    return SimpleEndpointManager(app, crawl_manager)