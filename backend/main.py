# main.py - ë¯¸ë””ì–´ ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ í¬í•¨ ì™„ì „ ë²„ì „

from fastapi import FastAPI, WebSocket, Query, Request, HTTPException
from fastapi.responses import JSONResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, Dict, Set, Any, List
import os, asyncio, json, requests, time, logging, traceback
from datetime import datetime, timedelta
from dotenv import load_dotenv
from pathlib import Path
import sys
import importlib.util
from core.site_detector import DynamicSiteDetector
from core.messages import create_localized_message
from core.first_visitor import first_visitor_router

# ==================== ğŸ”¥ ë¡œê¹… ì„¤ì •ì„ ê°€ì¥ ë¨¼ì € ==================== 
# í™˜ê²½ ì„¤ì •
load_dotenv()
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

# í™˜ê²½ ë³€ìˆ˜
DEEPL_API_KEY = os.getenv("DEEPL_API_KEY")
APP_ENV = os.getenv("APP_ENV", "development")
DEBUG = os.getenv("DEBUG", "False").lower() == "true"
PORT = int(os.getenv("PORT", 8000))
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")

# ğŸ”¥ Supabase í™˜ê²½ ë³€ìˆ˜ í™•ì¸
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_ANON_KEY = os.getenv("SUPABASE_ANON_KEY")

# ğŸ”¥ ë¡œê¹… ì„¤ì • - ë‹¤ë¥¸ ëª¨ë“  ê²ƒë³´ë‹¤ ë¨¼ì €!
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL.upper()),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("pickpost")

# ğŸ”¥ ì´ì œ database ëª¨ë“ˆ import (ë¡œê¹… ì„¤ì • í›„)
try:
    from database import save_feedback_to_database, get_feedback_from_database, get_database_status
    DATABASE_AVAILABLE = True
    logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    DATABASE_AVAILABLE = False
    logger.warning(f"âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")

# AutoCrawler import ì¶”ê°€
try:
   from core.auto_crawler import AutoCrawler
   AUTO_CRAWLER_AVAILABLE = True
   logger.info("âœ… AutoCrawler ë¡œë“œ ì„±ê³µ")
except ImportError as e:
   logger.error(f"âŒ AutoCrawler ë¡œë“œ ì‹¤íŒ¨: {e}")
   AUTO_CRAWLER_AVAILABLE = False

# ğŸ”¥ ë¯¸ë””ì–´ ë‹¤ìš´ë¡œë“œ ëª¨ë“ˆ import ì¶”ê°€
try:
    from core.media_download import get_media_download_manager
    MEDIA_DOWNLOAD_AVAILABLE = True
    logger.info("âœ… ë¯¸ë””ì–´ ë‹¤ìš´ë¡œë“œ ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    MEDIA_DOWNLOAD_AVAILABLE = False
    logger.warning(f"âš ï¸ ë¯¸ë””ì–´ ë‹¤ìš´ë¡œë“œ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")

# ==================== ë™ì  í¬ë¡¤ëŸ¬ ë°œê²¬ ì‹œìŠ¤í…œ ====================

def discover_available_crawlers():
    """backend/crawlers/ ë””ë ‰í† ë¦¬ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë¡¤ëŸ¬ë“¤ì„ ë™ì ìœ¼ë¡œ ë°œê²¬"""
    current_dir = Path(__file__).parent
    crawlers_dir = current_dir / "crawlers"
    available_crawlers = set()
    
    if not crawlers_dir.exists():
        logger.warning(f"âš ï¸ crawlers ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {crawlers_dir}")
        if AUTO_CRAWLER_AVAILABLE:
            available_crawlers.add('universal')
            logger.debug("âœ… AutoCrawler(universal)ë§Œ ì‚¬ìš© ê°€ëŠ¥")
        return available_crawlers
    
    for py_file in crawlers_dir.glob("*.py"):
        if py_file.name.startswith('_') or py_file.stem == '__init__':
            continue
            
        crawler_name = py_file.stem
        
        try:
            spec = importlib.util.spec_from_file_location(f"crawlers.{crawler_name}", py_file)
            if spec and spec.loader:
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)
                
                crawl_functions = [
                    f'crawl_{crawler_name}_board',
                    f'fetch_posts',
                    f'crawl_{crawler_name}',
                    f'{crawler_name}_crawl',
                    f'crawl_{crawler_name.lower()}_board',
                ]
                
                has_crawl_function = any(hasattr(module, func) for func in crawl_functions)
                
                if has_crawl_function:
                    available_crawlers.add(crawler_name)
                    logger.debug(f"âœ… í¬ë¡¤ëŸ¬ ë°œê²¬: crawlers/{crawler_name}")
                else:
                    logger.debug(f"âš ï¸ í¬ë¡¤ë§ í•¨ìˆ˜ ì—†ìŒ: crawlers/{crawler_name}")
                    
        except Exception as e:
            logger.debug(f"âš ï¸ í¬ë¡¤ëŸ¬ í™•ì¸ ì‹¤íŒ¨ crawlers/{crawler_name}: {e}")
    
    if AUTO_CRAWLER_AVAILABLE:
        available_crawlers.add('universal')
        logger.debug("âœ… AutoCrawler(universal) ì‚¬ìš© ê°€ëŠ¥")
    
    logger.info(f"ğŸ¯ ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë¡¤ëŸ¬: {sorted(available_crawlers)}")
    return available_crawlers

def import_available_crawlers():
    """ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë¡¤ëŸ¬ë“¤ì„ ë™ì ìœ¼ë¡œ import"""
    crawl_functions = {}
    
    current_dir = Path(__file__).parent
    crawlers_dir = current_dir / "crawlers"
    
    if not crawlers_dir.exists():
        logger.warning(f"âš ï¸ crawlers ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {crawlers_dir}")
        return crawl_functions
    
    for py_file in crawlers_dir.glob("*.py"):
        if py_file.name.startswith('_') or py_file.stem == '__init__':
            continue
            
        crawler_name = py_file.stem
        
        try:
            spec = importlib.util.spec_from_file_location(f"crawlers.{crawler_name}", py_file)
            if spec and spec.loader:
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)
                
                possible_functions = [
                    f'crawl_{crawler_name}_board',
                    f'fetch_posts',
                    f'crawl_{crawler_name}',
                    f'{crawler_name}_crawl',
                    f'crawl_board',
                    f'crawl'
                ]
                
                found_functions = []
                for func_name in possible_functions:
                    if hasattr(module, func_name):
                        func = getattr(module, func_name)
                        if callable(func):
                            found_functions.append((func_name, func))
                
                if found_functions:
                    main_func_name, main_func = found_functions[0]
                    crawl_functions[crawler_name] = main_func
                    logger.debug(f"âœ… {crawler_name} í¬ë¡¤ëŸ¬ import ì„±ê³µ ({main_func_name})")
                    
                    for func_name, func in found_functions[1:]:
                        special_key = f"{crawler_name}_{func_name}"
                        crawl_functions[special_key] = func
                        logger.debug(f"   ì¶”ê°€ í•¨ìˆ˜: {special_key}")
                    
                    special_functions = [
                        f'detect_{crawler_name}_url_and_extract_info',
                        f'parse_{crawler_name}',
                        f'extract_{crawler_name}_info'
                    ]
                    
                    for special_func_name in special_functions:
                        if hasattr(module, special_func_name):
                            special_func = getattr(module, special_func_name)
                            if callable(special_func):
                                crawl_functions[special_func_name] = special_func
                                logger.debug(f"   íŠ¹ë³„ í•¨ìˆ˜: {special_func_name}")
                else:
                    logger.debug(f"âš ï¸ í¬ë¡¤ë§ í•¨ìˆ˜ ì—†ìŒ: crawlers/{crawler_name}")
                    
        except Exception as e:
            logger.debug(f"âš ï¸ í¬ë¡¤ëŸ¬ import ì‹¤íŒ¨ crawlers/{crawler_name}: {e}")
    
    logger.info(f"ğŸš€ ë™ì  ë¡œë“œëœ í¬ë¡¤ë§ í•¨ìˆ˜: {list(crawl_functions.keys())}")
    return crawl_functions

# ì•± ì‹œì‘ì‹œ í¬ë¡¤ëŸ¬ ëª©ë¡ ë° í•¨ìˆ˜ë“¤ ê°€ì ¸ì˜¤ê¸°
AVAILABLE_CRAWLERS = discover_available_crawlers()
CRAWL_FUNCTIONS = import_available_crawlers()

# core/ í´ë”ì˜ SiteDetector import (í´ë°±ìš©)
try:
    from core.site_detector import SiteDetector as CoreSiteDetector
    CORE_SITE_DETECTOR_AVAILABLE = True
    logger.info("âœ… Core SiteDetector ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    logger.warning(f"âš ï¸ Core SiteDetector ë¡œë“œ ì‹¤íŒ¨: {e}")
    CORE_SITE_DETECTOR_AVAILABLE = False

# ==================== FastAPI ì•± ì´ˆê¸°í™” ====================
app = FastAPI(title="PickPost API v2.1", debug=DEBUG)

# ==================== ğŸ”¥ ì •ì  íŒŒì¼ ë¼ìš°íŒ… ìµœìš°ì„  ì„¤ì • ====================

# ğŸ”¥ ì •ì  íŒŒì¼ ê°œë³„ ì—”ë“œí¬ì¸íŠ¸ (ë¼ìš°í„°ë³´ë‹¤ ë¨¼ì € ì •ì˜)
@app.get("/js/{file_path:path}")
async def serve_js_files(file_path: str):
    """JavaScript íŒŒì¼ ì§ì ‘ ì„œë¹™ - ìµœìš°ì„  ì²˜ë¦¬"""
    # í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ js í´ë” ì°¾ê¸°
    possible_paths = [
        Path("js") / file_path,           # ./js/
        Path("static/js") / file_path,    # ./static/js/
        Path("frontend/js") / file_path,  # ./frontend/js/
        Path(".") / "js" / file_path      # ì ˆëŒ€ ê²½ë¡œ
    ]
    
    for js_path in possible_paths:
        if js_path.exists() and js_path.is_file():
            logger.info(f"ğŸ¯ JavaScript íŒŒì¼ ì„œë¹™: {js_path}")
            return FileResponse(
                js_path,
                media_type="application/javascript; charset=utf-8",
                headers={
                    "Content-Type": "application/javascript; charset=utf-8",
                    "Cache-Control": "public, max-age=3600",
                    "Access-Control-Allow-Origin": "*"
                }
            )
    
    logger.error(f"âŒ JavaScript íŒŒì¼ ì—†ìŒ: {file_path}")
    logger.error(f"   ì‹œë„í•œ ê²½ë¡œë“¤: {[str(p) for p in possible_paths]}")
    raise HTTPException(status_code=404, detail=f"JavaScript file not found: {file_path}")

@app.get("/css/{file_path:path}")
async def serve_css_files(file_path: str):
    """CSS íŒŒì¼ ì§ì ‘ ì„œë¹™"""
    possible_paths = [
        Path("css") / file_path,
        Path("static/css") / file_path,
        Path("frontend/css") / file_path,
        Path(".") / "css" / file_path
    ]
    
    for css_path in possible_paths:
        if css_path.exists() and css_path.is_file():
            logger.info(f"ğŸ¨ CSS íŒŒì¼ ì„œë¹™: {css_path}")
            return FileResponse(
                css_path,
                media_type="text/css; charset=utf-8",
                headers={
                    "Content-Type": "text/css; charset=utf-8",
                    "Cache-Control": "public, max-age=3600",
                    "Access-Control-Allow-Origin": "*"
                }
            )
    
    raise HTTPException(status_code=404, detail=f"CSS file not found: {file_path}")

# ë£¨íŠ¸ ê²½ë¡œì—ì„œ index.html ì„œë¹™ (ì •ì  íŒŒì¼ ì—”ë“œí¬ì¸íŠ¸ ë‹¤ìŒì— ì •ì˜)
@app.get("/")
async def serve_index():
    """ë£¨íŠ¸ ê²½ë¡œì—ì„œ index.html ì œê³µ"""
    possible_paths = [
        Path("index.html"),
        Path("static/index.html"),
        Path("frontend/index.html"),
        Path(".") / "index.html"
    ]
    
    for index_path in possible_paths:
        if index_path.exists() and index_path.is_file():
            logger.info(f"ğŸ  index.html ì„œë¹™: {index_path}")
            return FileResponse(
                index_path,
                media_type="text/html; charset=utf-8",
                headers={
                    "Content-Type": "text/html; charset=utf-8",
                    "Cache-Control": "public, max-age=300",
                    "Access-Control-Allow-Origin": "*"
                }
            )
    
    # index.htmlì´ ì—†ìœ¼ë©´ API ì •ë³´ ë°˜í™˜
    return {
        "message": "PickPost API Server", 
        "status": "running",
        "version": "2.1.1 (Media Download Support)",
        "docs": "/docs",
        "static_files_status": "Direct routing enabled",
        "media_download_status": "âœ… Available" if MEDIA_DOWNLOAD_AVAILABLE else "âŒ Unavailable"
    }

# ì²« ë²ˆì§¸ ë°©ë¬¸ì ë¼ìš°í„° ì¶”ê°€ (ì •ì  íŒŒì¼ ë¼ìš°íŒ… ì´í›„)
app.include_router(first_visitor_router)

# ==================== ğŸ”¥ CORS ë° ë¯¸ë“¤ì›¨ì–´ ì„¤ì • ====================
@app.middleware("http")
async def cors_and_static_middleware(request, call_next):
    """CORS ë° ì •ì  íŒŒì¼ ì²˜ë¦¬ ë¯¸ë“¤ì›¨ì–´"""
    
    # OPTIONS ìš”ì²­ ì²˜ë¦¬
    if request.method == "OPTIONS":
        response = JSONResponse({"message": "OK"})
        response.headers["Access-Control-Allow-Origin"] = "*"
        response.headers["Access-Control-Allow-Methods"] = "GET, POST, PUT, DELETE, OPTIONS"
        response.headers["Access-Control-Allow-Headers"] = "*"
        response.headers["Access-Control-Max-Age"] = "86400"
        response.headers["Access-Control-Allow-Credentials"] = "true"
        return response
    
    # ì¼ë°˜ ìš”ì²­ ì²˜ë¦¬
    response = await call_next(request)
    
    # ëª¨ë“  ì‘ë‹µì— CORS í—¤ë” ì¶”ê°€
    response.headers["Access-Control-Allow-Origin"] = "*"
    response.headers["Access-Control-Allow-Credentials"] = "true"
    response.headers["Access-Control-Allow-Methods"] = "GET, POST, PUT, DELETE, OPTIONS"
    response.headers["Access-Control-Allow-Headers"] = "*"
    
    return response

# ê¸°ë³¸ CORS ë¯¸ë“¤ì›¨ì–´
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "HEAD"],
    allow_headers=["*"],
    expose_headers=["*"],
    max_age=86400
)

# ==================== ğŸ”¥ ë¯¸ë””ì–´ ë‹¤ìš´ë¡œë“œ ëª¨ë¸ ì •ì˜ ====================

class MediaDownloadRequest(BaseModel):
    posts: List[Dict[str, Any]]  # í¬ë¡¤ë§ëœ ê²Œì‹œë¬¼ ë°ì´í„°
    site_type: str               # reddit, 4chan, dcinside ë“±
    include_images: bool = True
    include_videos: bool = True
    include_audio: bool = False
    max_file_size_mb: int = 50   # ê°œë³„ íŒŒì¼ ìµœëŒ€ í¬ê¸°

class MediaDownloadResponse(BaseModel):
    success: bool
    download_url: str = ""
    zip_filename: str = ""
    total_files: int = 0
    downloaded_files: int = 0
    failed_files: int = 0
    zip_size_mb: float = 0
    error: str = ""

# ==================== ë‚˜ë¨¸ì§€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ ====================
def get_user_language(config):
    if not config:
        return "en"
    return config.get("language", "en")

def calculate_actual_dates(time_filter, start_date_input=None, end_date_input=None):
    if time_filter == 'custom' and start_date_input and end_date_input:
        return start_date_input, end_date_input
    
    now = datetime.now()
    
    if time_filter == 'day':
        start = now - timedelta(days=1)
        return start.strftime('%Y-%m-%d'), now.strftime('%Y-%m-%d')
    elif time_filter == 'week':
        start = now - timedelta(weeks=1)
        return start.strftime('%Y-%m-%d'), now.strftime('%Y-%m-%d')
    elif time_filter == 'month':
        start = now - timedelta(days=30)
        return start.strftime('%Y-%m-%d'), now.strftime('%Y-%m-%d')
    elif time_filter == 'year':
        start = now - timedelta(days=365)
        return start.strftime('%Y-%m-%d'), now.strftime('%Y-%m-%d')
    
    return None, None

# ==================== í¬ë¡¤ë§ ê´€ë¦¬ì ====================
class CrawlManager:
    def __init__(self):
        self.cancelled_crawls: Set[str] = set()
        self.creation_time = time.time()
    
    def cancel_crawl(self, crawl_id: str):
        self.cancelled_crawls.add(crawl_id)
        logger.info(f"ğŸš« í¬ë¡¤ë§ ì·¨ì†Œ: {crawl_id}")
    
    def is_cancelled(self, crawl_id: str) -> bool:
        return crawl_id in self.cancelled_crawls
    
    def cleanup_crawl(self, crawl_id: str):
        self.cancelled_crawls.discard(crawl_id)

crawl_manager = CrawlManager()

# ==================== ì‚¬ì´íŠ¸ë³„ í¬ë¡¤ë§ ì‹¤í–‰ ====================
async def execute_crawl_by_site(site_type: str, target_input: str, **config):
    crawl_id = config.pop('crawl_id', None)
    
    if crawl_id and crawl_manager.is_cancelled(crawl_id):
        raise asyncio.CancelledError("í¬ë¡¤ë§ ì·¨ì†Œë¨")
    
    # ì „ìš© í¬ë¡¤ëŸ¬ ì§ì ‘ í˜¸ì¶œ
    if site_type in CRAWL_FUNCTIONS:
        try:
            crawl_func = CRAWL_FUNCTIONS[site_type]
            logger.info(f"ğŸ¯ ì „ìš© í¬ë¡¤ëŸ¬ ì§ì ‘ ì‚¬ìš©: {site_type} -> {target_input}")
            
            # ì‚¬ì´íŠ¸ë³„ ë§¤ê°œë³€ìˆ˜ ì¤€ë¹„ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
            if site_type == 'reddit':
                params = {
                    'subreddit_name': target_input,
                    'limit': config.get('limit', config.get('end_index', 20) + 5),
                    'sort': config.get('sort', 'top'),
                    'time_filter': config.get('time_filter', 'day'),
                    'websocket': config.get('websocket'),
                    'min_views': config.get('min_views', 0),
                    'min_likes': config.get('min_likes', 0),
                    'start_date': config.get('start_date'),
                    'end_date': config.get('end_date'),
                    'start_index': config.get('start_index', 1),
                    'end_index': config.get('end_index', 20)
                }
            else:
                # ì¼ë°˜ì ì¸ ë§¤ê°œë³€ìˆ˜
                params = {
                    'target_input': target_input,
                    **{k: v for k, v in config.items() if v is not None}
                }
            
            params = {k: v for k, v in params.items() if v is not None}
            return await crawl_func(**params)
            
        except Exception as e:
            logger.warning(f"âš ï¸ ì „ìš© í¬ë¡¤ëŸ¬ ì‹¤íŒ¨, AutoCrawlerë¡œ í´ë°±: {e}")
    
    # AutoCrawler ì‚¬ìš©
    if AUTO_CRAWLER_AVAILABLE:
        try:
            auto_crawler = AutoCrawler()
            config['force_site_type'] = site_type
            logger.info(f"ğŸ¤– AutoCrawler í´ë°± ì‚¬ìš©: {site_type} -> {target_input}")
            return await auto_crawler.crawl(target_input, **config)
        except Exception as e:
            logger.error(f"âŒ AutoCrawlerë„ ì‹¤íŒ¨: {e}")
            raise
    
    raise Exception(f"ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë¡¤ëŸ¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {site_type}")

# ==================== ë²ˆì—­ ì„œë¹„ìŠ¤ ====================
async def deepl_translate(text: str, target_lang: str) -> str:
    try:
        if not text.strip() or not DEEPL_API_KEY:
            return text
            
        response = requests.post(
            "https://api-free.deepl.com/v2/translate",
            data={
                "auth_key": DEEPL_API_KEY,
                "text": text,
                "target_lang": target_lang.upper()
            },
            timeout=10
        )
        
        if response.status_code == 200:
            result = response.json()
            return result["translations"][0]["text"]
        else:
            logger.warning(f"ë²ˆì—­ API ì˜¤ë¥˜: {response.status_code}")
            return text
            
    except Exception as e:
        logger.error(f"ë²ˆì—­ ì˜¤ë¥˜: {e}")
        return text

# ==================== ë¯¸ë””ì–´ ë‹¤ìš´ë¡œë“œ API ì—”ë“œí¬ì¸íŠ¸ ====================
@app.post("/api/download-media", response_model=MediaDownloadResponse)
async def download_media_files(request: MediaDownloadRequest):
    """ë¯¸ë””ì–´ íŒŒì¼ í¬ë¡¤ë§ ë° ZIP ë‹¤ìš´ë¡œë“œ ìƒì„±"""
    
    if not MEDIA_DOWNLOAD_AVAILABLE:
        raise HTTPException(
            status_code=503, 
            detail="Media download service is currently unavailable"
        )
    
    try:
        logger.info(f"ğŸ–¼ï¸ Media download request: {len(request.posts)} posts ({request.site_type})")
        
        # ì‚¬ìš©ì ì–¸ì–´ ê°ì§€ (ê¸°ë³¸ê°’: ì˜ì–´)
        user_lang = getattr(request, 'user_lang', 'en')
        
        # ë¯¸ë””ì–´ ë‹¤ìš´ë¡œë“œ ë§¤ë‹ˆì € ì‚¬ìš©
        async with get_media_download_manager(user_lang) as manager:
            
            # ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜ (ì„ íƒì‚¬í•­)
            async def progress_callback(current, total):
                percentage = (current / total) * 100
                logger.debug(f"Media download progress: {percentage:.1f}% ({current}/{total})")
            
            # ë¯¸ë””ì–´ ì¼ê´„ ë‹¤ìš´ë¡œë“œ ì‹¤í–‰ - ì˜¬ë°”ë¥¸ ë©”ì„œë“œ ì´ë¦„ ì‚¬ìš©
            result = await manager.download_media_from_posts(
                posts=request.posts,
                site_type=request.site_type,
                progress_callback=progress_callback
            )
            
            if result['success']:
                # ì‘ë‹µ í˜•ì‹ì´ ë‹¤ë¥´ë¯€ë¡œ ìˆ˜ì •
                return MediaDownloadResponse(
                    success=True,
                    download_url=result['download_url'],
                    zip_filename=result['zip_filename'],
                    total_files=result['total_media_found'],
                    downloaded_files=result['downloaded_files'],
                    failed_files=result['failed_files'],
                    zip_size_mb=result['zip_size_mb']
                )
            else:
                return MediaDownloadResponse(
                    success=False,
                    error=result.get('error', 'Media download failed'),
                    total_files=result.get('total_media_found', 0),
                    downloaded_files=result.get('downloaded_files', 0),
                    failed_files=result.get('failed_files', 0)
                )
                
    except Exception as e:
        logger.error(f"âŒ Media download processing error: {e}")
        return MediaDownloadResponse(
            success=False,
            error=f"Error during media download: {str(e)}"
        )

@app.get("/api/download-file/{filename}")
async def download_file(filename: str):
    """ZIP íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # ì„ì‹œ ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ì—ì„œ íŒŒì¼ ì°¾ê¸°
        from core.media_download import TEMP_DOWNLOAD_DIR
        
        file_path = TEMP_DOWNLOAD_DIR / filename
        
        if not file_path.exists() or not file_path.is_file():
            raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # íŒŒì¼ í¬ê¸° í™•ì¸ (ë„ˆë¬´ í° íŒŒì¼ ë°©ì§€)
        file_size_mb = file_path.stat().st_size / (1024 * 1024)
        if file_size_mb > 1000:  # 1GB ì œí•œ
            raise HTTPException(status_code=413, detail="íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤")
        
        logger.info(f"ğŸ“¥ ë¯¸ë””ì–´ ZIP ë‹¤ìš´ë¡œë“œ: {filename} ({file_size_mb:.1f}MB)")
        
        return FileResponse(
            path=file_path,
            filename=filename,
            media_type="application/zip",
            headers={
                "Content-Disposition": f"attachment; filename={filename}",
                "Cache-Control": "no-cache, no-store, must-revalidate",
                "Pragma": "no-cache",
                "Expires": "0"
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

@app.get("/api/media-info")
async def get_media_manager_info():
    """ë¯¸ë””ì–´ ë‹¤ìš´ë¡œë“œ ë§¤ë‹ˆì € ì •ë³´ ì¡°íšŒ"""
    
    if not MEDIA_DOWNLOAD_AVAILABLE:
        return {
            "available": False,
            "error": "ë¯¸ë””ì–´ ë‹¤ìš´ë¡œë“œ ëª¨ë“ˆì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        }
    
    try:
        manager = get_media_download_manager()
        info = manager.get_manager_info()
        
        return {
            "available": True,
            "manager_info": info,
            "supported_sites": manager.extractor_manager.get_supported_sites()
        }
        
    except Exception as e:
        logger.error(f"âŒ ë¯¸ë””ì–´ ë§¤ë‹ˆì € ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return {
            "available": False,
            "error": str(e)
        }

# ==================== ë””ë²„ê¹… ì—”ë“œí¬ì¸íŠ¸ ====================
@app.get("/api/debug/static-files")
async def debug_static_files():
    """ì •ì  íŒŒì¼ ë””ë²„ê¹… ì •ë³´"""
    current_path = Path(".")
    
    # ê°€ëŠ¥í•œ ëª¨ë“  ê²½ë¡œ ì²´í¬
    paths_to_check = [
        "js/first_one.js",
        "js/present.js",
        "static/js/first_one.js",
        "static/js/present.js",
        "frontend/js/first_one.js",
        "frontend/js/present.js",
        "index.html",
        "static/index.html",
        "frontend/index.html"
    ]
    
    file_status = {}
    for path_str in paths_to_check:
        path = Path(path_str)
        file_status[path_str] = {
            "exists": path.exists(),
            "is_file": path.is_file() if path.exists() else False,
            "absolute_path": str(path.absolute())
        }
    
    # ë””ë ‰í† ë¦¬ êµ¬ì¡°
    js_dirs = []
    for js_dir in ["js", "static/js", "frontend/js"]:
        js_path = Path(js_dir)
        if js_path.exists() and js_path.is_dir():
            js_files = [f.name for f in js_path.glob("*.js")]
            js_dirs.append({
                "path": str(js_path),
                "exists": True,
                "files": js_files
            })
        else:
            js_dirs.append({
                "path": str(js_path),
                "exists": False,
                "files": []
            })
    
    return {
        "current_directory": str(current_path.absolute()),
        "file_status": file_status,
        "js_directories": js_dirs,
        "routing_order": [
            "1. /js/{file_path:path} -> serve_js_files()",
            "2. /css/{file_path:path} -> serve_css_files()",
            "3. / -> serve_index()",
            "4. first_visitor_router",
            "5. other API routes"
        ]
    }

# ==================== í”¼ë“œë°± ì‹œìŠ¤í…œ ====================
class FeedbackRequest(BaseModel):
    description: Optional[str] = None
    message: Optional[str] = None
    hasFile: Optional[bool] = False
    fileName: Optional[str] = None
    fileSize: Optional[int] = None
    systemInfo: Optional[Dict] = None
    currentLanguage: Optional[str] = "en"
    currentSite: Optional[str] = None
    url: Optional[str] = None

def save_feedback_locally(feedback_data: Dict) -> Dict:
    """ë¡œì»¬ íŒŒì¼ì— í”¼ë“œë°± ì €ì¥"""
    try:
        feedback_dir = "outputs/feedback"
        os.makedirs(feedback_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        
        local_feedback = {
            "timestamp": timestamp,
            "content": feedback_data.get("description", ""),
            "metadata": feedback_data,
            "system_info": {
                "auto_crawler_available": AUTO_CRAWLER_AVAILABLE,
                "core_site_detector_available": CORE_SITE_DETECTOR_AVAILABLE,
                "available_crawlers": list(AVAILABLE_CRAWLERS),
                "loaded_crawl_functions": list(CRAWL_FUNCTIONS.keys()),
                "database_available": DATABASE_AVAILABLE,
                "media_download_available": MEDIA_DOWNLOAD_AVAILABLE
            }
        }
        
        filename = f"feedback_{timestamp}.json"
        filepath = os.path.join(feedback_dir, filename)
        
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(local_feedback, f, ensure_ascii=False, indent=2)

        return {
            "feedback_id": f"local_{timestamp}",
            "status": "success",
            "message": "í”¼ë“œë°±ì´ ë¡œì»¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤"
        }
        
    except Exception as e:
        logger.error(f"âŒ ë¡œì»¬ ì €ì¥ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë¡œì»¬ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/feedback")
async def submit_feedback(request: Request, feedback: Optional[FeedbackRequest] = None):
    """í”¼ë“œë°± ì œì¶œ API"""
    try:
        if feedback:
            feedback_content = feedback.description or feedback.message or ""
            raw_data = feedback.dict()
        else:
            raw_data = await request.json()
            feedback_content = raw_data.get("description", raw_data.get("message", "")).strip()
        
        if not feedback_content:
            return JSONResponse(
                status_code=400, 
                content={"error": "í”¼ë“œë°± ë‚´ìš©ì´ í•„ìš”í•©ë‹ˆë‹¤"}
            )
        
        raw_data["client_ip"] = request.client.host
        raw_data["timestamp"] = datetime.now().isoformat()
        
        if DATABASE_AVAILABLE:
            try:
                result = await save_feedback_to_database(raw_data)
                logger.info(f"âœ… Supabase í”¼ë“œë°± ì €ì¥ ì„±ê³µ: ID {result.get('feedback_id')}")
                return {
                    "success": True,
                    "feedback_id": result.get("feedback_id"),
                    "message": "í”¼ë“œë°±ì´ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤",
                    "storage": "supabase"
                }
            except Exception as e:
                logger.warning(f"âš ï¸ Supabase ì €ì¥ ì‹¤íŒ¨, ë¡œì»¬ ì €ì¥ìœ¼ë¡œ í´ë°±: {e}")
        
        result = save_feedback_locally(raw_data)
        logger.info(f"ğŸ“ í”¼ë“œë°± ë¡œì»¬ ì €ì¥: {feedback_content[:50]}...")
        
        return {
            "success": True,
            "feedback_id": result.get("feedback_id"),
            "message": "í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤",
            "storage": "local"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ í”¼ë“œë°± ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return JSONResponse(
            status_code=500, 
            content={"error": f"í”¼ë“œë°± ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}"}
        )

# ==================== ì›¹ì†Œì¼“ í¬ë¡¤ë§ ì—”ë“œí¬ì¸íŠ¸ ====================
@app.websocket("/ws/crawl")
async def websocket_crawl(websocket: WebSocket):
    await websocket.accept()
    crawl_id = f"unified_{id(websocket)}_{int(time.time())}"
    logger.info(f"ğŸ”¥ í†µí•© í¬ë¡¤ë§ ì‹œì‘: {crawl_id}")
    
    try:
        config = await websocket.receive_json()
        user_lang = config.get("language", "en")
        
        logger.info(f"ğŸ”§ ë²ˆì—­ ì„¤ì • ìˆ˜ì‹ : translate={config.get('translate')}, user_lang={user_lang}")
        
        input_data = config.get("input", "").strip()
        frontend_site = config.get("site", "")
        
        crawl_options = {
            'sort': config.get("sort", "recent"),
            'start_index': config.get("start", 1),
            'end_index': config.get("end", 20),
            'min_views': config.get("min_views", 0),
            'min_likes': config.get("min_likes", 0),
            'min_comments': config.get("min_comments", 0),
            'time_filter': config.get("time_filter", "day"),
            'start_date': config.get("start_date"),
            'end_date': config.get("end_date"),
            'translate': config.get("translate", False),
            'target_languages': config.get("target_languages", []),
            'skip_translation': config.get("skip_translation", False),
            'websocket': websocket,
            'crawl_id': crawl_id
        }
        
        def check_cancelled():
            if crawl_manager.is_cancelled(crawl_id):
                raise asyncio.CancelledError(f"í¬ë¡¤ë§ {crawl_id} ì·¨ì†Œë¨")

        check_cancelled()

        if not input_data:
            await websocket.send_json({"error": "ì…ë ¥ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"})
            return

        actual_start_date, actual_end_date = calculate_actual_dates(
            crawl_options['time_filter'], 
            crawl_options['start_date'], 
            crawl_options['end_date']
        )
        crawl_options.update({
            'start_date': actual_start_date,
            'end_date': actual_end_date
        })
        
        # ì‚¬ì´íŠ¸ ê°ì§€ ë¡œì§
        if frontend_site and frontend_site != 'auto':
            detected_site = frontend_site
            
            if detected_site in CRAWL_FUNCTIONS:
                board_identifier = input_data
                logger.info(f"ğŸ¯ ì „ìš© í¬ë¡¤ëŸ¬ ì‚¬ìš©: {detected_site} â†’ ì…ë ¥: {board_identifier}")
            else:
                detected_site = 'universal'
                board_identifier = input_data
                logger.info(f"ğŸŒ AutoCrawler(universal) ì²˜ë¦¬: {frontend_site} â†’ {input_data}")
            
        else:
            # ì‚¬ì´íŠ¸ ê°ì§€ ë©”ì‹œì§€
            message = create_localized_message(
                progress=5,
                status_key="crawlingProgress.site_detecting",
                lang=user_lang
            )
            await websocket.send_json(message)
            check_cancelled()
            
            # core SiteDetector ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ í´ë°±
            if CORE_SITE_DETECTOR_AVAILABLE:
                site_detector = CoreSiteDetector()
                detected_site = await site_detector.detect_site_type(input_data)
            else:
                site_detector = DynamicSiteDetector()
                detected_site = await site_detector.detect_site_type(input_data)
                
            board_identifier = site_detector.extract_board_identifier(input_data, detected_site)
            logger.info(f"ğŸ” ìë™ ê°ì§€ëœ ì‚¬ì´íŠ¸: {detected_site}, ê²Œì‹œíŒ: {board_identifier}")

        # ì‚¬ì´íŠ¸ ì—°ê²° ë©”ì‹œì§€
        site_display = detected_site.upper() if detected_site != 'lemmy' else 'Lemmy'
        message = create_localized_message(
            progress=15,
            status_key="crawlingProgress.site_connecting",
            lang=user_lang,
            status_data={"site": site_display}
        )
        await websocket.send_json(message)
        check_cancelled()

        # ê²Œì‹œë¬¼ ìˆ˜ì§‘ ë©”ì‹œì§€
        message = create_localized_message(
            progress=30,
            status_key="crawlingProgress.posts_collecting",
            lang=user_lang,
            status_data={"site": site_display}
        )
        await websocket.send_json(message)
        check_cancelled()

        # í¬ë¡¤ë§ ì‹¤í–‰
        raw_posts = await execute_crawl_by_site(
            detected_site, 
            board_identifier, 
            **crawl_options
        )

        check_cancelled()

        if raw_posts:
            # ê²Œì‹œë¬¼ í•„í„°ë§ ë©”ì‹œì§€
            message = create_localized_message(
                progress=60,
                status_key="crawlingProgress.posts_filtering",
                lang=user_lang,
                status_data={"matched": len(raw_posts), "total": len(raw_posts)}
            )
            await websocket.send_json(message)

        # ë°ì´í„° ì²˜ë¦¬ ë©”ì‹œì§€
        message = create_localized_message(
            progress=75,
            status_key="crawlingProgress.posts_processing",
            lang=user_lang
        )
        await websocket.send_json(message)
        check_cancelled()

        if not raw_posts:
            await websocket.send_json({
                "error": f"{site_display}ì—ì„œ ê²Œì‹œë¬¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            })
            return

        # ë²ˆì—­ ì²˜ë¦¬ ë¡œì§
        results = []
        translate = crawl_options.get('translate', False)
        target_languages = crawl_options.get('target_languages', [])
        skip_translation = crawl_options.get('skip_translation', False)

        # ë²ˆì—­ ìŠ¤í‚µ ì¡°ê±´ í™•ì¸
        if skip_translation or not translate or not target_languages:
            logger.info(f"ğŸš« ë²ˆì—­ ê±´ë„ˆë›°ê¸°: skip_translation={skip_translation}")
            
            await websocket.send_json({
                "progress": 85, 
                "status": "ë²ˆì—­ ê±´ë„ˆë›°ê¸° - ë™ì¼ ì–¸ì–´"
            })
            
            for post in raw_posts:
                results.append(post)
            
        elif translate and target_languages:
            logger.info(f"âœ… ë²ˆì—­ ìˆ˜í–‰: target_languages={target_languages}")
            
            # ë²ˆì—­ ì¤€ë¹„ ë©”ì‹œì§€
            message = create_localized_message(
                progress=80,
                status_key="crawlingProgress.translation_preparing",
                lang=user_lang,
                status_data={"count": len(raw_posts)}
            )
            await websocket.send_json(message)
            
            for idx, post in enumerate(raw_posts):
                check_cancelled()
                
                original_title = post.get('ì›ì œëª©', '')
                
                # ë²ˆì—­ ìˆ˜í–‰
                for lang_code in target_languages:
                    translated = await deepl_translate(original_title, lang_code)
                    if lang_code.lower() == 'ko':
                        post['ë²ˆì—­ì œëª©'] = translated
                    else:
                        post[f'ë²ˆì—­ì œëª©_{lang_code}'] = translated
                
                results.append(post)
                
                if len(raw_posts) > 0:
                    translation_progress = 85 + int((idx + 1) / len(raw_posts) * 10)
                    
                    # ë²ˆì—­ ì§„í–‰ ë©”ì‹œì§€
                    message = create_localized_message(
                        progress=translation_progress,
                        status_key="crawlingProgress.translation_progress",
                        lang=user_lang,
                        status_data={"current": idx + 1, "total": len(raw_posts)}
                    )
                    await websocket.send_json(message)

        check_cancelled()

        # ê²°ê³¼ ì •ë¦¬ ë©”ì‹œì§€
        message = create_localized_message(
            progress=95,
            status_key="crawlingProgress.finalizing",
            lang=user_lang
        )
        await websocket.send_json(message)

        await websocket.send_json({
            "done": True,
            "data": results,
            "progress": 100,
            "detected_site": detected_site,
            "summary": f"í¬ë¡¤ë§ ì™„ë£Œ: {len(results)}ê°œ ê²Œì‹œë¬¼"
        })
        
        logger.info(f"âœ… í†µí•© í¬ë¡¤ë§ ì™„ë£Œ: {len(results)}ê°œ ({detected_site})")

    except asyncio.CancelledError:
        logger.info(f"âŒ í¬ë¡¤ë§ ì·¨ì†Œ: {crawl_id}")
        await websocket.send_json({"cancelled": True})
    except Exception as e:
        logger.error(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        await websocket.send_json({"error": str(e)})
    finally:
        crawl_manager.cleanup_crawl(crawl_id)
        await websocket.close()

# ==================== ìë™ì™„ì„± API ====================
@app.get("/autocomplete/{site}")
async def autocomplete(site: str, keyword: str = Query(...)):
    keyword = keyword.strip().lower()
    
    # BBC ìë™ì™„ì„±
    if site == "bbc" and 'detect_bbc_url_and_extract_info' in CRAWL_FUNCTIONS:
        try:
            detect_func = CRAWL_FUNCTIONS['detect_bbc_url_and_extract_info']
            bbc_detection = detect_func(keyword)
            if bbc_detection["is_bbc"]:
                return {
                    "matches": [bbc_detection["board_name"]],
                    "detected_site": "bbc",
                    "auto_detected": True
                }
        except Exception as e:
            logger.warning(f"BBC ìë™ì™„ì„± ì˜¤ë¥˜: {e}")
    
    matches = []
    
    if site == "reddit":
        reddit_subreddits = ["askreddit", "todayilearned", "funny", "pics", "worldnews", "gaming", "technology", "programming", "korea"]
        matches = [sub for sub in reddit_subreddits if keyword in sub.lower()]
    elif site == "lemmy":
        lemmy_communities = ["technology@lemmy.world", "asklemmy@lemmy.ml", "worldnews@lemmy.ml", "programming@programming.dev"]
        matches = [comm for comm in lemmy_communities if keyword in comm.lower()]
    elif site == "blind":
        blind_topics = ["ë¸”ë¼ë¸”ë¼", "íšŒì‚¬ìƒí™œ", "ììœ í† í¬", "ê°œë°œì", "ê²½ë ¥ê°œë°œ", "ì·¨ì—…/ì´ì§"]
        matches = [topic for topic in blind_topics if keyword in topic.lower()]
    elif site == "dcinside":
        dc_galleries = ["ì‹±ê¸€ë²™ê¸€", "ìœ ë¨¸", "ì •ì¹˜", "ì¶•êµ¬", "ì•¼êµ¬", "ê²Œì„", "í”„ë¡œê·¸ë˜ë°"]
        matches = [gallery for gallery in dc_galleries if keyword in gallery.lower()]
    
    return {"matches": matches[:15], "auto_detected": False}

# ==================== í¬ë¡¤ë§ ì·¨ì†Œ API ====================
class CancelRequest(BaseModel):
    crawl_id: str
    action: str = "cancel"

@app.post("/api/cancel-crawl")
async def cancel_crawl_endpoint(request: CancelRequest):
    try:
        crawl_id = request.crawl_id
        if not crawl_id:
            raise HTTPException(status_code=400, detail="crawl_idê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        crawl_manager.cancel_crawl(crawl_id)
        return {
            "success": True,
            "message": f"í¬ë¡¤ë§ {crawl_id} ì·¨ì†Œ ì™„ë£Œ",
            "crawl_id": crawl_id,
            "timestamp": time.time()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ==================== í”¼ë“œë°± ê´€ë¦¬ API ====================
@app.get("/api/feedback")
async def get_feedback_list(limit: int = 50, offset: int = 0):
    """í”¼ë“œë°± ëª©ë¡ ì¡°íšŒ (ê´€ë¦¬ììš©)"""
    
    if not DATABASE_AVAILABLE:
        return JSONResponse(
            status_code=503,
            content={"error": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
        )
    
    try:
        result = await get_feedback_from_database(limit, offset)
        return result
        
    except Exception as e:
        logger.error(f"âŒ í”¼ë“œë°± ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/database-status")
async def get_database_status_endpoint():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒíƒœ í™•ì¸"""
    
    status = {
        "database_module_available": DATABASE_AVAILABLE,
        "supabase_configured": bool(SUPABASE_URL and SUPABASE_ANON_KEY),
        "environment_variables": {
            "SUPABASE_URL": "âœ… ì„¤ì •ë¨" if SUPABASE_URL else "âŒ ë¯¸ì„¤ì •",
            "SUPABASE_ANON_KEY": "âœ… ì„¤ì •ë¨" if SUPABASE_ANON_KEY else "âŒ ë¯¸ì„¤ì •"
        }
    }
    
    if DATABASE_AVAILABLE:
        try:
            db_status = get_database_status()
            status.update(db_status)
        except Exception as e:
            status["database_error"] = str(e)
    
    return status

# ==================== ê¸°ë³¸ API ì—”ë“œí¬ì¸íŠ¸ë“¤ ====================
@app.get("/health")
def health_check():
    return {
        "status": "healthy",
        "message": "PickPost API is running",
        "static_files_routing": "Direct routing enabled",
        "version": "2.1.1-media",
        "media_download": "âœ… Available" if MEDIA_DOWNLOAD_AVAILABLE else "âŒ Unavailable"
    }

@app.get("/api/health")
async def api_health_check():
    return {
        "status": "healthy",
        "api_version": "2.1",
        "static_files_enabled": True,
        "media_download_enabled": MEDIA_DOWNLOAD_AVAILABLE,
        "endpoints": [
            "/api/check-first-visitor",
            "/api/claim-first-visitor", 
            "/api/admin/login",
            "/api/admin/first-visitor-info",
            "/api/admin/reset-first-visitor",
            "/api/download-media",
            "/api/media-info"
        ]
    }

@app.get("/api/system-info")
async def get_system_info():
    return {
        "version": "2.1.1",
        "environment": APP_ENV,
        "localized_messages": True,
        "supported_sites": list(AVAILABLE_CRAWLERS),
        "loaded_crawlers": list(CRAWL_FUNCTIONS.keys()),
        "endpoints": {
            "unified": "/ws/crawl",
            "autocomplete": "/autocomplete/{site}",
            "feedback": "/api/feedback",
            "media_download": "/api/download-media",
            "media_info": "/api/media-info"
        },
        "features": {
            "dynamic_crawler_discovery": True,
            "progress_localization": True,
            "error_localization": True,
            "multi_language_translation": True,
            "translation_skip_support": True,
            "cancellation_support": True,
            "automatic_parameter_filtering": True,
            "auto_crawler_integration": AUTO_CRAWLER_AVAILABLE,
            "core_site_detector": CORE_SITE_DETECTOR_AVAILABLE,
            "supabase_database": DATABASE_AVAILABLE,
            "enhanced_cors": True,
            "static_files_serving": True,
            "media_download": MEDIA_DOWNLOAD_AVAILABLE
        },
        "system_status": {
            "auto_crawler": AUTO_CRAWLER_AVAILABLE,
            "core_site_detector": CORE_SITE_DETECTOR_AVAILABLE,
            "available_crawlers": list(AVAILABLE_CRAWLERS),
            "functional_crawlers": list(CRAWL_FUNCTIONS.keys()),
            "database_available": DATABASE_AVAILABLE,
            "supabase_configured": bool(SUPABASE_URL and SUPABASE_ANON_KEY),
            "cors_configured": True,
            "static_files_enabled": True,
            "media_download_available": MEDIA_DOWNLOAD_AVAILABLE
        }
    }

@app.get("/api/crawlers")
async def get_available_crawlers():
    """ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë¡¤ëŸ¬ ëª©ë¡ ë° ìƒíƒœ ë°˜í™˜"""
    crawler_info = {}
    
    for crawler_name in AVAILABLE_CRAWLERS:
        crawler_info[crawler_name] = {
            "available": True,
            "functional": crawler_name in CRAWL_FUNCTIONS,
            "type": "AutoCrawler" if crawler_name == "universal" else "Direct",
            "status": "âœ… ì‚¬ìš© ê°€ëŠ¥" if crawler_name in CRAWL_FUNCTIONS or crawler_name == "universal" else "âš ï¸ í•¨ìˆ˜ ì—†ìŒ"
        }
    
    return {
        "total_discovered": len(AVAILABLE_CRAWLERS),
        "functional_count": len(CRAWL_FUNCTIONS) + (1 if AUTO_CRAWLER_AVAILABLE else 0),
        "crawlers": crawler_info,
        "discovery_method": "Dynamic file scanning",
        "auto_crawler_enabled": AUTO_CRAWLER_AVAILABLE,
        "database_available": DATABASE_AVAILABLE,
        "cors_enabled": True,
        "static_files_enabled": True,
        "media_download_enabled": MEDIA_DOWNLOAD_AVAILABLE
    }

# ==================== ì„œë²„ ì‹œì‘ ====================
if __name__ == "__main__":
    import uvicorn
    
    logger.info("ğŸš€ PickPost v2.1.1 ì„œë²„ ì‹œì‘ (ë¯¸ë””ì–´ ë‹¤ìš´ë¡œë“œ ì§€ì›)")
    logger.info(f"   ì •ì  íŒŒì¼ ë¼ìš°íŒ…: âœ… ì§ì ‘ ì—”ë“œí¬ì¸íŠ¸ ë°©ì‹")
    logger.info(f"   JavaScript íŒŒì¼: /js/{{filename}} -> serve_js_files()")
    logger.info(f"   CSS íŒŒì¼: /css/{{filename}} -> serve_css_files()")
    logger.info(f"   AutoCrawler: {'âœ…' if AUTO_CRAWLER_AVAILABLE else 'âŒ'}")
    logger.info(f"   Core SiteDetector: {'âœ…' if CORE_SITE_DETECTOR_AVAILABLE else 'âŒ'}")
    logger.info(f"   Database: {'âœ…' if DATABASE_AVAILABLE else 'âŒ'}")
    logger.info(f"   Media Download: {'âœ…' if MEDIA_DOWNLOAD_AVAILABLE else 'âŒ'}")
    logger.info(f"   ë°œê²¬ëœ í¬ë¡¤ëŸ¬: {list(AVAILABLE_CRAWLERS)}")
    logger.info(f"   ë¡œë“œëœ í•¨ìˆ˜: {list(CRAWL_FUNCTIONS.keys())}")
    
    if MEDIA_DOWNLOAD_AVAILABLE:
        try:
            manager = get_media_download_manager()
            supported_sites = manager.extractor_manager.get_supported_sites()
            logger.info(f"   ë¯¸ë””ì–´ ì§€ì› ì‚¬ì´íŠ¸: {supported_sites}")
        except Exception as e:
            logger.warning(f"   ë¯¸ë””ì–´ ë§¤ë‹ˆì € ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    if not AUTO_CRAWLER_AVAILABLE and not CRAWL_FUNCTIONS:
        logger.error("âŒ ëª¨ë“  í¬ë¡¤ë§ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
    
    uvicorn.run(app, host="0.0.0.0", port=PORT)