# dcinside.py - ìˆœìˆ˜ í¬ë¡¤ë§ ê¸°ëŠ¥ë§Œ ìˆ˜í–‰

import requests
from bs4 import BeautifulSoup
import json
import os
import re
import asyncio
from datetime import datetime, timedelta
from typing import Tuple, List, Dict, Optional

# ==================== ì„¤ì • ë° ê²½ë¡œ ====================

# JSON íŒŒì¼ ê²½ë¡œë“¤
GALLERIES_JSON_PATH = os.path.join("crawlers", "id_data", "id_data", "galleries.json")
MGALLERIES_JSON_PATH = os.path.join("crawlers", "id_data", "id_data", "mgalleries.json")

# í¬ë¡¤ë§ ì„¤ì •
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

# ==================== ê°¤ëŸ¬ë¦¬ ë°ì´í„° ê´€ë¦¬ ====================

def load_separated_gallery_data() -> Tuple[dict, dict]:
    """ì¼ë°˜ ê°¤ëŸ¬ë¦¬ì™€ ë§ˆì´ë„ˆ ê°¤ëŸ¬ë¦¬ë¥¼ ë¶„ë¦¬í•´ì„œ ë¡œë“œ"""
    galleries_data = {}
    mgalleries_data = {}
    
    # ì¼ë°˜ ê°¤ëŸ¬ë¦¬ ë¡œë“œ
    if os.path.exists(GALLERIES_JSON_PATH):
        try:
            with open(GALLERIES_JSON_PATH, "r", encoding="utf-8") as f:
                raw_data = json.load(f)
                for gallery_name, gallery_id in raw_data.items():
                    galleries_data[gallery_name] = {
                        "id": gallery_id,
                        "type": "gallery"
                    }
                print(f"âœ… ì¼ë°˜ ê°¤ëŸ¬ë¦¬ ë¡œë“œ: {len(galleries_data)}ê°œ")
        except Exception as e:
            print(f"âŒ galleries.json íŒŒì‹± ì˜¤ë¥˜: {e}")
    
    # ë§ˆì´ë„ˆ ê°¤ëŸ¬ë¦¬ ë¡œë“œ
    if os.path.exists(MGALLERIES_JSON_PATH):
        try:
            with open(MGALLERIES_JSON_PATH, "r", encoding="utf-8") as f:
                raw_data = json.load(f)
                for gallery_name, gallery_id in raw_data.items():
                    mgalleries_data[gallery_name] = {
                        "id": gallery_id,
                        "type": "mgallery"
                    }
                print(f"âœ… ë§ˆì´ë„ˆ ê°¤ëŸ¬ë¦¬ ë¡œë“œ: {len(mgalleries_data)}ê°œ")
        except Exception as e:
            print(f"âŒ mgalleries.json íŒŒì‹± ì˜¤ë¥˜: {e}")
    
    return galleries_data, mgalleries_data

def resolve_dc_board_id(board_input: str) -> Tuple[str, str]:
    """ê°¤ëŸ¬ë¦¬ëª…/IDë¥¼ ì‹¤ì œ ê°¤ëŸ¬ë¦¬ IDì™€ íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
    if not board_input:
        raise Exception("ê°¤ëŸ¬ë¦¬ëª… ë˜ëŠ” IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    board_input = board_input.strip()
    board_input_lower = board_input.lower()
    
    galleries_data, mgalleries_data = load_separated_gallery_data()
    
    if not galleries_data and not mgalleries_data:
        raise Exception("ê°¤ëŸ¬ë¦¬ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    def search_in_gallery_data(gallery_data: dict, gallery_type: str) -> Optional[Tuple[str, str]]:
        # 1. ì •í™•í•œ ID ë§¤ì¹˜
        for name, info in gallery_data.items():
            if board_input_lower == info['id'].lower():
                return info['id'], gallery_type
        
        # 2. ì •í™•í•œ ê°¤ëŸ¬ë¦¬ëª… ë§¤ì¹˜
        for name, info in gallery_data.items():
            if board_input_lower == name.lower():
                return info['id'], gallery_type
        
        # 3. ë¶€ë¶„ ë§¤ì¹˜
        matches = []
        for name, info in gallery_data.items():
            name_lower = name.lower()
            if board_input_lower in name_lower or name_lower.startswith(board_input_lower):
                matches.append((name, info))
        
        if matches:
            best_match = min(matches, key=lambda x: len(x[0]))
            return best_match[1]['id'], gallery_type
            
        return None
    
    # ë§ˆì´ë„ˆ ê°¤ëŸ¬ë¦¬ ìš°ì„  ê²€ìƒ‰
    if mgalleries_data:
        result = search_in_gallery_data(mgalleries_data, "mgallery")
        if result:
            return result
    
    # ì¼ë°˜ ê°¤ëŸ¬ë¦¬ ê²€ìƒ‰
    if galleries_data:
        result = search_in_gallery_data(galleries_data, "gallery")
        if result:
            return result
    
    raise Exception(f"'{board_input}'ì™€ ë§¤ì¹­ë˜ëŠ” ê°¤ëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ==================== ê²Œì‹œë¬¼ ë°ì´í„° ì¶”ì¶œ ====================

def extract_post_metrics(item) -> Tuple[int, int, int]:
    """ê²Œì‹œë¬¼ì—ì„œ ì¡°íšŒìˆ˜, ì¶”ì²œìˆ˜, ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ"""
    views = likes = comments = 0

    try:
        # ì¡°íšŒìˆ˜ ì¶”ì¶œ
        view_selectors = ['.gall_count', '.view_count', '.hit']
        for selector in view_selectors:
            elements = item.select(selector)
            for element in elements:
                numbers = re.findall(r'\d+', element.text.strip())
                if numbers:
                    views = int(numbers[0])
                    break
            if views > 0:
                break

        # ì¶”ì²œìˆ˜ ì¶”ì¶œ
        like_selectors = ['.gall_recommend', '.recommend_count', '.up_num']
        for selector in like_selectors:
            elements = item.select(selector)
            for element in elements:
                numbers = re.findall(r'\d+', element.text.strip())
                if numbers:
                    likes = int(numbers[0])
                    break
            if likes > 0:
                break

        # ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ
        comment_selectors = ['.gall_reply_num', '.reply_num', '.comment_count']
        for selector in comment_selectors:
            elements = item.select(selector)
            for element in elements:
                numbers = re.findall(r'\d+', element.text.strip())
                if numbers:
                    comments = int(numbers[0])
                    break
            if comments > 0:
                break

    except Exception as e:
        print(f"ë©”íŠ¸ë¦­ ì¶”ì¶œ ì˜¤ë¥˜: {e}")

    return views, likes, comments

def extract_dcinside_post_data(item) -> Optional[Dict]:
    """ê°œë³„ DCInside ê²Œì‹œë¬¼ ë°ì´í„° ì¶”ì¶œ"""
    try:
        # ì œëª© ì¶”ì¶œ
        title_selectors = ['.gall_tit a', '.ub-word a', 'td.gall_tit a', '.title a', '.subject a']
        title_element = None
        for selector in title_selectors:
            title_element = item.select_one(selector)
            if title_element:
                break
        
        if not title_element:
            return None
        
        title = title_element.text.strip()
        link = title_element.get('href', '')
        
        # ì ˆëŒ€ URLë¡œ ë³€í™˜
        if link.startswith('/'):
            link = f"https://gall.dcinside.com{link}"
        elif not link.startswith('http'):
            link = f"https://gall.dcinside.com/{link}"
        
        # ë©”íŠ¸ë¦­ ì¶”ì¶œ
        views, likes, comments = extract_post_metrics(item)
        
        # ì‘ì„±ì¼ ì¶”ì¶œ
        date_element = item.select_one('.gall_date, .date, .posting_time')
        date_str = date_element.text.strip() if date_element else "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
        
        # ì‘ì„±ì ì¶”ì¶œ
        author_element = item.select_one('.gall_writer, .writer, .nickname')
        author = author_element.text.strip() if author_element else "ìµëª…"
        
        return {
            "ì›ì œëª©": title,
            "ë²ˆì—­ì œëª©": None,
            "ë§í¬": link,
            "ë³¸ë¬¸": "",
            "ì¸ë„¤ì¼ URL": None,
            "ì¡°íšŒìˆ˜": views,
            "ì¶”ì²œìˆ˜": likes,
            "ëŒ“ê¸€ìˆ˜": comments,
            "ì‘ì„±ì¼": date_str,
            "ì‘ì„±ì": author,
            "í¬ë¡¤ë§ë°©ì‹": "DCInside-Enhanced"
        }
    
    except Exception as e:
        print(f"ê²Œì‹œë¬¼ ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return None

# ==================== ë‚ ì§œ ë° ì¡°ê±´ ì²˜ë¦¬ ====================

def parse_dc_date(date_text: str) -> Optional[datetime]:
    """DCInside ë‚ ì§œ íŒŒì‹±"""
    if not date_text:
        return None
    
    try:
        now = datetime.now()
        date_text = date_text.strip()
        
        # ìƒëŒ€ì  ì‹œê°„ íŒŒì‹±
        if 'ë¶„ì „' in date_text or 'ë¶„ ì „' in date_text:
            numbers = re.findall(r'\d+', date_text)
            if numbers:
                minutes = int(numbers[0])
                return now - timedelta(minutes=minutes)
        elif 'ì‹œê°„ì „' in date_text or 'ì‹œê°„ ì „' in date_text:
            numbers = re.findall(r'\d+', date_text)
            if numbers:
                hours = int(numbers[0])
                return now - timedelta(hours=hours)
        elif 'ì¼ì „' in date_text or 'ì¼ ì „' in date_text:
            numbers = re.findall(r'\d+', date_text)
            if numbers:
                days = int(numbers[0])
                return now - timedelta(days=days)
        
        # ì ˆëŒ€ì  ë‚ ì§œ íŒŒì‹±
        date_patterns = [
            r'(\d{4})\.(\d{1,2})\.(\d{1,2})',  # YYYY.MM.DD
            r'(\d{1,2})\.(\d{1,2})',          # MM.DD (ì˜¬í•´)
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, date_text)
            if match:
                groups = match.groups()
                try:
                    if len(groups) == 3:
                        year, month, day = map(int, groups)
                    elif len(groups) == 2:
                        year = now.year
                        month, day = map(int, groups)
                        if month > 12:
                            month, day = day, month
                    else:
                        continue
                    
                    return datetime(year, month, day)
                except ValueError:
                    continue
    
    except Exception:
        pass
    
    return None

class DCInsideConditionChecker:
    """DCInside ê²Œì‹œë¬¼ ì¡°ê±´ ì²´í¬ í´ë˜ìŠ¤"""
    
    def __init__(self, min_views: int = 0, min_likes: int = 0, min_comments: int = 0,
                 start_date: str = None, end_date: str = None):
        self.min_views = min_views
        self.min_likes = min_likes  
        self.min_comments = min_comments
        
        # ë‚ ì§œ ë²”ìœ„ íŒŒì‹±
        self.start_dt = None
        self.end_dt = None
        if start_date and end_date:
            try:
                self.start_dt = datetime.strptime(start_date, '%Y-%m-%d')
                self.end_dt = datetime.strptime(end_date, '%Y-%m-%d')
                self.end_dt = self.end_dt.replace(hour=23, minute=59, second=59)
            except Exception as e:
                print(f"ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {e}")

    def check_post_conditions(self, post: Dict) -> Tuple[bool, str]:
        """ê²Œì‹œë¬¼ì´ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ”ì§€ í™•ì¸"""
        try:
            # ì¡°íšŒìˆ˜ ì²´í¬
            if post.get('ì¡°íšŒìˆ˜', 0) < self.min_views:
                return False, f"ì¡°íšŒìˆ˜ ë¶€ì¡±: {post.get('ì¡°íšŒìˆ˜', 0)} < {self.min_views}"
            
            # ì¶”ì²œìˆ˜ ì²´í¬
            if post.get('ì¶”ì²œìˆ˜', 0) < self.min_likes:
                return False, f"ì¶”ì²œìˆ˜ ë¶€ì¡±: {post.get('ì¶”ì²œìˆ˜', 0)} < {self.min_likes}"
            
            # ëŒ“ê¸€ìˆ˜ ì²´í¬
            if post.get('ëŒ“ê¸€ìˆ˜', 0) < self.min_comments:
                return False, f"ëŒ“ê¸€ìˆ˜ ë¶€ì¡±: {post.get('ëŒ“ê¸€ìˆ˜', 0)} < {self.min_comments}"
            
            # ë‚ ì§œ ë²”ìœ„ ì²´í¬
            if self.start_dt and self.end_dt:
                post_date = parse_dc_date(post.get('ì‘ì„±ì¼', ''))
                if not post_date:
                    return False, "ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨"
                
                if not (self.start_dt <= post_date <= self.end_dt):
                    return False, "ë‚ ì§œ ë²”ìœ„ ë²—ì–´ë‚¨"
            
            return True, "ì¡°ê±´ ë§Œì¡±"
        
        except Exception as e:
            return False, f"ì¡°ê±´ ì²´í¬ ì˜¤ë¥˜: {e}"

    def should_stop_crawling(self, consecutive_fails: int, has_date_filter: bool) -> Tuple[bool, str]:
        """í¬ë¡¤ë§ ì¤‘ë‹¨ ì—¬ë¶€ ê²°ì •"""
        fail_threshold = 10 if has_date_filter else 20
        if consecutive_fails >= fail_threshold:
            return True, "ì¡°ê±´ ë¶ˆë§Œì¡±ìœ¼ë¡œ ì¤‘ë‹¨"
        return False, "ê³„ì† ì§„í–‰"

# ==================== í¬ë¡¤ë§ ì‹¤í–‰ ====================

async def crawl_dcinside_page(base_url: str, page: int) -> List[Dict]:
    """DCInside ë‹¨ì¼ í˜ì´ì§€ í¬ë¡¤ë§"""
    page_url = f"{base_url}{'&' if '?' in base_url else '?'}page={page}"
    
    try:
        response = requests.get(page_url, headers=HEADERS, timeout=10)
        if response.status_code != 200:
            return []
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸ ì…€ë ‰í„°
        item_selectors = [
            'tr.ub-content',  # ì¼ë°˜ ê°¤ëŸ¬ë¦¬
            'tr.us-post',     # ë§ˆì´ë„ˆ ê°¤ëŸ¬ë¦¬  
            '.gall_list tr',  # ê¸°ë³¸ ê°¤ëŸ¬ë¦¬ ë¦¬ìŠ¤íŠ¸
            'tbody tr'        # ì¼ë°˜ì ì¸ í…Œì´ë¸” í–‰
        ]
        
        items = []
        for selector in item_selectors:
            items = soup.select(selector)
            if items:
                break
        
        posts = []
        for item in items:
            post_data = extract_dcinside_post_data(item)
            if post_data:
                posts.append(post_data)
        
        return posts
    
    except Exception as e:
        print(f"í˜ì´ì§€ {page} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        return []

def get_dcinside_sort_params(sort_method: str) -> str:
    """DCInside ì •ë ¬ íŒŒë¼ë¯¸í„° ìƒì„±"""
    dc_sort_map = {
        "recommend": "sort_type=recommend&order=desc",
        "popular": "sort_type=hit&order=desc",
        "comments": "sort_type=reply&order=desc",
        "recent": "",
    }
    return dc_sort_map.get(sort_method, "")

async def execute_dcinside_crawling(
    board_name: str, condition_checker: DCInsideConditionChecker, 
    sort: str, start_index: int, end_index: int, websocket=None, enforce_date_limit=False
) -> List[Dict]:
    """DCInside í¬ë¡¤ë§ ì‹¤í–‰"""
    
    # ê°¤ëŸ¬ë¦¬ ID ë° íƒ€ì… í•´ê²°
    board_id, board_type = resolve_dc_board_id(board_name)
    
    # URL êµ¬ì„±
    if board_type == "mgallery":
        base_url = f"https://gall.dcinside.com/mgallery/board/lists/?id={board_id}"
        print(f"ğŸ¯ ë§ˆì´ë„ˆ ê°¤ëŸ¬ë¦¬ ì ‘ê·¼: mgallery/{board_id}")
    else:
        base_url = f"https://gall.dcinside.com/board/lists/?id={board_id}"
        print(f"ğŸ¯ ì¼ë°˜ ê°¤ëŸ¬ë¦¬ ì ‘ê·¼: board/{board_id}")
    
    # ì •ë ¬ íŒŒë¼ë¯¸í„° ì¶”ê°€
    sort_params = get_dcinside_sort_params(sort)
    if sort_params:
        base_url += f"&{sort_params}"

    all_posts = []
    matched_posts = []
    consecutive_fails = 0
    page = 1
    max_pages = 200 if enforce_date_limit else min(20, (end_index // 20) + 3)
    
    while page <= max_pages:
        try:
            # í˜ì´ì§€ë³„ ì§„í–‰ë¥  ë©”ì‹œì§€ (WebSocketìœ¼ë¡œ ì¤‘ê°„ ì§„í–‰ë¥  ì „ì†¡)
            if websocket:
                page_progress = 25 + int((page / max_pages) * 50)  # 25%~75% êµ¬ê°„
                try:
                    await websocket.send_json({
                        "progress": page_progress,
                        "status": f"DCInside í˜ì´ì§€ {page}/{max_pages} ìˆ˜ì§‘ ì¤‘... (ë§¤ì¹­: {len(matched_posts)}ê°œ)"
                    })
                except Exception as ws_error:
                    print(f"WebSocket ë©”ì‹œì§€ ì „ì†¡ ì˜¤ë¥˜: {ws_error}")
            
            page_posts = await crawl_dcinside_page(base_url, page)
            
            if not page_posts:
                consecutive_fails += 1
                if consecutive_fails >= 3:
                    break
                page += 1
                continue
            
            consecutive_fails = 0
            print(f"âœ… í˜ì´ì§€ {page}: {len(page_posts)}ê°œ ê²Œì‹œë¬¼ ìˆ˜ì§‘")
            
            # ê²Œì‹œë¬¼ ì²˜ë¦¬ ë° í•„í„°ë§
            for post in page_posts:
                all_posts.append(post)
                
                # ì¡°ê±´ ì²´í¬
                is_valid, reason = condition_checker.check_post_conditions(post)
                if is_valid:
                    matched_posts.append(post)
                    
                    # ëª©í‘œ ê°œìˆ˜ ë‹¬ì„±ì‹œ ì¤‘ë‹¨
                    if len(matched_posts) >= (end_index - start_index + 1):
                        break
            
            # ì¤‘ë‹¨ ì¡°ê±´ ì²´í¬
            should_stop, stop_reason = condition_checker.should_stop_crawling(
                consecutive_fails, bool(condition_checker.start_dt and condition_checker.end_dt)
            )
            if should_stop:
                break
            
            # ëª©í‘œ ê°œìˆ˜ ë‹¬ì„±ì‹œ ì¤‘ë‹¨
            if len(matched_posts) >= (end_index - start_index + 1):
                break
            
            page += 1
            
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ {page} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            consecutive_fails += 1
            page += 1
            if consecutive_fails > 5:
                break
    
    # ìµœì¢… ê²°ê³¼ ìŠ¬ë¼ì´ì‹± ë° ë²ˆí˜¸ ë¶€ì—¬
    final_posts = matched_posts[start_index-1:end_index] if start_index <= len(matched_posts) else matched_posts
    
    for idx, post in enumerate(final_posts):
        post['ë²ˆí˜¸'] = start_index + idx
    
    print(f"âœ… DCInside í¬ë¡¤ë§ ì™„ë£Œ: ì „ì²´ {len(all_posts)}ê°œ â†’ ë§¤ì¹­ {len(matched_posts)}ê°œ â†’ ìµœì¢… {len(final_posts)}ê°œ")
    
    return final_posts

# ==================== ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜ ====================

async def crawl_dcinside_board(
    board_name: str, limit: int = 50, sort: str = "recent", 
    min_views: int = 0, min_likes: int = 0, min_comments: int = 0,
    time_filter: str = "all", start_date: str = None, 
    end_date: str = None, websocket=None, enforce_date_limit=False,
    start_index: int = 1, end_index: int = 20
) -> List[Dict]:
    """DCInside í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜"""
    
    try:
        print(f"ğŸ¯ DCInside í¬ë¡¤ë§ ì‹œì‘ - ì…ë ¥ê°’: '{board_name}'")
        
        # ì¡°ê±´ ì²´ì»¤ ì´ˆê¸°í™”
        condition_checker = DCInsideConditionChecker(
            min_views=min_views,
            min_likes=min_likes, 
            min_comments=min_comments,
            start_date=start_date,
            end_date=end_date
        )
        
        # í•„í„° ì ìš© ì—¬ë¶€ í™•ì¸
        has_filters = (min_views > 0 or min_likes > 0 or min_comments > 0 or 
                      (start_date and end_date))
        
        if has_filters:
            enforce_date_limit = True
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        final_posts = await execute_dcinside_crawling(
            board_name, condition_checker, sort,
            start_index, end_index, websocket, enforce_date_limit
        )
        
        print(f"âœ… DCInside í¬ë¡¤ë§ ì™„ë£Œ: {len(final_posts)}ê°œ ê²Œì‹œë¬¼")
        return final_posts
    
    except Exception as e:
        error_msg = f"DCInside í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}"
        print(error_msg)
        return []

# ==================== ëª¨ë“ˆ ì •ë³´ ====================

__all__ = [
    'crawl_dcinside_board',
    'resolve_dc_board_id',
    'get_gallery_statistics'
]

def get_gallery_statistics() -> Dict[str, any]:
    """ê°¤ëŸ¬ë¦¬ í†µê³„ ì •ë³´ ë°˜í™˜"""
    galleries_data, mgalleries_data = load_separated_gallery_data()
    
    return {
        "total_galleries": len(galleries_data) + len(mgalleries_data),
        "regular_galleries": len(galleries_data),
        "minor_galleries": len(mgalleries_data),
        "files_status": {
            "galleries_json": os.path.exists(GALLERIES_JSON_PATH),
            "mgalleries_json": os.path.exists(MGALLERIES_JSON_PATH)
        }
    }