# core/site_detector.py - í†µí•© ë™ì  ì‚¬ì´íŠ¸ ê°ì§€ê¸° (4chan ì™„ì „ ì§€ì›)
"""
ğŸ” í†µí•© ë™ì  ì‚¬ì´íŠ¸ ê°ì§€ê¸°
- ìë™ìœ¼ë¡œ URL/ì…ë ¥ì—ì„œ ì‚¬ì´íŠ¸ íƒ€ì… ê°ì§€
- ë™ì  í¬ë¡¤ëŸ¬ íƒì§€ ë° ë“±ë¡
- ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë¡¤ëŸ¬ì™€ ì—°ë™
- 4chan ì™„ì „ ì§€ì› ì¶”ê°€
"""

import logging
import importlib.util
from urllib.parse import urlparse
from pathlib import Path
from typing import Optional, Dict, Set, List, Tuple, Any
import asyncio
import aiohttp

logger = logging.getLogger(__name__)

class DynamicSiteDetector:
    """í†µí•© ë™ì  ì‚¬ì´íŠ¸ ê°ì§€ê¸°"""
    
    def __init__(self, crawlers_dir: Optional[Path] = None):
        """
        Args:
            crawlers_dir: í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ê¸°ë³¸ê°’: backend/crawlers/)
        """
        # í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ ì„¤ì •
        if crawlers_dir:
            self.crawlers_dir = crawlers_dir
        else:
            # main.pyì—ì„œ í˜¸ì¶œë  ë•Œì˜ ê¸°ë³¸ ê²½ë¡œ
            current_file = Path(__file__).parent  # core/
            self.crawlers_dir = current_file.parent / "crawlers"  # backend/crawlers/
        
        # ë™ì ìœ¼ë¡œ ë°œê²¬ëœ ì •ë³´
        self.available_crawlers: Set[str] = set()
        self.crawl_functions: Dict[str, Any] = {}
        self.crawler_metadata: Dict[str, Dict] = {}
        
        # ê¸°ë³¸ ì‚¬ì´íŠ¸ íŒ¨í„´ (ë°±ì—…ìš©) - 4chan ì¶”ê°€
        self.fallback_patterns = {
            'reddit': {
                'domains': ['reddit.com', 'www.reddit.com', 'old.reddit.com', 'new.reddit.com'],
                'keywords': ['reddit', 'subreddit', '/r/'],
                'url_patterns': [r'/r/([^/]+)']
            },
            'dcinside': {
                'domains': ['dcinside.com', 'gall.dcinside.com', 'm.dcinside.com'],
                'keywords': ['dcinside', 'dcin', 'ë””ì‹œ', 'ê°¤ëŸ¬ë¦¬'],
                'url_patterns': [r'[?&]id=([^&]+)']
            },
            'blind': {
                'domains': ['teamblind.com', 'blind.com', 'www.teamblind.com'],
                'keywords': ['blind', 'ë¸”ë¼ì¸ë“œ', 'teamblind'],
                'url_patterns': []
            },
            'bbc': {
                'domains': ['bbc.com', 'www.bbc.com', 'bbc.co.uk', 'www.bbc.co.uk'],
                'keywords': ['bbc', 'british broadcasting'],
                'url_patterns': []
            },
            'lemmy': {
                'domains': [
                    'lemmy.world', 'lemmy.ml', 'beehaw.org', 'sh.itjust.works',
                    'feddit.de', 'lemm.ee', 'sopuli.xyz', 'lemmy.ca'
                ],
                'keywords': ['lemmy', 'fediverse', '@lemmy'],
                'url_patterns': [r'/c/([^/]+)']
            },
            # ğŸ”¥ 4chan íŒ¨í„´ ì¶”ê°€
            '4chan': {
                'domains': [
                    '4chan.org', 'boards.4chan.org', 'www.4chan.org',
                    '4channel.org', 'boards.4channel.org', 'www.4channel.org'
                ],
                'keywords': ['4chan', '4channel', 'imageboard', '/g/', '/v/', '/a/', '/pol/', '/b/', '/mu/', '/fit/'],
                'url_patterns': [
                    r'/([a-z0-9]+)/?$',           # /a/ í˜•íƒœ
                    r'/([a-z0-9]+)/thread/(\d+)', # /a/thread/12345 í˜•íƒœ
                    r'/([a-z0-9]+)/catalog'       # /a/catalog í˜•íƒœ
                ]
            }
        }
        
        # Lemmy ì¸ìŠ¤í„´ìŠ¤ ìºì‹œ
        self.lemmy_instances_cache = set()
        self.cache_initialized = False
        
        # ì´ˆê¸°í™”
        self._initialize()
    
    def _initialize(self):
        """ì´ˆê¸°í™”: ë™ì  í¬ë¡¤ëŸ¬ íƒì§€"""
        logger.info(f"ğŸ” ë™ì  ì‚¬ì´íŠ¸ ê°ì§€ê¸° ì´ˆê¸°í™”: {self.crawlers_dir}")
        
        # 1. í¬ë¡¤ëŸ¬ íŒŒì¼ íƒì§€
        self._discover_crawlers()
        
        # 2. í¬ë¡¤ëŸ¬ í•¨ìˆ˜ ë¡œë“œ
        self._load_crawler_functions()
        
        # 3. ë¡œê·¸ ì¶œë ¥
        logger.info(f"âœ… ë°œê²¬ëœ í¬ë¡¤ëŸ¬: {sorted(self.available_crawlers)}")
        logger.info(f"âœ… ë¡œë“œëœ í•¨ìˆ˜: {list(self.crawl_functions.keys())}")
    
    def _discover_crawlers(self):
        """í¬ë¡¤ëŸ¬ íŒŒì¼ë“¤ì„ ë™ì ìœ¼ë¡œ ë°œê²¬"""
        if not self.crawlers_dir.exists():
            logger.warning(f"âš ï¸ í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ ì—†ìŒ: {self.crawlers_dir}")
            return
        
        for py_file in self.crawlers_dir.glob("*.py"):
            # ì œì™¸í•  íŒŒì¼ë“¤
            if py_file.name.startswith('_') or py_file.stem == '__init__':
                continue
            
            crawler_name = py_file.stem
            
            try:
                # íŒŒì¼ì—ì„œ í¬ë¡¤ë§ í•¨ìˆ˜ í™•ì¸
                spec = importlib.util.spec_from_file_location(f"crawlers.{crawler_name}", py_file)
                if spec and spec.loader:
                    module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(module)
                    
                    # ğŸ”¥ 4chan ì „ìš© íŒ¨í„´ ì¶”ê°€
                    crawl_function_patterns = [
                        f'crawl_{crawler_name}_board',
                        f'crawl_4chan_board',  # 4chan ì „ìš© íŒ¨í„´
                        f'crawl_4chan_board',     # í‘œì¤€ 4chan íŒ¨í„´
                        f'fetch_posts',
                        f'crawl_{crawler_name}',
                        f'{crawler_name}_crawl'
                    ]
                    
                    has_crawl_function = any(hasattr(module, func) for func in crawl_function_patterns)
                    
                    if has_crawl_function:
                        self.available_crawlers.add(crawler_name)
                        
                        # ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
                        self.crawler_metadata[crawler_name] = {
                            'file_path': str(py_file),
                            'display_name': getattr(module, 'DISPLAY_NAME', crawler_name.title()),
                            'description': getattr(module, 'DESCRIPTION', f'{crawler_name} í¬ë¡¤ëŸ¬'),
                            'version': getattr(module, 'VERSION', '1.0.0'),
                            'supported_domains': getattr(module, 'SUPPORTED_DOMAINS', []),
                            'keywords': getattr(module, 'KEYWORDS', [])
                        }
                        
                        logger.debug(f"âœ… í¬ë¡¤ëŸ¬ ë°œê²¬: {crawler_name}")
                    else:
                        logger.debug(f"âš ï¸ í¬ë¡¤ë§ í•¨ìˆ˜ ì—†ìŒ: {crawler_name}")
            
            except Exception as e:
                logger.debug(f"âš ï¸ í¬ë¡¤ëŸ¬ í™•ì¸ ì‹¤íŒ¨ {crawler_name}: {e}")
        
        # AutoCrawler universal ê¸°ëŠ¥ ì¶”ê°€
        self.available_crawlers.add('universal')
        self.crawler_metadata['universal'] = {
            'display_name': 'Universal Crawler',
            'description': 'AutoCrawler ë²”ìš© í¬ë¡¤ë§',
            'version': '2.0.0',
            'supported_domains': ['*'],
            'keywords': ['universal', 'generic']
        }
    
    def _load_crawler_functions(self):
        """í¬ë¡¤ëŸ¬ í•¨ìˆ˜ë“¤ì„ ë™ì ìœ¼ë¡œ ë¡œë“œ"""
        # ê°œë³„ í¬ë¡¤ëŸ¬ í•¨ìˆ˜ë“¤ import ì‹œë„ - 4chan ì¶”ê°€
        crawler_imports = {
            'reddit': ('crawlers.reddit', 'fetch_posts'),
            'dcinside': ('crawlers.dcinside', 'crawl_dcinside_board'),
            'blind': ('crawlers.blind', 'crawl_blind_board'),
            'lemmy': ('crawlers.lemmy', 'crawl_lemmy_board'),
            'bbc': ('crawlers.bbc', 'crawl_bbc_board'),
            '4chan': ('crawlers.4chan', 'crawl_4chan_board'),  # ğŸ”¥ 4chan ì¶”ê°€
        }
        
        for crawler_name, (module_path, function_name) in crawler_imports.items():
            if crawler_name in self.available_crawlers:
                try:
                    module = __import__(module_path, fromlist=[function_name])
                    crawler_function = getattr(module, function_name)
                    self.crawl_functions[crawler_name] = crawler_function
                    logger.debug(f"âœ… {crawler_name} í•¨ìˆ˜ ë¡œë“œ ì„±ê³µ")
                    
                    # BBCì˜ ê²½ìš° ì¶”ê°€ í•¨ìˆ˜ë„ ë¡œë“œ
                    if crawler_name == 'bbc' and hasattr(module, 'detect_bbc_url_and_extract_info'):
                        self.crawl_functions['detect_bbc_url_and_extract_info'] = getattr(module, 'detect_bbc_url_and_extract_info')
                    
                    # ğŸ”¥ 4chanì˜ ê²½ìš° ì¶”ê°€ í•¨ìˆ˜ë„ ë¡œë“œ
                    if crawler_name == '4chan' and hasattr(module, 'detect_4chan_url_and_extract_info'):
                        self.crawl_functions['detect_4chan_url_and_extract_info'] = getattr(module, 'detect_4chan_url_and_extract_info')
                
                except ImportError as e:
                    logger.debug(f"âš ï¸ {crawler_name} í•¨ìˆ˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
                except Exception as e:
                    logger.debug(f"âš ï¸ {crawler_name} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    async def detect_site_type(self, url_or_input: str) -> str:
        """URL ë˜ëŠ” ì…ë ¥ì—ì„œ ì‚¬ì´íŠ¸ íƒ€ì… ìë™ ê°ì§€"""
        if not url_or_input:
            return "universal"
        
        url_or_input = url_or_input.lower().strip()
        
        # 1. URL ê¸°ë°˜ ê°ì§€
        if url_or_input.startswith('http'):
            detected = await self._detect_by_url(url_or_input)
            if detected != 'universal':
                return detected
        
        # 2. í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì§€
        detected = self._detect_by_keyword(url_or_input)
        if detected != 'universal':
            return detected
        
        # 3. ë™ì  í¬ë¡¤ëŸ¬ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê°ì§€
        detected = self._detect_by_crawler_metadata(url_or_input)
        if detected != 'universal':
            return detected
        
        # 4. ê¸°ë³¸ê°’
        logger.info(f"â“ ì•Œ ìˆ˜ ì—†ëŠ” ì…ë ¥: {url_or_input[:50]} â†’ universalë¡œ ì²˜ë¦¬")
        return 'universal'
    
    async def _detect_by_url(self, url: str) -> str:
        """URL ê¸°ë°˜ ì‚¬ì´íŠ¸ ê°ì§€"""
        try:
            domain = urlparse(url).netloc.lower()
            
            # 1. ê¸°ë³¸ íŒ¨í„´ í™•ì¸
            for site_type, patterns in self.fallback_patterns.items():
                if any(domain_pattern in domain for domain_pattern in patterns['domains']):
                    logger.debug(f"ğŸ¯ ë„ë©”ì¸ ë§¤ì¹­: {site_type} ({domain})")
                    return site_type
            
            # 2. ë™ì  í¬ë¡¤ëŸ¬ ë©”íƒ€ë°ì´í„° í™•ì¸
            for crawler_name, metadata in self.crawler_metadata.items():
                supported_domains = metadata.get('supported_domains', [])
                if any(supported_domain in domain for supported_domain in supported_domains):
                    logger.debug(f"ğŸ¯ ë™ì  í¬ë¡¤ëŸ¬ ë„ë©”ì¸ ë§¤ì¹­: {crawler_name} ({domain})")
                    return crawler_name
            
            # 3. Lemmy ì¸ìŠ¤í„´ìŠ¤ ë™ì  í™•ì¸
            if await self._is_lemmy_instance(domain):
                logger.info(f"ğŸ¯ Lemmy ì¸ìŠ¤í„´ìŠ¤ ê°ì§€: {domain}")
                return 'lemmy'
            
            return 'universal'
        
        except Exception as e:
            logger.warning(f"URL ë¶„ì„ ì˜¤ë¥˜: {e}")
            return 'universal'
    
    def _detect_by_keyword(self, input_text: str) -> str:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ì‚¬ì´íŠ¸ ê°ì§€"""
        input_lower = input_text.lower()
        
        # 1. ê¸°ë³¸ í‚¤ì›Œë“œ íŒ¨í„´
        for site_type, patterns in self.fallback_patterns.items():
            if any(keyword in input_lower for keyword in patterns['keywords']):
                logger.debug(f"ğŸ¯ í‚¤ì›Œë“œ ë§¤ì¹­: {site_type}")
                return site_type
        
        # 2. ë™ì  í¬ë¡¤ëŸ¬ í‚¤ì›Œë“œ
        for crawler_name, metadata in self.crawler_metadata.items():
            keywords = metadata.get('keywords', [])
            if any(keyword.lower() in input_lower for keyword in keywords):
                logger.debug(f"ğŸ¯ ë™ì  í‚¤ì›Œë“œ ë§¤ì¹­: {crawler_name}")
                return crawler_name
        
        return 'universal'
    
    def _detect_by_crawler_metadata(self, input_text: str) -> str:
        """í¬ë¡¤ëŸ¬ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê°ì§€"""
        input_lower = input_text.lower()
        
        # í¬ë¡¤ëŸ¬ ì´ë¦„ ìì²´ë¡œ ë§¤ì¹­
        for crawler_name in self.available_crawlers:
            if crawler_name.lower() in input_lower:
                logger.debug(f"ğŸ¯ í¬ë¡¤ëŸ¬ëª… ë§¤ì¹­: {crawler_name}")
                return crawler_name
        
        return 'universal'
    
    async def _is_lemmy_instance(self, domain: str) -> bool:
        """Lemmy ì¸ìŠ¤í„´ìŠ¤ì¸ì§€ ë™ì  í™•ì¸"""
        if domain in self.lemmy_instances_cache:
            return True
        
        try:
            async with aiohttp.ClientSession() as session:
                try:
                    async with session.get(
                        f"https://{domain}/api/v3/site", 
                        timeout=aiohttp.ClientTimeout(total=5)
                    ) as response:
                        if response.status == 200:
                            data = await response.json()
                            if 'site_view' in data or 'version' in data:
                                self.lemmy_instances_cache.add(domain)
                                logger.debug(f"ğŸ†• ìƒˆë¡œìš´ Lemmy ì¸ìŠ¤í„´ìŠ¤ ë°œê²¬: {domain}")
                                return True
                except:
                    pass
            
            return False
        
        except Exception:
            return False
    
    def extract_board_identifier(self, url: str, site_type: str) -> str:
        """URLì—ì„œ ê²Œì‹œíŒ ì‹ë³„ì ì¶”ì¶œ"""
        try:
            if site_type == 'reddit':
                import re
                match = re.search(r'/r/([^/]+)', url)
                return match.group(1) if match else url
            
            elif site_type == 'dcinside':
                import re
                match = re.search(r'[?&]id=([^&]+)', url)
                return match.group(1) if match else url
            
            elif site_type == 'lemmy':
                if '/c/' in url:
                    parts = url.split('/c/')
                    if len(parts) > 1:
                        community_part = parts[1].split('/')[0]
                        domain = urlparse(url).netloc
                        return f"{community_part}@{domain}"
                return url
            
            # ğŸ”¥ 4chan ì‹ë³„ì ì¶”ì¶œ ì¶”ê°€
            elif site_type == '4chan':
                import re
                # https://boards.4chan.org/a/ â†’ a
                # https://boards.4chan.org/g/thread/12345 â†’ g
                match = re.search(r'(?:4chan\.org|4channel\.org)/([a-z0-9]+)', url)
                if match:
                    return match.group(1)  # ê²Œì‹œíŒëª… (a, g, v ë“±)
                else:
                    return url  # ê²Œì‹œíŒëª…ë§Œ ì…ë ¥ëœ ê²½ìš°
            
            elif site_type in ['blind', 'bbc']:
                return url
            
            else:
                return url
        
        except Exception as e:
            logger.warning(f"ê²Œì‹œíŒ ì‹ë³„ì ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return url
    
    def is_crawler_functional(self, site_type: str) -> bool:
        """í¬ë¡¤ëŸ¬ê°€ ì‹¤ì œë¡œ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸"""
        return site_type in self.crawl_functions or site_type == 'universal'
    
    def get_crawler_priority(self, site_type: str) -> Tuple[str, str]:
        """
        í¬ë¡¤ëŸ¬ ìš°ì„ ìˆœìœ„ ê²°ì •
        Returns: (priority_type, reason)
        """
        if site_type in self.crawl_functions and site_type != 'universal':
            return ('direct', f'{site_type} ì „ìš© í¬ë¡¤ëŸ¬ ì‚¬ìš© ê°€ëŠ¥')
        elif site_type in self.available_crawlers:
            return ('auto_crawler', f'{site_type} AutoCrawlerë¡œ ì²˜ë¦¬')
        else:
            return ('universal', 'Universal í¬ë¡¤ëŸ¬ë¡œ ì²˜ë¦¬')
    
    def get_supported_sites(self) -> List[str]:
        """ì§€ì›ë˜ëŠ” ì‚¬ì´íŠ¸ ëª©ë¡ ë°˜í™˜"""
        return sorted(list(self.available_crawlers))
    
    def get_functional_crawlers(self) -> List[str]:
        """ì‹¤ì œ ì‘ë™í•˜ëŠ” í¬ë¡¤ëŸ¬ ëª©ë¡ ë°˜í™˜"""
        functional = list(self.crawl_functions.keys())
        if 'universal' not in functional:
            functional.append('universal')
        return sorted(functional)
    
    def get_crawler_info(self, site_type: str) -> Dict[str, Any]:
        """í¬ë¡¤ëŸ¬ ì •ë³´ ë°˜í™˜"""
        base_info = {
            'site_type': site_type,
            'available': site_type in self.available_crawlers,
            'functional': self.is_crawler_functional(site_type)
        }
        
        if site_type in self.crawler_metadata:
            base_info.update(self.crawler_metadata[site_type])
        
        priority, reason = self.get_crawler_priority(site_type)
        base_info.update({
            'priority': priority,
            'priority_reason': reason
        })
        
        return base_info
    
    def get_system_status(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ë°˜í™˜"""
        return {
            'crawlers_dir': str(self.crawlers_dir),
            'crawlers_dir_exists': self.crawlers_dir.exists(),
            'total_discovered': len(self.available_crawlers),
            'functional_count': len(self.get_functional_crawlers()),
            'available_crawlers': list(self.available_crawlers),
            'functional_crawlers': self.get_functional_crawlers(),
            'crawler_metadata': self.crawler_metadata,
            'lemmy_instances_discovered': len(self.lemmy_instances_cache),
            'discovery_method': 'Dynamic file scanning + metadata analysis',
            'fallback_patterns_count': len(self.fallback_patterns),
            '4chan_support': '4chan' in self.available_crawlers
        }

# ë°±ì›Œë“œ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ SiteDetector í´ë˜ìŠ¤
class SiteDetector(DynamicSiteDetector):
    """ê¸°ì¡´ SiteDetectorì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼"""
    
    def __init__(self):
        super().__init__()
        logger.info("ğŸ“¡ SiteDetector (í˜¸í™˜ì„± ëª¨ë“œ) ì´ˆê¸°í™” ì™„ë£Œ")