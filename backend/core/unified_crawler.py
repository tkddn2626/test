# core/unified_crawler.py
"""
ğŸ”¥ í†µí•© í¬ë¡¤ë§ ì‹œìŠ¤í…œ
ëª¨ë“  ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬ë¥¼ í•˜ë‚˜ì˜ ì¸í„°í˜ì´ìŠ¤ë¡œ í†µí•© ê´€ë¦¬
"""

import logging
import asyncio
from typing import Dict, List, Any, Optional, Callable
import inspect

logger = logging.getLogger(__name__)

class UnifiedCrawlerManager:
    """í†µí•© í¬ë¡¤ë§ ë§¤ë‹ˆì €"""
    
    def __init__(self):
        # ğŸ”¥ ì‚¬ì´íŠ¸ë³„ í¬ë¡¤ëŸ¬ í•¨ìˆ˜ì™€ ì§€ì› ë§¤ê°œë³€ìˆ˜ ë§¤í•‘
        self.crawler_registry = {
            'reddit': {
                'module': 'reddit',
                'function': 'fetch_posts',
                'params': {
                    'required': ['subreddit_name'],
                    'optional': [
                        'limit', 'sort', 'time_filter', 'websocket',
                        'min_views', 'min_likes', 'start_date', 'end_date',
                        'enforce_date_limit', 'start_index', 'end_index'
                    ],
                    'mapping': {
                        'board_identifier': 'subreddit_name',
                        'board_input': 'subreddit_name',
                        'board_url': 'subreddit_name'
                    }
                }
            },
            'lemmy': {
                'module': 'lemmy',
                'function': 'crawl_lemmy_board',
                'params': {
                    'required': ['community_input'],
                    'optional': [
                        'limit', 'sort', 'min_views', 'min_likes',
                        'time_filter', 'start_date', 'end_date', 'websocket',
                        'enforce_date_limit', 'start_index', 'end_index'
                    ],
                    'mapping': {
                        'board_identifier': 'community_input',
                        'board_input': 'community_input',
                        'board_url': 'community_input'
                    }
                }
            },
            'dcinside': {
                'module': 'dcinside',
                'function': 'crawl_dcinside_board',
                'params': {
                    'required': ['board_name'],
                    'optional': [
                        'limit', 'sort', 'min_views', 'min_likes',
                        'time_filter', 'start_date', 'end_date', 'websocket',
                        'enforce_date_limit', 'start_index', 'end_index'
                    ],
                    'mapping': {
                        'board_identifier': 'board_name',
                        'board_input': 'board_name',
                        'board_url': 'board_name'
                    }
                }
            },
            'blind': {
                'module': 'blind',
                'function': 'crawl_blind_board',
                'params': {
                    'required': ['board_input'],
                    'optional': [
                        'limit', 'sort', 'min_views', 'min_likes',
                        'time_filter', 'start_date', 'end_date', 'websocket',
                        'enforce_date_limit', 'start_index', 'end_index'
                    ],
                    'mapping': {
                        'board_identifier': 'board_input',
                        'board_name': 'board_input',
                        'board_url': 'board_input'
                    }
                }
            },
            'bbc': {
                'module': 'bbc',
                'function': 'crawl_bbc_board',
                'params': {
                    'required': ['board_url'],
                    'optional': [
                        'limit', 'sort', 'min_views', 'min_likes', 'min_comments',
                        'time_filter', 'start_date', 'end_date', 'websocket',
                        'board_name', 'enforce_date_limit', 'start_index', 'end_index'
                    ],
                    'mapping': {
                        'board_identifier': 'board_url',
                        'board_input': 'board_url',
                        'board_name': 'board_url'
                    }
                }
            },
            'universal': {
                'module': 'universal',
                'function': 'crawl_universal_board',
                'params': {
                    'required': ['board_url'],
                    'optional': [
                        'limit', 'sort', 'min_views', 'min_likes', 'min_comments',
                        'time_filter', 'start_date', 'end_date', 'websocket',
                        'board_name', 'enforce_date_limit', 'start_index', 'end_index'
                    ],
                    'mapping': {
                        'board_identifier': 'board_url',
                        'board_input': 'board_url',
                        'board_name': 'board_url'
                    }
                }
            }
        }
        
        # í¬ë¡¤ëŸ¬ í•¨ìˆ˜ ìºì‹œ
        self._crawler_cache = {}
    
    async def unified_crawl(self, site_type: str, target_input: str, **kwargs) -> List[Dict]:
        """
        í†µí•© í¬ë¡¤ë§ í•¨ìˆ˜
        
        Args:
            site_type: ì‚¬ì´íŠ¸ íƒ€ì… (reddit, lemmy, dcinside, etc.)
            target_input: í¬ë¡¤ë§ ëŒ€ìƒ (ê²Œì‹œíŒëª…, URL ë“±)
            **kwargs: í¬ë¡¤ë§ ì˜µì…˜ë“¤
        
        Returns:
            í¬ë¡¤ë§ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if site_type not in self.crawler_registry:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸: {site_type}")
        
        # 1. í¬ë¡¤ëŸ¬ í•¨ìˆ˜ ë¡œë“œ
        crawler_func = await self._get_crawler_function(site_type)
        
        # 2. ë§¤ê°œë³€ìˆ˜ ë§¤í•‘ ë° í•„í„°ë§
        filtered_params = self._prepare_parameters(site_type, target_input, **kwargs)
        
        # 3. í¬ë¡¤ë§ ì‹¤í–‰
        logger.info(f"ğŸš€ í†µí•© í¬ë¡¤ë§ ì‹œì‘: {site_type} -> {target_input}")
        logger.debug(f"   ë§¤ê°œë³€ìˆ˜: {filtered_params}")
        
        try:
            if asyncio.iscoroutinefunction(crawler_func):
                result = await crawler_func(**filtered_params)
            else:
                result = crawler_func(**filtered_params)
            
            logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(result) if result else 0}ê°œ ê²°ê³¼")
            return result or []
            
        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜ ({site_type}): {e}")
            raise
    
    async def _get_crawler_function(self, site_type: str) -> Callable:
        """í¬ë¡¤ëŸ¬ í•¨ìˆ˜ë¥¼ ë™ì ìœ¼ë¡œ ë¡œë“œ"""
        if site_type in self._crawler_cache:
            return self._crawler_cache[site_type]
        
        config = self.crawler_registry[site_type]
        module_name = config['module']
        function_name = config['function']
        
        try:
            # ë™ì  ëª¨ë“ˆ ì„í¬íŠ¸
            module = __import__(module_name, fromlist=[function_name])
            crawler_func = getattr(module, function_name)
            
            # ìºì‹œì— ì €ì¥
            self._crawler_cache[site_type] = crawler_func
            
            logger.debug(f"í¬ë¡¤ëŸ¬ í•¨ìˆ˜ ë¡œë“œ: {module_name}.{function_name}")
            return crawler_func
            
        except (ImportError, AttributeError) as e:
            logger.error(f"í¬ë¡¤ëŸ¬ í•¨ìˆ˜ ë¡œë“œ ì‹¤íŒ¨: {module_name}.{function_name} - {e}")
            raise ImportError(f"í¬ë¡¤ëŸ¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {site_type}")
    
    def _prepare_parameters(self, site_type: str, target_input: str, **kwargs) -> Dict[str, Any]:
        """ì‚¬ì´íŠ¸ë³„ ì§€ì› ë§¤ê°œë³€ìˆ˜ë§Œ í•„í„°ë§í•˜ì—¬ ì¤€ë¹„"""
        config = self.crawler_registry[site_type]
        required_params = config['params']['required']
        optional_params = config['params']['optional']
        param_mapping = config['params']['mapping']
        
        # 1. ê¸°ë³¸ ë§¤ê°œë³€ìˆ˜ ì„¤ì •
        filtered_params = {}
        
        # 2. í•„ìˆ˜ ë§¤ê°œë³€ìˆ˜ ì„¤ì • (íƒ€ê²Ÿ ì…ë ¥)
        main_param = required_params[0]  # ì²« ë²ˆì§¸ í•„ìˆ˜ ë§¤ê°œë³€ìˆ˜
        filtered_params[main_param] = target_input
        
        # 3. ë§¤ê°œë³€ìˆ˜ ë§¤í•‘ ì ìš©
        for source_key, target_key in param_mapping.items():
            if source_key in kwargs:
                filtered_params[target_key] = kwargs[source_key]
        
        # 4. ì§ì ‘ ë§¤í•‘ë˜ëŠ” ë§¤ê°œë³€ìˆ˜ë“¤
        for param in optional_params:
            if param in kwargs and kwargs[param] is not None:
                filtered_params[param] = kwargs[param]
        
        # 5. ê³µí†µ ë§¤ê°œë³€ìˆ˜ ë§¤í•‘
        common_mappings = {
            'start': 'start_index',
            'end': 'end_index',
            'board': main_param,
            'input': main_param,
            'board_identifier': main_param
        }
        
        for source, target in common_mappings.items():
            if source in kwargs and target in optional_params:
                filtered_params[target] = kwargs[source]
        
        # 6. ì‚¬ì´íŠ¸ë³„ íŠ¹ìˆ˜ ì²˜ë¦¬
        filtered_params = self._apply_site_specific_processing(
            site_type, filtered_params, **kwargs
        )
        
        # 7. None ê°’ ì œê±°
        filtered_params = {k: v for k, v in filtered_params.items() if v is not None}
        
        logger.debug(f"ë§¤ê°œë³€ìˆ˜ í•„í„°ë§ ì™„ë£Œ ({site_type}): {list(filtered_params.keys())}")
        return filtered_params
    
    def _apply_site_specific_processing(self, site_type: str, params: Dict, **kwargs) -> Dict:
        """ì‚¬ì´íŠ¸ë³„ íŠ¹ìˆ˜ ì²˜ë¦¬"""
        
        if site_type == 'reddit':
            # Reddit ì •ë ¬ ë°©ì‹ ë§¤í•‘
            sort_mapping = {
                "popular": "hot", "recommend": "top", "recent": "new",
                "comments": "top"
            }
            if 'sort' in params and params['sort'] in sort_mapping:
                params['sort'] = sort_mapping[params['sort']]
        
        elif site_type == 'lemmy':
            # Lemmy ì»¤ë®¤ë‹ˆí‹° í˜•ì‹ ì²˜ë¦¬
            if 'community_input' in params:
                community = params['community_input']
                if '@' not in community and '.' not in community:
                    params['community_input'] = f"{community}@lemmy.world"
        
        elif site_type == 'bbc':
            # BBCëŠ” board_nameì´ ë¹ˆ ë¬¸ìì—´ë¡œ í•„ìš”í•  ìˆ˜ ìˆìŒ
            if 'board_name' not in params:
                params['board_name'] = ""
        
        return params
    
    def get_supported_parameters(self, site_type: str) -> Dict[str, List[str]]:
        """ì‚¬ì´íŠ¸ë³„ ì§€ì› ë§¤ê°œë³€ìˆ˜ ëª©ë¡ ë°˜í™˜"""
        if site_type not in self.crawler_registry:
            return {}
        
        config = self.crawler_registry[site_type]
        return {
            'required': config['params']['required'],
            'optional': config['params']['optional'],
            'all': config['params']['required'] + config['params']['optional']
        }
    
    def validate_parameters(self, site_type: str, **kwargs) -> tuple[bool, List[str]]:
        """ë§¤ê°œë³€ìˆ˜ ìœ íš¨ì„± ê²€ì‚¬"""
        if site_type not in self.crawler_registry:
            return False, [f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸: {site_type}"]
        
        config = self.crawler_registry[site_type]
        required_params = config['params']['required']
        optional_params = config['params']['optional']
        all_supported = required_params + optional_params
        
        errors = []
        
        # ì§€ì›í•˜ì§€ ì•ŠëŠ” ë§¤ê°œë³€ìˆ˜ í™•ì¸
        unsupported = [k for k in kwargs.keys() if k not in all_supported and k not in ['board_identifier', 'board_input', 'board_url', 'input', 'start', 'end']]
        if unsupported:
            errors.append(f"{site_type}ì—ì„œ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë§¤ê°œë³€ìˆ˜: {unsupported}")
        
        return len(errors) == 0, errors
    
    def get_registry_info(self) -> Dict[str, Any]:
        """ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì •ë³´ ë°˜í™˜"""
        return {
            'total_sites': len(self.crawler_registry),
            'supported_sites': list(self.crawler_registry.keys()),
            'parameters_by_site': {
                site: self.get_supported_parameters(site)
                for site in self.crawler_registry.keys()
            }
        }

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
unified_crawler = UnifiedCrawlerManager()

# í¸ì˜ í•¨ìˆ˜ë“¤
async def crawl_any_site(site_type: str, target: str, **options) -> List[Dict]:
    """ëª¨ë“  ì‚¬ì´íŠ¸ í¬ë¡¤ë§ì„ ìœ„í•œ í¸ì˜ í•¨ìˆ˜"""
    return await unified_crawler.unified_crawl(site_type, target, **options)

def get_site_parameters(site_type: str) -> Dict[str, List[str]]:
    """ì‚¬ì´íŠ¸ë³„ ì§€ì› ë§¤ê°œë³€ìˆ˜ ì¡°íšŒ"""
    return unified_crawler.get_supported_parameters(site_type)

def validate_crawl_request(site_type: str, **params) -> tuple[bool, List[str]]:
    """í¬ë¡¤ë§ ìš”ì²­ ìœ íš¨ì„± ê²€ì‚¬"""
    return unified_crawler.validate_parameters(site_type, **params)