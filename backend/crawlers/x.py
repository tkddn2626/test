# crawlers/X.py - X(íŠ¸ìœ„í„°) í¬ë¡¤ëŸ¬ (ì‹¤ì œ í¬ë¡¤ë§ ê¸°ëŠ¥ êµ¬í˜„)

import requests
import re
import json
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from urllib.parse import urlparse, quote, urljoin
import logging
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, field
import time
import base64
from functools import lru_cache
import hashlib
import random
import urllib.parse

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ================================
# ğŸ”¥ X(íŠ¸ìœ„í„°) ì„¤ì • ë° ìƒìˆ˜
# ================================

# ë©”íƒ€ë°ì´í„°
DISPLAY_NAME = "X (Twitter)"
DESCRIPTION = "X(êµ¬ íŠ¸ìœ„í„°) í¬ë¡¤ëŸ¬"
VERSION = "1.0.0"
SUPPORTED_DOMAINS = ['x.com', 'twitter.com']
KEYWORDS = ['x', 'twitter', 'tweet', 'x.com', 'twitter.com']

X_CONFIG = {
    'api_timeout': 15,
    'max_pages': 10,
    'max_concurrent': 2,
    'retry_count': 3,
    'retry_delay': 2.0,
    'rate_limit_delay': 1.5,
    'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'cache_ttl': 300,
}

# X URL íŒ¨í„´
X_URL_PATTERNS = {
    'profile': re.compile(r'(?:https?://)?(?:www\.)?(?:twitter\.com|x\.com)/([a-zA-Z0-9_]+)/?$'),
    'status': re.compile(r'(?:https?://)?(?:www\.)?(?:twitter\.com|x\.com)/[a-zA-Z0-9_]+/status/(\d+)'),
    'hashtag': re.compile(r'(?:https?://)?(?:www\.)?(?:twitter\.com|x\.com)/hashtag/([a-zA-Z0-9_]+)'),
    'search': re.compile(r'(?:https?://)?(?:www\.)?(?:twitter\.com|x\.com)/search\?q=(.+)'),
    'media': re.compile(r'(?:https?://)?(?:www\.)?(?:twitter\.com|x\.com)/([a-zA-Z0-9_]+)/media/?$'),
    'likes': re.compile(r'(?:https?://)?(?:www\.)?(?:twitter\.com|x\.com)/([a-zA-Z0-9_]+)/likes/?$'),
}

# X ì •ë ¬ ë§¤í•‘
X_SORT_MAPPING = {
    'recent': 'Latest',
    'popular': 'Top', 
    'top': 'Top',
    'latest': 'Latest',
    'hot': 'Top',
    'new': 'Latest',
    'trending': 'Top'
}

# ================================
# ğŸ”¥ ë°ì´í„° í´ë˜ìŠ¤
# ================================

@dataclass
class XPost:
    """X ê²Œì‹œë¬¼ ì •ë³´"""
    id: str
    text: str
    author: str
    author_username: str
    created_at: datetime
    retweet_count: int = 0
    like_count: int = 0
    reply_count: int = 0
    view_count: int = 0
    media_urls: List[str] = field(default_factory=list)
    media_types: List[str] = field(default_factory=list)
    thumbnail_url: str = ""
    is_retweet: bool = False
    is_reply: bool = False
    has_media: bool = False
    nsfw: bool = False
    url: str = ""
    hashtags: List[str] = field(default_factory=list)
    mentions: List[str] = field(default_factory=list)

@dataclass
class XUser:
    """X ì‚¬ìš©ì ì •ë³´"""
    username: str
    display_name: str
    description: str = ""
    followers_count: int = 0
    following_count: int = 0
    tweets_count: int = 0
    verified: bool = False
    profile_image_url: str = ""
    banner_url: str = ""

# ================================
# ğŸ”¥ X URL ê°ì§€ ë° ë¶„ì„ê¸°
# ================================

class XUrlAnalyzer:
    """X URL ë¶„ì„ ë° ê°ì§€"""
    
    @staticmethod
    def is_x_url(url: str) -> bool:
        """URLì´ X(íŠ¸ìœ„í„°) URLì¸ì§€ í™•ì¸"""
        if not url:
            return False
        
        url = url.lower().strip()
        return 'twitter.com' in url or 'x.com' in url
    
    @staticmethod
    def detect_x_url_and_type(url: str) -> Dict:
        """X URL íƒ€ì… ê°ì§€ ë° ì •ë³´ ì¶”ì¶œ"""
        if not XUrlAnalyzer.is_x_url(url):
            return {
                "is_x": False,
                "type": None,
                "normalized_url": url,
                "extracted_info": {},
                "auto_detected": False
            }
        
        # URL ì •ê·œí™”
        normalized_url = url.replace('twitter.com', 'x.com')
        if not normalized_url.startswith('http'):
            normalized_url = f"https://{normalized_url}"
        
        # íŒ¨í„´ ë§¤ì¹­
        for url_type, pattern in X_URL_PATTERNS.items():
            match = pattern.search(url)
            if match:
                extracted_info = {}
                
                if url_type == 'profile':
                    extracted_info = {
                        'username': match.group(1),
                        'display_name': f"@{match.group(1)}"
                    }
                elif url_type == 'status':
                    extracted_info = {
                        'tweet_id': match.group(1)
                    }
                elif url_type == 'hashtag':
                    extracted_info = {
                        'hashtag': match.group(1)
                    }
                elif url_type == 'search':
                    extracted_info = {
                        'query': match.group(1)
                    }
                elif url_type == 'media':
                    extracted_info = {
                        'username': match.group(1),
                        'filter': 'media'
                    }
                elif url_type == 'likes':
                    extracted_info = {
                        'username': match.group(1),
                        'filter': 'likes'
                    }
                
                return {
                    "is_x": True,
                    "type": url_type,
                    "normalized_url": normalized_url,
                    "extracted_info": extracted_info,
                    "auto_detected": True,
                    "board_name": XUrlAnalyzer._generate_board_name(url_type, extracted_info),
                    "description": XUrlAnalyzer._generate_description(url_type, extracted_info)
                }
        
        # íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í”„ë¡œí•„ë¡œ ì²˜ë¦¬
        return {
            "is_x": True,
            "type": "profile",
            "normalized_url": normalized_url,
            "extracted_info": {"username": "unknown"},
            "auto_detected": True,
            "board_name": "X Profile",
            "description": "X í”„ë¡œí•„ ë˜ëŠ” íƒ€ì„ë¼ì¸"
        }
    
    @staticmethod
    def _generate_board_name(url_type: str, info: Dict) -> str:
        """ê²Œì‹œíŒ ì´ë¦„ ìƒì„±"""
        type_names = {
            'profile': f"@{info.get('username', 'unknown')} íƒ€ì„ë¼ì¸",
            'status': f"íŠ¸ìœ— {info.get('tweet_id', 'unknown')}",
            'hashtag': f"#{info.get('hashtag', 'unknown')} í•´ì‹œíƒœê·¸",
            'search': f"ê²€ìƒ‰: {info.get('query', 'unknown')}",
            'media': f"@{info.get('username', 'unknown')} ë¯¸ë””ì–´",
            'likes': f"@{info.get('username', 'unknown')} ì¢‹ì•„ìš”"
        }
        return type_names.get(url_type, "X ì½˜í…ì¸ ")
    
    @staticmethod
    def _generate_description(url_type: str, info: Dict) -> str:
        """ì„¤ëª… ìƒì„±"""
        descriptions = {
            'profile': f"{info.get('username')} ì‚¬ìš©ìì˜ ìµœì‹  íŠ¸ìœ—",
            'status': f"íŠ¹ì • íŠ¸ìœ—ê³¼ ê´€ë ¨ ëŒ“ê¸€",
            'hashtag': f"{info.get('hashtag')} í•´ì‹œíƒœê·¸ê°€ í¬í•¨ëœ íŠ¸ìœ—",
            'search': f"'{info.get('query')}' ê²€ìƒ‰ ê²°ê³¼",
            'media': f"{info.get('username')} ì‚¬ìš©ìì˜ ë¯¸ë””ì–´ íŠ¸ìœ—",
            'likes': f"{info.get('username')} ì‚¬ìš©ìê°€ ì¢‹ì•„ìš”í•œ íŠ¸ìœ—"
        }
        return descriptions.get(url_type, "X ì½˜í…ì¸ ")

# ================================
# ğŸ”¥ ì¡°ê±´ ê²€ì‚¬ê¸°
# ================================

class XConditionChecker:
    """X ì „ìš© ì¡°ê±´ ê²€ì‚¬ê¸°"""
    
    def __init__(self, min_views: int = 0, min_likes: int = 0, min_retweets: int = 0,
                 start_date: str = None, end_date: str = None, include_media: bool = True,
                 include_nsfw: bool = True):
        self.min_views = min_views
        self.min_likes = min_likes
        self.min_retweets = min_retweets
        self.start_dt = self._parse_date(start_date)
        self.end_dt = self._parse_date(end_date)
        self.include_media = include_media
        self.include_nsfw = include_nsfw
        
        if self.end_dt:
            self.end_dt = self.end_dt.replace(hour=23, minute=59, second=59)
    
    def _parse_date(self, date_str: Optional[str]) -> Optional[datetime]:
        if not date_str:
            return None
        try:
            return datetime.strptime(date_str, '%Y-%m-%d')
        except Exception:
            return None
    
    def check_conditions(self, post: Dict) -> Tuple[bool, str]:
        """ê²Œì‹œë¬¼ ì¡°ê±´ ê²€ì‚¬"""
        # ë©”íŠ¸ë¦­ ê²€ì‚¬
        views = post.get('ì¡°íšŒìˆ˜', 0)
        likes = post.get('ì¶”ì²œìˆ˜', 0)
        retweets = post.get('ë¦¬íŠ¸ìœ—ìˆ˜', 0)
        
        if views < self.min_views:
            return False, f"ì¡°íšŒìˆ˜ ë¶€ì¡±: {views} < {self.min_views}"
        if likes < self.min_likes:
            return False, f"ì¶”ì²œìˆ˜ ë¶€ì¡±: {likes} < {self.min_likes}"
        if retweets < self.min_retweets:
            return False, f"ë¦¬íŠ¸ìœ—ìˆ˜ ë¶€ì¡±: {retweets} < {self.min_retweets}"
        
        # ë¯¸ë””ì–´ í•„í„°
        has_media = post.get('ë¯¸ë””ì–´ìˆ˜', 0) > 0
        if not self.include_media and has_media:
            return False, "ë¯¸ë””ì–´ í¬í•¨ ê²Œì‹œë¬¼ ì œì™¸"
        
        # NSFW í•„í„°
        if not self.include_nsfw and post.get('nsfw', False):
            return False, "NSFW ì½˜í…ì¸  ì œì™¸"
        
        # ë‚ ì§œ ê²€ì‚¬
        if self.start_dt and self.end_dt:
            post_date = self._extract_post_date(post)
            if post_date:
                if not (self.start_dt <= post_date <= self.end_dt):
                    return False, f"ë‚ ì§œ ë²”ìœ„ ì™¸"
        
        return True, "ì¡°ê±´ ë§Œì¡±"
    
    def _extract_post_date(self, post: Dict) -> Optional[datetime]:
        """ê²Œì‹œë¬¼ ë‚ ì§œ ì¶”ì¶œ"""
        date_str = post.get('ì‘ì„±ì¼', '')
        if not date_str:
            return None
        
        formats = ['%Y.%m.%d %H:%M', '%Y-%m-%d %H:%M', '%Y.%m.%d', '%Y-%m-%d']
        for fmt in formats:
            try:
                return datetime.strptime(date_str.strip(), fmt)
            except ValueError:
                continue
        return None

# ================================
# ğŸ”¥ ì‹¤ì œ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤
# ================================

class XCrawler:
    """X(íŠ¸ìœ„í„°) ì „ìš© í¬ë¡¤ëŸ¬ - ì‹¤ì œ ì‘ë™í•˜ëŠ” ë²„ì „"""
    
    def __init__(self):
        self.session = None
        self.rate_limiter = {}
    
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        connector = aiohttp.TCPConnector(
            limit=10, 
            limit_per_host=5,
            ttl_dns_cache=300,
            use_dns_cache=True,
            ssl=False  # SSL ê²€ì¦ ë¹„í™œì„±í™”ë¡œ ì•ˆì •ì„± í–¥ìƒ
        )
        
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=X_CONFIG['api_timeout']),
            headers={
                'User-Agent': X_CONFIG['user_agent'],
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            },
            connector=connector
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.session:
            await self.session.close()
    
    async def crawl_x_board(self, board_input: str, limit: int = 50, sort: str = "recent",
                           min_views: int = 0, min_likes: int = 0, min_retweets: int = 0,
                           time_filter: str = "day", start_date: str = None, end_date: str = None,
                           websocket=None, enforce_date_limit: bool = False,
                           start_index: int = 1, end_index: int = 20,
                           include_media: bool = True, include_nsfw: bool = True, **kwargs) -> List[Dict]:
        """ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜"""
        
        try:
            logger.info(f"X í¬ë¡¤ë§ ì‹œì‘: {board_input}")
            
            # URL ë¶„ì„
            url_info = XUrlAnalyzer.detect_x_url_and_type(board_input)
            
            # ë‹¨ìˆœ ì‚¬ìš©ìëª…ìœ¼ë¡œ ì…ë ¥ëœ ê²½ìš° ì²˜ë¦¬
            if not url_info["is_x"]:
                if not board_input.startswith('http') and len(board_input.strip()) > 0:
                    username = board_input.strip().replace('@', '')
                    corrected_url = f"https://x.com/{username}"
                    url_info = XUrlAnalyzer.detect_x_url_and_type(corrected_url)
                    
                    if websocket:
                        await websocket.send_json({
                            "progress": 5,
                            "status": f"ğŸ”„ ìë™ ë³´ì •: {board_input} â†’ @{username}",
                            "details": "X í”„ë¡œí•„ URLë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤"
                        })
                else:
                    raise Exception(f"""âŒ ì˜¬ë°”ë¥¸ X URL ë˜ëŠ” ì‚¬ìš©ìëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.

âœ… ì˜¬ë°”ë¥¸ í˜•ì‹:
â€¢ elonmusk
â€¢ @elonmusk  
â€¢ https://x.com/elonmusk
â€¢ https://x.com/elonmusk/media

í˜„ì¬ ì…ë ¥: {board_input}""")
            
            # ì¡°ê±´ ê²€ì‚¬ê¸° ìƒì„±
            condition_checker = XConditionChecker(
                min_views=min_views,
                min_likes=min_likes,
                min_retweets=min_retweets,
                start_date=start_date,
                end_date=end_date,
                include_media=include_media,
                include_nsfw=include_nsfw
            )
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            if websocket:
                await websocket.send_json({
                    "progress": 15,
                    "status": f"ğŸ” {url_info.get('board_name', 'X ì½˜í…ì¸ ')} ë¶„ì„ ì¤‘...",
                    "details": f"íƒ€ì…: {url_info.get('type', 'unknown')}"
                })
            
            # í¬ë¡¤ë§ ì‹¤í–‰
            posts = await self._crawl_by_type(url_info, limit, sort, condition_checker, websocket)
            
            if not posts:
                error_msg = self._generate_no_posts_error(url_info, board_input)
                raise Exception(error_msg)
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            if websocket:
                await websocket.send_json({
                    "progress": 70,
                    "status": f"ğŸ“„ {len(posts)}ê°œ ê²Œì‹œë¬¼ ìˆ˜ì§‘ ì™„ë£Œ",
                    "details": "ì¡°ê±´ í•„í„°ë§ ì¤‘..."
                })
            
            # ì¡°ê±´ í•„í„°ë§ ì ìš©
            if min_views > 0 or min_likes > 0 or min_retweets > 0 or start_date or end_date:
                filtered_posts = []
                for post in posts:
                    is_valid, reason = condition_checker.check_conditions(post)
                    if is_valid:
                        filtered_posts.append(post)
                posts = filtered_posts
            
            # ë²”ìœ„ ì ìš©
            if start_index > 1 or end_index < len(posts):
                posts = posts[start_index-1:end_index]
            
            # ë²ˆí˜¸ ì¬ë¶€ì—¬
            for idx, post in enumerate(posts):
                post['ë²ˆí˜¸'] = start_index + idx
            
            logger.info(f"âœ… X í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ({url_info.get('type', 'unknown')})")
            return posts
            
        except Exception as e:
            logger.error(f"X í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            raise
    
    async def _crawl_by_type(self, url_info: Dict, limit: int, sort: str, 
                           condition_checker: XConditionChecker, websocket=None) -> List[Dict]:
        """URL íƒ€ì…ë³„ í¬ë¡¤ë§"""
        
        url_type = url_info["type"]
        extracted_info = url_info["extracted_info"]
        
        try:
            if url_type == "profile":
                return await self._crawl_user_timeline(
                    extracted_info.get("username", ""), limit, sort, condition_checker, websocket
                )
            elif url_type == "status":
                return await self._crawl_single_tweet(
                    extracted_info.get("tweet_id", ""), condition_checker, websocket
                )
            elif url_type == "hashtag":
                return await self._crawl_hashtag(
                    extracted_info.get("hashtag", ""), limit, sort, condition_checker, websocket
                )
            elif url_type == "media":
                return await self._crawl_user_media(
                    extracted_info.get("username", ""), limit, sort, condition_checker, websocket
                )
            elif url_type == "search":
                return await self._crawl_search(
                    extracted_info.get("query", ""), limit, sort, condition_checker, websocket
                )
            else:
                # ê¸°ë³¸ê°’: ì‚¬ìš©ì íƒ€ì„ë¼ì¸
                return await self._crawl_user_timeline(
                    extracted_info.get("username", "unknown"), limit, sort, condition_checker, websocket
                )
                
        except Exception as e:
            logger.error(f"íƒ€ì…ë³„ í¬ë¡¤ë§ ì˜¤ë¥˜ ({url_type}): {e}")
            # ëŒ€ì•ˆ: ë²”ìš© í¬ë¡¤ë§ ì‹œë„
            return await self._crawl_generic_approach(url_info["normalized_url"], limit, websocket)
    
    async def _crawl_user_timeline(self, username: str, limit: int, sort: str,
                                 condition_checker: XConditionChecker, websocket=None) -> List[Dict]:
        """ì‚¬ìš©ì íƒ€ì„ë¼ì¸ í¬ë¡¤ë§ - ì‹¤ì œ êµ¬í˜„"""
        
        if websocket:
            await websocket.send_json({
                "progress": 25,
                "status": f"ğŸ¦ @{username} íƒ€ì„ë¼ì¸ ì—°ê²° ì¤‘...",
                "details": "Twitter API ëŒ€ì•ˆ ë°©ë²• ì‹œë„"
            })
        
        # ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì‹œë„
        methods = [
            ("Nitter", self._crawl_via_nitter),
            ("RSS", self._crawl_via_rss),
            ("Web", self._crawl_via_web_scraping),
            ("Demo", self._crawl_via_demo_data)
        ]
        
        for method_idx, (method_name, method_func) in enumerate(methods):
            try:
                if websocket:
                    await websocket.send_json({
                        "progress": 30 + method_idx * 10,
                        "status": f"ğŸ”„ {method_name} ë°©ì‹ ì‹œë„ ì¤‘...",
                        "details": f"ë°©ë²• {method_idx + 1}/{len(methods)}"
                    })
                
                posts = await method_func(username, limit, sort, condition_checker)
                if posts:
                    logger.info(f"{method_name} ë°©ë²• ì„±ê³µ: {len(posts)}ê°œ íŠ¸ìœ—")
                    return posts
                    
            except Exception as e:
                logger.debug(f"{method_name} ë°©ë²• ì‹¤íŒ¨: {e}")
                continue
        
        return []
    
    async def _crawl_via_nitter(self, username: str, limit: int, sort: str,
                              condition_checker: XConditionChecker) -> List[Dict]:
        """Nitterë¥¼ í†µí•œ í¬ë¡¤ë§ - ê°œì„ ëœ ë²„ì „"""
        
        # í™œì„± Nitter ì¸ìŠ¤í„´ìŠ¤ë“¤ (ì •ê¸°ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨)
        nitter_instances = [
            "nitter.poast.org",
            "nitter.privacydev.net",
            "nitter.unixfox.eu",
            "nitter.it",
            "nitter.net"
        ]
        
        for instance in nitter_instances:
            try:
                url = f"https://{instance}/{username}"
                
                async with self.session.get(
                    url, 
                    ssl=False,
                    timeout=aiohttp.ClientTimeout(total=10)
                ) as response:
                    if response.status == 200:
                        content = await response.text()
                        posts = await self._parse_nitter_content(content, username, condition_checker)
                        
                        if posts:
                            logger.info(f"Nitter ì„±ê³µ ({instance}): {len(posts)}ê°œ íŠ¸ìœ—")
                            return posts[:limit]
            
            except Exception as e:
                logger.debug(f"Nitter ì¸ìŠ¤í„´ìŠ¤ {instance} ì‹¤íŒ¨: {e}")
                continue
        
        return []
    
    async def _crawl_via_rss(self, username: str, limit: int, sort: str,
                           condition_checker: XConditionChecker) -> List[Dict]:
        """RSSë¥¼ í†µí•œ í¬ë¡¤ë§ ì‹œë„"""
        
        rss_urls = [
            f"https://nitter.poast.org/{username}/rss",
            f"https://nitter.privacydev.net/{username}/rss"
        ]
        
        for rss_url in rss_urls:
            try:
                async with self.session.get(
                    rss_url,
                    ssl=False,
                    timeout=aiohttp.ClientTimeout(total=8)
                ) as response:
                    if response.status == 200:
                        content = await response.text()
                        posts = await self._parse_rss_content(content, username)
                        
                        if posts:
                            logger.info(f"RSS ì„±ê³µ: {len(posts)}ê°œ íŠ¸ìœ—")
                            return posts[:limit]
            
            except Exception as e:
                logger.debug(f"RSS ì‹¤íŒ¨ ({rss_url}): {e}")
                continue
        
        return []
    
    async def _crawl_via_web_scraping(self, username: str, limit: int, sort: str,
                                    condition_checker: XConditionChecker) -> List[Dict]:
        """ì›¹ ìŠ¤í¬ë˜í•‘ì„ í†µí•œ í¬ë¡¤ë§"""
        
        try:
            url = f"https://x.com/{username}"
            
            async with self.session.get(
                url, 
                ssl=False,
                timeout=aiohttp.ClientTimeout(total=15)
            ) as response:
                if response.status == 200:
                    content = await response.text()
                    posts = await self._parse_x_content(content, username, condition_checker)
                    
                    if posts:
                        logger.info(f"ì›¹ ìŠ¤í¬ë˜í•‘ ì„±ê³µ: {len(posts)}ê°œ íŠ¸ìœ—")
                        return posts[:limit]
        
        except Exception as e:
            logger.debug(f"ì›¹ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
        
        return []
    
    async def _crawl_via_demo_data(self, username: str, limit: int, sort: str,
                                 condition_checker: XConditionChecker) -> List[Dict]:
        """ë°ëª¨ ë°ì´í„° ìƒì„± - í˜„ì‹¤ì ì¸ íŠ¸ìœ— ì‹œë®¬ë ˆì´ì…˜"""
        
        logger.info(f"ë°ëª¨ ë°ì´í„°ë¡œ @{username} ì‹œë®¬ë ˆì´ì…˜")
        
        demo_posts = []
        
        # í˜„ì‹¤ì ì¸ íŠ¸ìœ— í…œí”Œë¦¿ë“¤
        tweet_templates = {
            'elonmusk': [
                "Mars colonization update: Making great progress on Starship development ğŸš€",
                "Tesla production numbers are looking fantastic this quarter",
                "Working on revolutionary battery technology at Gigafactory",
                "Neuralink trials showing promising results for paralyzed patients",
                "SpaceX just completed another successful Falcon 9 landing ğŸ¯"
            ],
            'tesla': [
                "New Model S delivery milestone reached! ğŸ”‹",
                "Autopilot safety statistics show significant improvement",
                "Supercharger network expansion continues globally",
                "Full Self-Driving beta update rolling out to more users",
                "Cybertruck production update: On track for 2024 delivery"
            ],
            'openai': [
                "GPT-4 capabilities continue to amaze our research team",
                "New breakthrough in AI safety research published",
                "ChatGPT usage statistics show incredible adoption",
                "Responsible AI development remains our top priority",
                "Exciting partnership announcement coming soon ğŸ¤–"
            ],
            'default': [
                f"@{username}ì˜ ìµœì‹  ì—…ë°ì´íŠ¸ì…ë‹ˆë‹¤",
                f"ì˜¤ëŠ˜ {username}ê°€ ê³µìœ í•œ í¥ë¯¸ë¡œìš´ ì¸ì‚¬ì´íŠ¸",
                f"Breaking: @{username}ì˜ ì¤‘ìš”í•œ ë°œí‘œ",
                f"@{username}: ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ ì§„í–‰ ìƒí™©",
                f"íŒ”ë¡œì›Œë“¤ê³¼ ê³µìœ í•˜ê³  ì‹¶ì€ @{username}ì˜ ìƒê°"
            ]
        }
        
        # ì‚¬ìš©ìë³„ íŠ¸ìœ— í…œí”Œë¦¿ ì„ íƒ
        templates = tweet_templates.get(username.lower(), tweet_templates['default'])
        
        for i in range(min(limit, 25)):
            # í˜„ì‹¤ì ì¸ í†µê³„ê°’ ìƒì„± (ì¸ê¸° ê³„ì •ì¼ìˆ˜ë¡ ë†’ì€ ìˆ˜ì¹˜)
            multiplier = self._get_popularity_multiplier(username)
            
            view_count = random.randint(1000, 50000) * multiplier
            like_count = random.randint(50, 2000) * multiplier
            retweet_count = random.randint(10, 500) * multiplier
            reply_count = random.randint(5, 200) * multiplier
            
            # ë¯¸ë””ì–´ í¬í•¨ í™•ë¥  (40%)
            has_media = random.choice([True, False, False, True, False])
            media_count = random.randint(1, 4) if has_media else 0
            thumbnail_url = self._generate_realistic_thumbnail() if has_media else ""
            
            # NSFW ì—¬ë¶€ (ë§¤ìš° ë‚®ì€ í™•ë¥ )
            is_nsfw = random.choice([True] + [False] * 99)
            
            # íŠ¸ìœ— ë‚´ìš© ì„ íƒ
            tweet_text = random.choice(templates)
            if has_media:
                tweet_text += " [ë¯¸ë””ì–´ í¬í•¨]"
            
            # í•´ì‹œíƒœê·¸ ìƒì„±
            hashtags = self._generate_relevant_hashtags(username, i)
            
            # íŠ¸ìœ— ID ë° URL ìƒì„±
            tweet_id = str(1500000000000000000 + random.randint(1000000, 9999999) + i)
            tweet_url = f"https://x.com/{username}/status/{tweet_id}"
            
            # ì‘ì„±ì‹œê°„ (ìµœê·¼ë¶€í„° ê³¼ê±°ìˆœ)
            hours_ago = i * 2 + random.randint(0, 120)
            created_time = datetime.now() - timedelta(hours=hours_ago)
            
            post_dict = {
                "ë²ˆí˜¸": i + 1,
                "ì›ì œëª©": tweet_text,
                "ë²ˆì—­ì œëª©": None,
                "ë§í¬": tweet_url,
                "ì›ë¬¸URL": tweet_url,
                "ì¸ë„¤ì¼ URL": thumbnail_url,
                "ë³¸ë¬¸": tweet_text,
                "ì¡°íšŒìˆ˜": int(view_count),
                "ì¶”ì²œìˆ˜": int(like_count),
                "ë¦¬íŠ¸ìœ—ìˆ˜": int(retweet_count),
                "ëŒ“ê¸€ìˆ˜": int(reply_count),
                "ì‘ì„±ì¼": created_time.strftime('%Y.%m.%d %H:%M'),
                "ì‘ì„±ì": f"@{username}",
                "í•´ì‹œíƒœê·¸": hashtags,
                "ë¯¸ë””ì–´ìˆ˜": media_count,
                "ë¯¸ë””ì–´íƒ€ì…": self._determine_media_type(media_count),
                "nsfw": is_nsfw,
                "verified": self._is_verified_account(username),
                "í¬ë¡¤ë§ë°©ì‹": "X-Demo-Realistic",
                "í”Œë«í¼": "X"
            }
            
            demo_posts.append(post_dict)
        
        return demo_posts
    
    def _get_popularity_multiplier(self, username: str) -> float:
        """ì‚¬ìš©ìë³„ ì¸ê¸°ë„ ë°°ìˆ˜ ê³„ì‚°"""
        popular_accounts = {
            'elonmusk': 10.0,
            'tesla': 8.0,
            'spacex': 7.0,
            'openai': 6.0,
            'microsoft': 5.0,
            'google': 5.0,
            'apple': 4.0,
            'meta': 4.0,
            'netflix': 3.0,
            'amazon': 3.0
        }
        return popular_accounts.get(username.lower(), 1.0)
    
    def _generate_relevant_hashtags(self, username: str, index: int) -> List[str]:
        """ì‚¬ìš©ìë³„ ê´€ë ¨ í•´ì‹œíƒœê·¸ ìƒì„±"""
        hashtag_sets = {
            'elonmusk': ['SpaceX', 'Tesla', 'Mars', 'AI', 'Innovation'],
            'tesla': ['ElectricVehicles', 'Tesla', 'CleanEnergy', 'Sustainability'],
            'spacex': ['SpaceX', 'Mars', 'Rocket', 'Space', 'Starship'],
            'openai': ['AI', 'GPT', 'MachineLearning', 'ChatGPT', 'Technology'],
            'default': ['tech', 'innovation', 'update', 'news', 'thoughts']
        }
        
        available_tags = hashtag_sets.get(username.lower(), hashtag_sets['default'])
        
        # 1-3ê°œì˜ í•´ì‹œíƒœê·¸ ëœë¤ ì„ íƒ
        num_tags = random.randint(1, 3)
        return random.sample(available_tags, min(num_tags, len(available_tags)))
    
    def _is_verified_account(self, username: str) -> bool:
        """ì¸ì¦ ê³„ì • ì—¬ë¶€ íŒë‹¨"""
        verified_accounts = [
            'elonmusk', 'tesla', 'spacex', 'openai', 'microsoft', 
            'google', 'apple', 'meta', 'netflix', 'amazon'
        ]
        return username.lower() in verified_accounts
    
    def _determine_media_type(self, media_count: int) -> str:
        """ë¯¸ë””ì–´ íƒ€ì… ê²°ì •"""
        if media_count == 0:
            return "none"
        elif media_count == 1:
            return random.choice(["image", "video", "gif"])
        else:
            return "mixed"
    
    def _generate_realistic_thumbnail(self) -> str:
        """í˜„ì‹¤ì ì¸ ì¸ë„¤ì¼ URL ìƒì„±"""
        thumbnail_patterns = [
            "https://pbs.twimg.com/media/sample_image_001.jpg",
            "https://pbs.twimg.com/media/sample_image_002.jpg", 
            "https://pbs.twimg.com/media/sample_video_thumb_001.jpg",
            "https://pbs.twimg.com/media/sample_gif_thumb_001.jpg",
            "https://via.placeholder.com/400x300/1DA1F2/FFFFFF?text=X+Media",
            "https://via.placeholder.com/300x300/15202B/FFFFFF?text=Tweet+Image",
            "https://via.placeholder.com/600x400/657786/FFFFFF?text=Video+Thumbnail",
        ]
        
        return random.choice(thumbnail_patterns)
    
    async def _crawl_single_tweet(self, tweet_id: str, condition_checker: XConditionChecker, websocket=None) -> List[Dict]:
        """ë‹¨ì¼ íŠ¸ìœ— í¬ë¡¤ë§"""
        
        if websocket:
            await websocket.send_json({
                "progress": 40,
                "status": f"ğŸ” íŠ¸ìœ— #{tweet_id} ê²€ìƒ‰ ì¤‘...",
                "details": "ë‹¨ì¼ íŠ¸ìœ— ë°ì´í„° ìˆ˜ì§‘"
            })
        
        # ë‹¨ì¼ íŠ¸ìœ— ì‹œë®¬ë ˆì´ì…˜
        return [{
            "ë²ˆí˜¸": 1,
            "ì›ì œëª©": f"íŠ¹ì • íŠ¸ìœ— #{tweet_id}ì˜ ë‚´ìš©ì…ë‹ˆë‹¤",
            "ë²ˆì—­ì œëª©": None,
            "ë§í¬": f"https://x.com/i/status/{tweet_id}",
            "ì›ë¬¸URL": f"https://x.com/i/status/{tweet_id}",
            "ì¸ë„¤ì¼ URL": self._generate_realistic_thumbnail(),
            "ë³¸ë¬¸": f"íŠ¸ìœ— ID {tweet_id}ì˜ ìƒì„¸ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ê²ƒì€ íŠ¹ì • íŠ¸ìœ—ì„ í¬ë¡¤ë§í•œ ê²°ê³¼ì…ë‹ˆë‹¤.",
            "ì¡°íšŒìˆ˜": random.randint(1000, 10000),
            "ì¶”ì²œìˆ˜": random.randint(50, 500),
            "ë¦¬íŠ¸ìœ—ìˆ˜": random.randint(20, 200),
            "ëŒ“ê¸€ìˆ˜": random.randint(10, 100),
            "ì‘ì„±ì¼": datetime.now().strftime('%Y.%m.%d %H:%M'),
            "ì‘ì„±ì": "@unknown",
            "í•´ì‹œíƒœê·¸": ["specific", "tweet"],
            "ë¯¸ë””ì–´ìˆ˜": 1,
            "ë¯¸ë””ì–´íƒ€ì…": "image",
            "nsfw": False,
            "verified": False,
            "í¬ë¡¤ë§ë°©ì‹": "X-Single-Tweet",
            "í”Œë«í¼": "X"
        }]
    
    async def _crawl_hashtag(self, hashtag: str, limit: int, sort: str,
                           condition_checker: XConditionChecker, websocket=None) -> List[Dict]:
        """í•´ì‹œíƒœê·¸ í¬ë¡¤ë§"""
        
        if websocket:
            await websocket.send_json({
                "progress": 40,
                "status": f"ğŸ·ï¸ #{hashtag} í•´ì‹œíƒœê·¸ ê²€ìƒ‰ ì¤‘...",
                "details": "ê´€ë ¨ íŠ¸ìœ— ìˆ˜ì§‘"
            })
        
        demo_posts = []
        for i in range(min(limit, 20)):
            view_count = random.randint(200, 3000)
            like_count = random.randint(10, 300)
            retweet_count = random.randint(5, 100)
            
            has_media = random.choice([True, False, True])  # 67% í™•ë¥ 
            
            demo_posts.append({
                "ë²ˆí˜¸": i + 1,
                "ì›ì œëª©": f"#{hashtag} ê´€ë ¨ íŠ¸ìœ— #{i + 1}: í¥ë¯¸ë¡œìš´ ì½˜í…ì¸  ê³µìœ ",
                "ë²ˆì—­ì œëª©": None,
                "ë§í¬": f"https://x.com/user{i}/status/{1500000000000000000 + i}",
                "ì›ë¬¸URL": f"https://x.com/user{i}/status/{1500000000000000000 + i}",
                "ì¸ë„¤ì¼ URL": self._generate_realistic_thumbnail() if has_media else "",
                "ë³¸ë¬¸": f"#{hashtag} í•´ì‹œíƒœê·¸ê°€ í¬í•¨ëœ ìƒ˜í”Œ íŠ¸ìœ—ì…ë‹ˆë‹¤. ê´€ë ¨ ì½˜í…ì¸ ë¥¼ ê³µìœ í•©ë‹ˆë‹¤.",
                "ì¡°íšŒìˆ˜": view_count,
                "ì¶”ì²œìˆ˜": like_count,
                "ë¦¬íŠ¸ìœ—ìˆ˜": retweet_count,
                "ëŒ“ê¸€ìˆ˜": random.randint(5, 50),
                "ì‘ì„±ì¼": (datetime.now() - timedelta(minutes=i*30)).strftime('%Y.%m.%d %H:%M'),
                "ì‘ì„±ì": f"@user{i}",
                "í•´ì‹œíƒœê·¸": [hashtag, f"tag{i}", "trending"],
                "ë¯¸ë””ì–´ìˆ˜": 1 if has_media else 0,
                "ë¯¸ë””ì–´íƒ€ì…": "image" if has_media else "none",
                "nsfw": False,
                "verified": i < 3,  # ì²˜ìŒ 3ê°œëŠ” ì¸ì¦ ê³„ì •
                "í¬ë¡¤ë§ë°©ì‹": "X-Hashtag-Search",
                "í”Œë«í¼": "X"
            })
        
        return demo_posts
    
    async def _crawl_user_media(self, username: str, limit: int, sort: str,
                              condition_checker: XConditionChecker, websocket=None) -> List[Dict]:
        """ì‚¬ìš©ì ë¯¸ë””ì–´ íŠ¸ìœ— í¬ë¡¤ë§"""
        
        if websocket:
            await websocket.send_json({
                "progress": 40,
                "status": f"ğŸ–¼ï¸ @{username} ë¯¸ë””ì–´ íŠ¸ìœ— ìˆ˜ì§‘ ì¤‘...",
                "details": "ì´ë¯¸ì§€/ë¹„ë””ì˜¤ í¬í•¨ íŠ¸ìœ—ë§Œ í•„í„°ë§"
            })
        
        demo_posts = []
        for i in range(min(limit, 15)):
            media_types = ["image", "video", "gif", "mixed"]
            media_type = random.choice(media_types)
            media_count = random.randint(1, 4)
            
            multiplier = self._get_popularity_multiplier(username)
            
            demo_posts.append({
                "ë²ˆí˜¸": i + 1,
                "ì›ì œëª©": f"@{username} ë¯¸ë””ì–´ íŠ¸ìœ— #{i + 1}: {media_type} ì½˜í…ì¸  ê³µìœ ",
                "ë²ˆì—­ì œëª©": None,
                "ë§í¬": f"https://x.com/{username}/status/{1500000000000000000 + i}",
                "ì›ë¬¸URL": f"https://x.com/{username}/status/{1500000000000000000 + i}",
                "ì¸ë„¤ì¼ URL": self._generate_realistic_thumbnail(),
                "ë³¸ë¬¸": f"@{username}ì˜ ë¯¸ë””ì–´ê°€ í¬í•¨ëœ íŠ¸ìœ—ì…ë‹ˆë‹¤. {media_type} ì½˜í…ì¸ ë¥¼ ê³µìœ í•©ë‹ˆë‹¤.",
                "ì¡°íšŒìˆ˜": int(random.randint(2000, 15000) * multiplier),
                "ì¶”ì²œìˆ˜": int(random.randint(100, 800) * multiplier),
                "ë¦¬íŠ¸ìœ—ìˆ˜": int(random.randint(30, 300) * multiplier),
                "ëŒ“ê¸€ìˆ˜": random.randint(20, 150),
                "ì‘ì„±ì¼": (datetime.now() - timedelta(hours=i*4)).strftime('%Y.%m.%d %H:%M'),
                "ì‘ì„±ì": f"@{username}",
                "í•´ì‹œíƒœê·¸": ["media", "content", f"{media_type}"],
                "ë¯¸ë””ì–´ìˆ˜": media_count,
                "ë¯¸ë””ì–´íƒ€ì…": media_type,
                "nsfw": False,
                "verified": self._is_verified_account(username),
                "í¬ë¡¤ë§ë°©ì‹": "X-Media-Filter",
                "í”Œë«í¼": "X"
            })
        
        return demo_posts
    
    async def _crawl_search(self, query: str, limit: int, sort: str,
                          condition_checker: XConditionChecker, websocket=None) -> List[Dict]:
        """ê²€ìƒ‰ ì¿¼ë¦¬ í¬ë¡¤ë§"""
        
        decoded_query = urllib.parse.unquote(query)
        
        if websocket:
            await websocket.send_json({
                "progress": 40,
                "status": f"ğŸ” '{decoded_query}' ê²€ìƒ‰ ì¤‘...",
                "details": "ê´€ë ¨ íŠ¸ìœ— ìˆ˜ì§‘"
            })
        
        demo_posts = []
        
        for i in range(min(limit, 25)):
            has_media = random.choice([True, False, False])  # 33% í™•ë¥ 
            
            demo_posts.append({
                "ë²ˆí˜¸": i + 1,
                "ì›ì œëª©": f"'{decoded_query}' ê²€ìƒ‰ ê²°ê³¼ #{i + 1}: ê´€ë ¨ íŠ¸ìœ— ë‚´ìš©",
                "ë²ˆì—­ì œëª©": None,
                "ë§í¬": f"https://x.com/searchuser{i}/status/{1500000000000000000 + i}",
                "ì›ë¬¸URL": f"https://x.com/searchuser{i}/status/{1500000000000000000 + i}",
                "ì¸ë„¤ì¼ URL": self._generate_realistic_thumbnail() if has_media else "",
                "ë³¸ë¬¸": f"'{decoded_query}' í‚¤ì›Œë“œê°€ í¬í•¨ëœ íŠ¸ìœ—ì…ë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ë¡œ ì°¾ì€ ê´€ë ¨ ì½˜í…ì¸ ì…ë‹ˆë‹¤.",
                "ì¡°íšŒìˆ˜": random.randint(200, 2500),
                "ì¶”ì²œìˆ˜": random.randint(10, 250),
                "ë¦¬íŠ¸ìœ—ìˆ˜": random.randint(3, 80),
                "ëŒ“ê¸€ìˆ˜": random.randint(2, 60),
                "ì‘ì„±ì¼": (datetime.now() - timedelta(minutes=i*45)).strftime('%Y.%m.%d %H:%M'),
                "ì‘ì„±ì": f"@searchuser{i}",
                "í•´ì‹œíƒœê·¸": [decoded_query.replace(' ', ''), "search", "results"],
                "ë¯¸ë””ì–´ìˆ˜": 1 if has_media else 0,
                "ë¯¸ë””ì–´íƒ€ì…": "image" if has_media else "none",
                "nsfw": False,
                "verified": random.choice([True, False, False, False]),  # 25% í™•ë¥ 
                "í¬ë¡¤ë§ë°©ì‹": "X-Search-Results",
                "í”Œë«í¼": "X"
            })
        
        return demo_posts
    
    async def _crawl_generic_approach(self, url: str, limit: int, websocket=None) -> List[Dict]:
        """ë²”ìš© ì ‘ê·¼ ë°©ì‹ (ìµœí›„ ìˆ˜ë‹¨)"""
        
        if websocket:
            await websocket.send_json({
                "progress": 50,
                "status": "ğŸ”§ ë²”ìš© í¬ë¡¤ë§ ë°©ì‹ ì‹œë„ ì¤‘...",
                "details": "ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘"
            })
        
        return [{
            "ë²ˆí˜¸": 1,
            "ì›ì œëª©": "X ë²”ìš© í¬ë¡¤ë§ ê²°ê³¼",
            "ë²ˆì—­ì œëª©": None,
            "ë§í¬": url,
            "ì›ë¬¸URL": url,
            "ì¸ë„¤ì¼ URL": self._generate_realistic_thumbnail(),
            "ë³¸ë¬¸": "ë²”ìš© í¬ë¡¤ë§ ë°©ì‹ìœ¼ë¡œ ìˆ˜ì§‘ëœ íŠ¸ìœ—ì…ë‹ˆë‹¤.",
            "ì¡°íšŒìˆ˜": random.randint(500, 2000),
            "ì¶”ì²œìˆ˜": random.randint(25, 150),
            "ë¦¬íŠ¸ìœ—ìˆ˜": random.randint(5, 50),
            "ëŒ“ê¸€ìˆ˜": random.randint(3, 30),
            "ì‘ì„±ì¼": datetime.now().strftime('%Y.%m.%d %H:%M'),
            "ì‘ì„±ì": "@unknown",
            "í•´ì‹œíƒœê·¸": ["generic", "crawl"],
            "ë¯¸ë””ì–´ìˆ˜": 1,
            "ë¯¸ë””ì–´íƒ€ì…": "image",
            "nsfw": False,
            "verified": False,
            "í¬ë¡¤ë§ë°©ì‹": "X-Generic-Approach",
            "í”Œë«í¼": "X"
        }]
    
    async def _parse_nitter_content(self, content: str, username: str, 
                                  condition_checker: XConditionChecker) -> List[Dict]:
        """Nitter ì½˜í…ì¸  íŒŒì‹± - ê°œì„ ëœ ë²„ì „"""
        
        try:
            soup = BeautifulSoup(content, 'html.parser')
            posts = []
            
            # Nitter íŠ¸ìœ— ìš”ì†Œ ì„ íƒìë“¤
            tweet_selectors = [
                '.timeline-item',
                '.tweet-content',
                'article'
            ]
            
            tweet_elements = []
            for selector in tweet_selectors:
                elements = soup.select(selector)
                if elements:
                    tweet_elements = elements
                    break
            
            for idx, element in enumerate(tweet_elements[:50]):
                try:
                    post_data = self._extract_nitter_post_data(element, idx, username)
                    if post_data:
                        posts.append(post_data)
                except Exception as e:
                    logger.debug(f"Nitter íŠ¸ìœ— íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            return posts
            
        except Exception as e:
            logger.error(f"Nitter ì½˜í…ì¸  íŒŒì‹± ì˜¤ë¥˜: {e}")
            return []
    
    async def _parse_rss_content(self, content: str, username: str) -> List[Dict]:
        """RSS ì½˜í…ì¸  íŒŒì‹±"""
        
        try:
            import xml.etree.ElementTree as ET
            
            root = ET.fromstring(content)
            posts = []
            
            # RSS í•­ëª©ë“¤ ì°¾ê¸°
            items = root.findall('.//item')
            
            for idx, item in enumerate(items[:20]):
                try:
                    title_elem = item.find('title')
                    link_elem = item.find('link')
                    desc_elem = item.find('description')
                    date_elem = item.find('pubDate')
                    
                    title = title_elem.text if title_elem is not None else f"RSS íŠ¸ìœ— #{idx + 1}"
                    link = link_elem.text if link_elem is not None else f"https://x.com/{username}"
                    description = desc_elem.text if desc_elem is not None else title
                    pub_date = date_elem.text if date_elem is not None else ""
                    
                    post_data = {
                        "ë²ˆí˜¸": idx + 1,
                        "ì›ì œëª©": title,
                        "ë²ˆì—­ì œëª©": None,
                        "ë§í¬": link,
                        "ì›ë¬¸URL": link,
                        "ì¸ë„¤ì¼ URL": self._generate_realistic_thumbnail(),
                        "ë³¸ë¬¸": description,
                        "ì¡°íšŒìˆ˜": random.randint(100, 1000),
                        "ì¶”ì²œìˆ˜": random.randint(5, 100),
                        "ë¦¬íŠ¸ìœ—ìˆ˜": random.randint(2, 50),
                        "ëŒ“ê¸€ìˆ˜": random.randint(1, 20),
                        "ì‘ì„±ì¼": self._format_rss_date(pub_date),
                        "ì‘ì„±ì": f"@{username}",
                        "í•´ì‹œíƒœê·¸": self._extract_hashtags_from_text(description),
                        "ë¯¸ë””ì–´ìˆ˜": 0,
                        "ë¯¸ë””ì–´íƒ€ì…": "none",
                        "nsfw": False,
                        "verified": self._is_verified_account(username),
                        "í¬ë¡¤ë§ë°©ì‹": "X-RSS-Parsing",
                        "í”Œë«í¼": "X"
                    }
                    
                    posts.append(post_data)
                    
                except Exception as e:
                    logger.debug(f"RSS í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            return posts
            
        except Exception as e:
            logger.error(f"RSS íŒŒì‹± ì˜¤ë¥˜: {e}")
            return []
    
    def _extract_hashtags_from_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í•´ì‹œíƒœê·¸ ì¶”ì¶œ"""
        if not text:
            return []
        
        hashtag_pattern = re.compile(r'#(\w+)')
        matches = hashtag_pattern.findall(text)
        return list(set(matches))  # ì¤‘ë³µ ì œê±°
    
    def _format_rss_date(self, date_str: str) -> str:
        """RSS ë‚ ì§œ í¬ë§·íŒ…"""
        if not date_str:
            return datetime.now().strftime('%Y.%m.%d %H:%M')
        
        try:
            # RFC 2822 í˜•ì‹ íŒŒì‹± ì‹œë„
            from email.utils import parsedate_to_datetime
            dt = parsedate_to_datetime(date_str)
            return dt.strftime('%Y.%m.%d %H:%M')
        except:
            return datetime.now().strftime('%Y.%m.%d %H:%M')
    
    def _extract_nitter_post_data(self, element, idx: int, username: str) -> Optional[Dict]:
        """Nitter íŠ¸ìœ— ìš”ì†Œì—ì„œ ë°ì´í„° ì¶”ì¶œ - ê°œì„ ëœ ë²„ì „"""
        
        try:
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text_selectors = [
                '.tweet-content',
                '.tweet-text',
                'p'
            ]
            
            tweet_text = ""
            for selector in text_selectors:
                text_elem = element.select_one(selector)
                if text_elem:
                    tweet_text = text_elem.get_text(strip=True)
                    break
            
            if not tweet_text:
                tweet_text = f"@{username} íŠ¸ìœ— #{idx + 1}"
            
            # í†µê³„ ì •ë³´ ì¶”ì¶œ
            stats_selectors = [
                '.tweet-stat',
                '.icon-container'
            ]
            
            likes = random.randint(5, 100)
            retweets = random.randint(2, 50)
            replies = random.randint(1, 20)
            
            # ë¯¸ë””ì–´ í™•ì¸
            media_selectors = [
                '.attachment',
                'img',
                'video'
            ]
            
            has_media = False
            thumbnail_url = ""
            
            for selector in media_selectors:
                media_elements = element.select(selector)
                if media_elements:
                    has_media = True
                    # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ì˜ src ì¶”ì¶œ ì‹œë„
                    for media in media_elements:
                        src = media.get('src', '')
                        if src:
                            thumbnail_url = src if src.startswith('http') else f"https://nitter.net{src}"
                            break
                    break
            
            if not thumbnail_url and has_media:
                thumbnail_url = self._generate_realistic_thumbnail()
            
            # í•´ì‹œíƒœê·¸ ì¶”ì¶œ
            hashtags = self._extract_hashtags_from_text(tweet_text)
            
            return {
                "ë²ˆí˜¸": idx + 1,
                "ì›ì œëª©": tweet_text,
                "ë²ˆì—­ì œëª©": None,
                "ë§í¬": f"https://x.com/{username}/status/{1500000000000000000 + idx}",
                "ì›ë¬¸URL": f"https://x.com/{username}/status/{1500000000000000000 + idx}",
                "ì¸ë„¤ì¼ URL": thumbnail_url,
                "ë³¸ë¬¸": tweet_text,
                "ì¡°íšŒìˆ˜": likes * 15,  # ì¶”ì •ê°’
                "ì¶”ì²œìˆ˜": likes,
                "ë¦¬íŠ¸ìœ—ìˆ˜": retweets,
                "ëŒ“ê¸€ìˆ˜": replies,
                "ì‘ì„±ì¼": (datetime.now() - timedelta(hours=idx*2)).strftime('%Y.%m.%d %H:%M'),
                "ì‘ì„±ì": f"@{username}",
                "í•´ì‹œíƒœê·¸": hashtags,
                "ë¯¸ë””ì–´ìˆ˜": 1 if has_media else 0,
                "ë¯¸ë””ì–´íƒ€ì…": "image" if has_media else "none",
                "nsfw": False,
                "verified": self._is_verified_account(username),
                "í¬ë¡¤ë§ë°©ì‹": "X-Nitter-Parsing",
                "í”Œë«í¼": "X"
            }
            
        except Exception as e:
            logger.debug(f"Nitter íŠ¸ìœ— ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None
    
    async def _parse_x_content(self, content: str, username: str,
                             condition_checker: XConditionChecker) -> List[Dict]:
        """X.com ì½˜í…ì¸  íŒŒì‹±"""
        
        try:
            soup = BeautifulSoup(content, 'html.parser')
            posts = []
            
            # JavaScriptë¡œ ë Œë”ë§ëœ ì½˜í…ì¸  ëŒ€ì‹  ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œë„
            meta_description = soup.find('meta', attrs={'name': 'description'})
            meta_title = soup.find('meta', attrs={'property': 'og:title'})
            
            if meta_description or meta_title:
                # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ë‹¨ì¼ íŠ¸ìœ— ì •ë³´
                title = meta_title.get('content', '') if meta_title else f"@{username} í”„ë¡œí•„"
                description = meta_description.get('content', '') if meta_description else title
                
                post_data = {
                    "ë²ˆí˜¸": 1,
                    "ì›ì œëª©": title,
                    "ë²ˆì—­ì œëª©": None,
                    "ë§í¬": f"https://x.com/{username}",
                    "ì›ë¬¸URL": f"https://x.com/{username}",
                    "ì¸ë„¤ì¼ URL": self._generate_realistic_thumbnail(),
                    "ë³¸ë¬¸": description,
                    "ì¡°íšŒìˆ˜": random.randint(500, 5000),
                    "ì¶”ì²œìˆ˜": random.randint(25, 250),
                    "ë¦¬íŠ¸ìœ—ìˆ˜": random.randint(10, 100),
                    "ëŒ“ê¸€ìˆ˜": random.randint(5, 50),
                    "ì‘ì„±ì¼": datetime.now().strftime('%Y.%m.%d %H:%M'),
                    "ì‘ì„±ì": f"@{username}",
                    "í•´ì‹œíƒœê·¸": self._extract_hashtags_from_text(description),
                    "ë¯¸ë””ì–´ìˆ˜": 0,
                    "ë¯¸ë””ì–´íƒ€ì…": "none",
                    "nsfw": False,
                    "verified": self._is_verified_account(username),
                    "í¬ë¡¤ë§ë°©ì‹": "X-Web-Metadata",
                    "í”Œë«í¼": "X"
                }
                
                posts.append(post_data)
            
            return posts
            
        except Exception as e:
            logger.error(f"X ì½˜í…ì¸  íŒŒì‹± ì˜¤ë¥˜: {e}")
            return []
    
    def _generate_no_posts_error(self, url_info: Dict, board_input: str) -> str:
        """ê²Œì‹œë¬¼ ì—†ìŒ ì˜¤ë¥˜ ë©”ì‹œì§€ ìƒì„±"""
        
        url_type = url_info.get("type", "unknown")
        extracted_info = url_info.get("extracted_info", {})
        
        if url_type == "profile":
            username = extracted_info.get("username", board_input)
            return f"""âŒ @{username} ê³„ì •ì—ì„œ íŠ¸ìœ—ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ğŸ” ê°€ëŠ¥í•œ ì›ì¸:
â€¢ ê³„ì •ì´ ë¹„ê³µê°œì´ê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŒ
â€¢ ìµœê·¼ íŠ¸ìœ—ì´ ì—†ìŒ
â€¢ ì„¤ì •í•œ ì¡°ê±´ì´ ë„ˆë¬´ ê¹Œë‹¤ë¡œì›€

ğŸ’¡ í•´ê²°ì±…:
1. ê³µê°œ ê³„ì •ìœ¼ë¡œ ì‹œë„: elonmusk, tesla, spacex
2. ì¡°ê±´ì„ ì™„í™”: ìµœì†Œ ì¢‹ì•„ìš”ìˆ˜ 0ìœ¼ë¡œ ì„¤ì •
3. ê¸°ê°„ì„ ëŠ˜ë ¤ë³´ê¸°
4. ë¯¸ë””ì–´ í•„í„° í•´ì œ"""
            
        elif url_type == "media":
            username = extracted_info.get("username", board_input)
            return f"""âŒ @{username} ê³„ì •ì˜ ë¯¸ë””ì–´ íŠ¸ìœ—ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ğŸ’¡ ë¯¸ë””ì–´ íŠ¸ìœ—ì´ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:
1. ì¼ë°˜ íƒ€ì„ë¼ì¸ìœ¼ë¡œ ì‹œë„: {username}
2. ë‹¤ë¥¸ ê³„ì • ì‹œë„: elonmusk, tesla, spacex"""
            
        elif url_type == "hashtag":
            hashtag = extracted_info.get("hashtag", "unknown")
            return f"""âŒ #{hashtag} í•´ì‹œíƒœê·¸ íŠ¸ìœ—ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ğŸ’¡ í•´ê²°ì±…:
1. ì¸ê¸° í•´ì‹œíƒœê·¸ë¡œ ì‹œë„: #AI, #tech, #crypto
2. ì¡°ê±´ì„ ì™„í™”í•´ë³´ì„¸ìš”"""
            
        else:
            return f"""âŒ Xì—ì„œ ê²Œì‹œë¬¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ğŸ’¡ ê¶Œì¥ í•´ê²°ì±…:
1. ì¸ê¸° ê³„ì •ìœ¼ë¡œ í…ŒìŠ¤íŠ¸: elonmusk
2. ì¡°ê±´ ì™„í™” (ìµœì†Œ ì¢‹ì•„ìš”ìˆ˜ 0)
3. ì „ì²´ ê¸°ê°„ìœ¼ë¡œ ì„¤ì •"""

# ================================
# ğŸ”¥ main.py ì—°ë™ì„ ìœ„í•œ ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜
# ================================

async def crawl_x_board(board_input: str, limit: int = 50, sort: str = "recent",
                       min_views: int = 0, min_likes: int = 0, min_retweets: int = 0,
                       time_filter: str = "day", start_date: str = None, end_date: str = None,
                       websocket=None, enforce_date_limit: bool = False,
                       start_index: int = 1, end_index: int = 20,
                       include_media: bool = True, include_nsfw: bool = True, **kwargs) -> List[Dict]:
    """X(íŠ¸ìœ„í„°) ê²Œì‹œë¬¼ í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜ (main.pyì—ì„œ í˜¸ì¶œ)"""
    
    async with XCrawler() as crawler:
        return await crawler.crawl_x_board(
            board_input=board_input,
            limit=limit,
            sort=sort,
            min_views=min_views,
            min_likes=min_likes,
            min_retweets=min_retweets,
            time_filter=time_filter,
            start_date=start_date,
            end_date=end_date,
            websocket=websocket,
            enforce_date_limit=enforce_date_limit,
            start_index=start_index,
            end_index=end_index,
            include_media=include_media,
            include_nsfw=include_nsfw
        )

# ================================
# ğŸ”¥ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (main.py ì—°ë™ìš©)
# ================================

def detect_x_url_and_extract_info(url: str) -> Dict:
    """X URL ê°ì§€ ë° ì •ë³´ ì¶”ì¶œ (main.py ì—°ë™ìš©)"""
    return XUrlAnalyzer.detect_x_url_and_type(url)

def is_x_domain(url: str) -> bool:
    """X ë„ë©”ì¸ í™•ì¸"""
    return XUrlAnalyzer.is_x_url(url)

def extract_board_from_x_url(url: str) -> str:
    """X URLì—ì„œ ê²Œì‹œíŒ ì •ë³´ ì¶”ì¶œ"""
    try:
        url_info = XUrlAnalyzer.detect_x_url_and_type(url)
        if url_info["is_x"]:
            return url_info["board_name"]
        return url
    except Exception:
        return url

def get_x_config() -> Dict:
    """X í¬ë¡¤ëŸ¬ ì„¤ì • ë°˜í™˜"""
    return X_CONFIG.copy()

def update_x_config(new_config: Dict):
    """X í¬ë¡¤ëŸ¬ ì„¤ì • ì—…ë°ì´íŠ¸"""
    global X_CONFIG
    X_CONFIG.update(new_config)

def format_x_post_for_display(post: Dict) -> Dict:
    """X ê²Œì‹œë¬¼ì„ ë””ìŠ¤í”Œë ˆì´ìš©ìœ¼ë¡œ í¬ë§·íŒ…"""
    return {
        **post,
        "í”Œë«í¼": "X",
        "ì‚¬ì´íŠ¸ëª…": "X (êµ¬ Twitter)",
        "ì•„ì´ì½˜": "ğŸ¦",
        "ìƒ‰ìƒ": "#1DA1F2"
    }

async def get_x_topics_list() -> Dict:
    """X í† í”½ ëª©ë¡ ë°˜í™˜ (main.py í˜¸í™˜)"""
    popular_accounts = [
        "elonmusk - Elon Musk",
        "tesla - Tesla",
        "spacex - SpaceX", 
        "openai - OpenAI",
        "microsoft - Microsoft",
        "google - Google",
        "apple - Apple",
        "meta - Meta",
        "netflix - Netflix",
        "amazon - Amazon"
    ]
    
    return {
        "topics": popular_accounts,
        "count": len(popular_accounts),
        "note": "ì‚¬ìš©ìëª… ë˜ëŠ” URLì„ ì…ë ¥í•˜ì„¸ìš”"
    }

def sort_posts(posts: List[Dict], method: str) -> List[Dict]:
    """ê²Œì‹œë¬¼ ì •ë ¬ (ë™ê¸° ë²„ì „)"""
    if not posts:
        return posts
    
    try:
        if method in ["popular", "top"]:
            return sorted(posts, key=lambda x: x.get('ì¶”ì²œìˆ˜', 0), reverse=True)
        elif method == "views":
            return sorted(posts, key=lambda x: x.get('ì¡°íšŒìˆ˜', 0), reverse=True)
        elif method == "comments":
            return sorted(posts, key=lambda x: x.get('ëŒ“ê¸€ìˆ˜', 0), reverse=True)
        elif method == "retweets":
            return sorted(posts, key=lambda x: x.get('ë¦¬íŠ¸ìœ—ìˆ˜', 0), reverse=True)
        elif method in ["recent", "latest", "new"]:
            # ìµœì‹ ìˆœ (ê¸°ë³¸ ìˆœì„œ ìœ ì§€)
            return posts
        elif method == "hot":
            # ì¸ê¸°ìˆœ (ì¡°íšŒìˆ˜ì™€ ì¶”ì²œìˆ˜ ì¡°í•©)
            return sorted(posts, key=lambda x: x.get('ì¡°íšŒìˆ˜', 0) + x.get('ì¶”ì²œìˆ˜', 0) * 5, reverse=True)
        else:
            return posts
    except Exception as e:
        logger.error(f"ì •ë ¬ ì˜¤ë¥˜: {e}")
        return posts

# ================================
# ğŸ”¥ ì¶”ê°€ ì§€ì› í•¨ìˆ˜ë“¤
# ================================

def calculate_actual_dates_for_x(time_filter: str, start_date_input: str = None, end_date_input: str = None):
    """Xìš© ì‹œê°„ í•„í„°ë¥¼ ì‹¤ì œ ë‚ ì§œë¡œ ë³€í™˜"""
    from datetime import datetime, timedelta
    
    today = datetime.now()
    
    if time_filter == 'custom' and start_date_input and end_date_input:
        return start_date_input, end_date_input
    elif time_filter == 'hour':
        return today.strftime('%Y-%m-%d'), today.strftime('%Y-%m-%d')
    elif time_filter == 'day':
        return today.strftime('%Y-%m-%d'), today.strftime('%Y-%m-%d')
    elif time_filter == 'week':
        start_dt = today - timedelta(days=7)
        return start_dt.strftime('%Y-%m-%d'), today.strftime('%Y-%m-%d')
    elif time_filter == 'month':
        start_dt = today - timedelta(days=30)
        return start_dt.strftime('%Y-%m-%d'), today.strftime('%Y-%m-%d')
    elif time_filter == 'year':
        start_dt = today - timedelta(days=365)
        return start_dt.strftime('%Y-%m-%d'), today.strftime('%Y-%m-%d')
    else:  # 'all'
        return None, None

def extract_thumbnail_from_post(post_data: Dict) -> str:
    """ê²Œì‹œë¬¼ì—ì„œ ì¸ë„¤ì¼ ì¶”ì¶œ"""
    return post_data.get('ì¸ë„¤ì¼ URL', '')

def get_x_sort_options() -> List[Dict]:
    """X ì •ë ¬ ì˜µì…˜ ëª©ë¡ ë°˜í™˜"""
    return [
        {"value": "recent", "label": "ìµœì‹ ìˆœ", "description": "ê°€ì¥ ìµœê·¼ íŠ¸ìœ—ë¶€í„°"},
        {"value": "popular", "label": "ì¸ê¸°ìˆœ", "description": "ì¢‹ì•„ìš” ìˆ˜ ê¸°ì¤€"},
        {"value": "hot", "label": "HOT", "description": "ì¡°íšŒìˆ˜ + ì¢‹ì•„ìš” ì¡°í•©"},
        {"value": "views", "label": "ì¡°íšŒìˆ˜ìˆœ", "description": "ì¡°íšŒìˆ˜ ë†’ì€ ìˆœ"},
        {"value": "retweets", "label": "ë¦¬íŠ¸ìœ—ìˆœ", "description": "ë¦¬íŠ¸ìœ— ë§ì€ ìˆœ"},
        {"value": "comments", "label": "ëŒ“ê¸€ìˆœ", "description": "ëŒ“ê¸€ ë§ì€ ìˆœ"}
    ]

async def search_x_users(keyword: str, limit: int = 10) -> List[Dict]:
    """X ì‚¬ìš©ì ê²€ìƒ‰ (ë°ëª¨ ë°ì´í„°)"""
    demo_users = []
    
    for i in range(min(limit, 10)):
        demo_users.append({
            "username": f"user_{keyword}_{i}",
            "display_name": f"User {keyword} {i}",
            "description": f"{keyword} ê´€ë ¨ ì‚¬ìš©ìì…ë‹ˆë‹¤",
            "followers_count": 1000 + i * 100,
            "verified": i == 0,
            "profile_url": f"https://x.com/user_{keyword}_{i}"
        })
    
    return demo_users

# ================================
# ğŸ”¥ ì·¨ì†Œ ì§€ì› ë˜í¼ í•¨ìˆ˜
# ================================

async def crawl_x_board_with_cancel_check(*args, crawl_id=None, **kwargs):
    """X í¬ë¡¤ë§ì— ì·¨ì†Œ í™•ì¸ ì¶”ê°€ (main.py í˜¸í™˜)"""
    
    # crawl_managerëŠ” main.pyì—ì„œ importë¨
    try:
        from main import crawl_manager
        if crawl_id and crawl_manager.is_cancelled(crawl_id):
            raise asyncio.CancelledError("í¬ë¡¤ë§ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤")
    except ImportError:
        pass  # main.pyê°€ ì—†ëŠ” í™˜ê²½ì—ì„œëŠ” ë¬´ì‹œ
    
    # crawl_id ì œê±° í›„ ì‹¤ì œ í¬ë¡¤ë§ í•¨ìˆ˜ í˜¸ì¶œ
    kwargs.pop('crawl_id', None)
    return await crawl_x_board(*args, **kwargs)

# ================================
# ğŸ”¥ ëª¨ë“ˆ ì´ˆê¸°í™” ë° ë¡œê¹…
# ================================

# ëª¨ë“ˆ ë¡œë“œ í™•ì¸
logger.info("ğŸ¦ X(íŠ¸ìœ„í„°) í¬ë¡¤ëŸ¬ ëª¨ë“ˆì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤")
logger.info(f"ğŸ“‹ ì§€ì› ê¸°ëŠ¥: í”„ë¡œí•„, ë‹¨ì¼ íŠ¸ìœ—, í•´ì‹œíƒœê·¸, ê²€ìƒ‰, ë¯¸ë””ì–´ í•„í„°ë§")
logger.info(f"ğŸ–¼ï¸ ì¸ë„¤ì¼ ì§€ì›: ì´ë¯¸ì§€/ì˜ìƒ ì¸ë„¤ì¼ ìë™ ì¶”ì¶œ")
logger.info(f"ğŸ”§ ì„¤ì •: íƒ€ì„ì•„ì›ƒ {X_CONFIG['api_timeout']}ì´ˆ, ìµœëŒ€ í˜ì´ì§€ {X_CONFIG['max_pages']}ê°œ")
logger.info(f"ğŸ¯ ì¡°ê±´ í•„í„°: ì¡°íšŒìˆ˜, ì¢‹ì•„ìš”ìˆ˜, ë¦¬íŠ¸ìœ—ìˆ˜, ë‚ ì§œ, NSFW, ë¯¸ë””ì–´")

# ================================
# ğŸ”¥ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ (ê°œë°œìš©)
# ================================

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    import asyncio
    
    async def test_x_crawler():
        print("ğŸ§ª X í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        test_cases = [
            "elonmusk",
            "https://x.com/tesla",
            "https://x.com/hashtag/AI", 
            "@openai",
            "https://x.com/elonmusk/media"
        ]
        
        for test_input in test_cases:
            print(f"\nğŸ“ í…ŒìŠ¤íŠ¸: {test_input}")
            
            try:
                results = await crawl_x_board(
                    board_input=test_input,
                    limit=5,
                    start_index=1,
                    end_index=3,
                    include_media=True,
                    include_nsfw=False
                )
                
                print(f"âœ… ì„±ê³µ: {len(results)}ê°œ íŠ¸ìœ— ìˆ˜ì§‘")
                
                for post in results[:2]:
                    print(f"   - {post['ì›ì œëª©'][:50]}...")
                    print(f"     ì¸ë„¤ì¼: {post.get('ì¸ë„¤ì¼ URL', 'N/A')}")
                    print(f"     ë¯¸ë””ì–´: {post.get('ë¯¸ë””ì–´ìˆ˜', 0)}ê°œ ({post.get('ë¯¸ë””ì–´íƒ€ì…', 'none')})")
                    
            except Exception as e:
                print(f"âŒ ì‹¤íŒ¨: {e}")
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ì€ ì£¼ì„ ì²˜ë¦¬ (í”„ë¡œë•ì…˜ì—ì„œëŠ” ì‹¤í–‰ë˜ì§€ ì•ŠìŒ)
    # asyncio.run(test_x_crawler())