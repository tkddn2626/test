# core/media_download.py
# ë¯¸ë””ì–´ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• ëª¨ë“ˆ - ì‚¬ì´íŠ¸ íƒ€ì… ë¬´ê´€

import os
import asyncio
import aiohttp
import aiofiles
import zipfile
import tempfile
import shutil
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable
from urllib.parse import urlparse, unquote
from datetime import datetime
import logging
import re

# import
from core.messages import create_localized_error_message, create_message_response

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ==================== ì„¤ì • ë° ìƒìˆ˜ ====================

# ë¯¸ë””ì–´ ë‹¤ìš´ë¡œë“œ ì„¤ì •
MEDIA_CONFIG = {
    'max_file_size_mb': 100,           # ê°œë³„ íŒŒì¼ ìµœëŒ€ í¬ê¸° (MB)
    'max_total_size_mb': 900,         # ì „ì²´ ZIP ìµœëŒ€ í¬ê¸° (MB)
    'download_timeout': 30,            # ë‹¤ìš´ë¡œë“œ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
    'max_concurrent_downloads': 5,     # ë™ì‹œ ë‹¤ìš´ë¡œë“œ ìˆ˜
    'retry_attempts': 3,               # ì¬ì‹œë„ íšŸìˆ˜
    'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# ì§€ì›í•˜ëŠ” ë¯¸ë””ì–´ íƒ€ì…
SUPPORTED_IMAGE_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp', '.svg'}
SUPPORTED_VIDEO_EXTENSIONS = {'.mp4', '.webm', '.mov', '.avi', '.mkv', '.flv', '.wmv'}
SUPPORTED_AUDIO_EXTENSIONS = {'.mp3', '.wav', '.ogg', '.m4a', '.flac', '.aac'}

# ì„ì‹œ ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬
TEMP_DOWNLOAD_DIR = Path(tempfile.gettempdir()) / "pickpost_media_downloads"
TEMP_DOWNLOAD_DIR.mkdir(exist_ok=True)

# ==================== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ====================

def get_file_extension_from_url(url: str) -> str:
    """URLì—ì„œ íŒŒì¼ í™•ì¥ì ì¶”ì¶œ"""
    try:
        parsed = urlparse(url)
        path = unquote(parsed.path)
        return Path(path).suffix.lower()
    except Exception:
        return ''

def get_media_type(extension: str) -> str:
    """í™•ì¥ìë¡œ ë¯¸ë””ì–´ íƒ€ì… ê²°ì •"""
    if extension in SUPPORTED_IMAGE_EXTENSIONS:
        return 'image'
    elif extension in SUPPORTED_VIDEO_EXTENSIONS:
        return 'video'
    elif extension in SUPPORTED_AUDIO_EXTENSIONS:
        return 'audio'
    else:
        return 'unknown'

def sanitize_filename(filename: str) -> str:
    """íŒŒì¼ëª… ì•ˆì „í•˜ê²Œ ì •ë¦¬"""
    # ìœ„í—˜í•œ ë¬¸ì ì œê±°
    sanitized = re.sub(r'[<>:"/\\|?*]', '_', filename)
    # ì—°ì†ëœ ë°‘ì¤„ ì œê±°
    sanitized = re.sub(r'_+', '_', sanitized)
    # ì•ë’¤ ë°‘ì¤„/ê³µë°± ì œê±°
    sanitized = sanitized.strip('_. ')
    # ë¹ˆ ë¬¸ìì—´ ë°©ì§€
    return sanitized if sanitized else 'untitled'

def is_valid_media_url(url: str) -> bool:
    """ìœ íš¨í•œ ë¯¸ë””ì–´ URLì¸ì§€ í™•ì¸"""
    if not url or not isinstance(url, str):
        return False
    
    # ê¸°ë³¸ URL í˜•ì‹ í™•ì¸
    if not url.startswith(('http://', 'https://')):
        return False
    
    # í™•ì¥ì í™•ì¸
    extension = get_file_extension_from_url(url)
    if extension:
        all_extensions = SUPPORTED_IMAGE_EXTENSIONS | SUPPORTED_VIDEO_EXTENSIONS | SUPPORTED_AUDIO_EXTENSIONS
        return extension in all_extensions
    
    # í™•ì¥ìê°€ ì—†ì–´ë„ ì•Œë ¤ì§„ ë¯¸ë””ì–´ í˜¸ìŠ¤íŒ… ë„ë©”ì¸ì´ë©´ í—ˆìš©
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        
        # ì•Œë ¤ì§„ ë¯¸ë””ì–´ í˜¸ìŠ¤íŒ… ë„ë©”ì¸ë“¤
        media_domains = {
            'i.imgur.com', 'imgur.com',
            'i.redd.it', 'v.redd.it',
            'i.pinimg.com', 'pinimg.com',  # Pinterest ì¶”ê°€
            'youtube.com', 'youtu.be',
            'vimeo.com',
            'streamable.com',
            'gfycat.com',
            'giphy.com',
            'media.discordapp.net',
            'cdn.discordapp.com'
        }
        
        return any(domain.endswith(md) for md in media_domains)
        
    except Exception:
        return False

def cleanup_old_downloads(max_age_hours: int = 4) -> int:
    """ì˜¤ë˜ëœ ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì •ë¦¬"""
    if not TEMP_DOWNLOAD_DIR.exists():
        return 0
    
    cleaned_count = 0
    cutoff_time = datetime.now().timestamp() - (max_age_hours * 3600)
    
    try:
        for file_path in TEMP_DOWNLOAD_DIR.iterdir():
            if file_path.is_file():
                try:
                    if file_path.stat().st_mtime < cutoff_time:
                        file_path.unlink()
                        cleaned_count += 1
                        logger.debug(f"ğŸ—‘ï¸ Cleaned: {file_path.name}")
                except Exception as e:
                    logger.warning(f"âš ï¸ File cleanup failed: {file_path.name} - {e}")
                    
    except Exception as e:
        logger.error(f"âŒ Directory cleanup error: {e}")
    
    if cleaned_count > 0:
        logger.info(f"ğŸ§¹ Cleaned up {cleaned_count} old files")
    
    return cleaned_count

# ==================== ë¯¸ë””ì–´ URL ì¶”ì¶œ í•¨ìˆ˜ ====================

def extract_media_from_posts(posts: List[Dict]) -> List[Dict]:
    """
    í¬ë¡¤ë§ëœ ê²Œì‹œë¬¼ì—ì„œ ë¯¸ë””ì–´ URLë“¤ë§Œ ì¶”ì¶œ
    ì‚¬ì´íŠ¸ íƒ€ì… ë¬´ê´€í•˜ê²Œ ë™ì‘
    """
    media_list = []
    
    for i, post in enumerate(posts):
        try:
            # ê° ê²Œì‹œë¬¼ì—ì„œ ë¯¸ë””ì–´ URL í•„ë“œë“¤ í™•ì¸
            media_fields = [
                'ì¸ë„¤ì¼ URL',
                'ì´ë¯¸ì§€ URL', 
                'thumbnail_url',
                'image_url',
                'media_url',
                'attachment_url'
            ]
            
            for field in media_fields:
                url = post.get(field, '')
                if url and url.startswith('http') and is_valid_media_url(url):
                    # íŒŒì¼ëª… ìƒì„±
                    try:
                        filename = Path(urlparse(url).path).name
                        if not filename or '.' not in filename:
                            extension = get_file_extension_from_url(url)
                            filename = f"media_{i}_{abs(hash(url)) % 1000}{extension}"
                        filename = sanitize_filename(filename)
                    except:
                        filename = f"media_{i}_{abs(hash(url)) % 1000}.jpg"
                    
                    media_info = {
                        'type': get_media_type(get_file_extension_from_url(url)),
                        'original_url': url,
                        'thumbnail_url': url,  # ì¸ë„¤ì¼ê³¼ ì›ë³¸ì´ ê°™ì„ ìˆ˜ ìˆìŒ
                        'filename': filename,
                        'post_title': post.get('ì›ì œëª©') or post.get('title') or post.get('ì œëª©', 'Untitled'),
                        'post_link': post.get('ë§í¬') or post.get('link') or post.get('url', ''),
                        'extracted_at': datetime.now().isoformat(),
                        'source_field': field
                    }
                    
                    # ì¤‘ë³µ ì²´í¬ (ê°™ì€ URLì´ ì—¬ëŸ¬ í•„ë“œì— ìˆì„ ìˆ˜ ìˆìŒ)
                    if not any(m['original_url'] == url for m in media_list):
                        media_list.append(media_info)
            
        except Exception as e:
            logger.warning(f"âš ï¸ Post {i} media extraction failed: {e}")
    
    logger.info(f"ğŸ“¦ Extracted {len(media_list)} media URLs from {len(posts)} posts")
    return media_list

# ==================== ë©”ì¸ ë¯¸ë””ì–´ ë‹¤ìš´ë¡œë“œ ë§¤ë‹ˆì € ====================

class MediaDownloadManager:
    def __init__(self, user_lang: str = "en"):
        self.user_lang = user_lang
        self.session = None
        self.download_semaphore = asyncio.Semaphore(MEDIA_CONFIG['max_concurrent_downloads'])
        self.downloaded_size = 0
        self.max_total_size = MEDIA_CONFIG['max_total_size_mb'] * 1024 * 1024
    
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        connector = aiohttp.TCPConnector(
            limit=MEDIA_CONFIG['max_concurrent_downloads'] * 2,
            limit_per_host=MEDIA_CONFIG['max_concurrent_downloads'],
            ttl_dns_cache=300,
            use_dns_cache=True
        )
        
        timeout = aiohttp.ClientTimeout(
            total=MEDIA_CONFIG['download_timeout'],
            connect=10,
            sock_read=30
        )
        
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={'User-Agent': MEDIA_CONFIG['user_agent']}
        )
        
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.session:
            await self.session.close()
    
    async def download_media_from_posts(
        self, 
        posts: List[Dict], 
        site_type: str = None,  # ì´ì œ ì‚¬ìš© ì•ˆí•¨
        progress_callback: Optional[Callable] = None
    ) -> Dict[str, Any]:
        """ê²Œì‹œë¬¼ë“¤ì—ì„œ ë¯¸ë””ì–´ ì¶”ì¶œ ë° ë‹¤ìš´ë¡œë“œ - ì‚¬ì´íŠ¸ íƒ€ì… ë¬´ê´€"""
        
        if not posts:
            return {
                'success': False,
                'error': 'No posts provided',
                'downloaded_files': 0,
                'failed_files': 0
            }
        
        # 1. ëª¨ë“  ê²Œì‹œë¬¼ì—ì„œ ë¯¸ë””ì–´ URL ì¶”ì¶œ (ì‚¬ì´íŠ¸ íƒ€ì… ë¬´ê´€)
        logger.info(f"ğŸ” Extracting media from {len(posts)} posts")
        
        media_list = extract_media_from_posts(posts)
        
        if not media_list:
            return {
                'success': False,
                'error': 'No media URLs found in posts',
                'downloaded_files': 0,
                'failed_files': 0
            }
        
        logger.info(f"ğŸ“¦ Found {len(media_list)} media URLs")
        
        # 2. ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        download_dir = TEMP_DOWNLOAD_DIR / f"media_{timestamp}"
        download_dir.mkdir(exist_ok=True)
        
        # 3. ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ
        logger.info(f"â¬‡ï¸ Starting download of {len(media_list)} files")
        download_tasks = []
        
        for media_info in media_list:
            task = self.download_single_file(media_info, download_dir)
            download_tasks.append(task)
        
        # ë³‘ë ¬ ì‹¤í–‰
        download_results = await asyncio.gather(*download_tasks, return_exceptions=True)
        
        # 4. ê²°ê³¼ ì§‘ê³„
        successful_files = []
        failed_count = 0
        
        for i, result in enumerate(download_results):
            if progress_callback:
                progress = int((i + 1) / len(download_results) * 80)  # 80%ê¹Œì§€ ë‹¤ìš´ë¡œë“œ
                await progress_callback(progress, len(download_results))
            
            if isinstance(result, Exception):
                logger.error(f"âŒ Download task error: {result}")
                failed_count += 1
            elif result is not None:
                successful_files.append(result)
            else:
                failed_count += 1
        
        if not successful_files:
            return {
                'success': False,
                'error': 'All file downloads failed',
                'downloaded_files': 0,
                'failed_files': failed_count
            }
        
        # 5. ZIP íŒŒì¼ ìƒì„±
        if progress_callback:
            await progress_callback(90, len(successful_files))
        
        zip_path = await self.create_zip_file(successful_files, download_dir, timestamp)
        
        if progress_callback:
            await progress_callback(100, len(successful_files))
        
        # 6. ì„ì‹œ íŒŒì¼ë“¤ ì •ë¦¬
        try:
            for file_path in successful_files:
                if file_path.exists():
                    file_path.unlink()
            
            # ë¹ˆ ë””ë ‰í† ë¦¬ ì œê±°
            if download_dir.exists() and not any(download_dir.iterdir()):
                download_dir.rmdir()
                
        except Exception as e:
            logger.warning(f"âš ï¸ Temporary file cleanup failed: {e}")
        
        # 7. ê²°ê³¼ ë°˜í™˜
        zip_size_mb = zip_path.stat().st_size / (1024 * 1024) if zip_path.exists() else 0
        
        logger.info(f"âœ… Media download complete: {len(successful_files)} successful, {failed_count} failed")
        logger.info(f"ğŸ“ ZIP file created: {zip_path.name} ({zip_size_mb:.2f}MB)")
        
        return {
            'success': True,
            'download_url': f'/api/download-file/{zip_path.name}',
            'zip_filename': zip_path.name,
            'downloaded_files': len(successful_files),
            'failed_files': failed_count,
            'zip_size_mb': round(zip_size_mb, 2),
            'total_media_found': len(media_list),
            'unique_media_count': len(media_list)
        }
    
    async def download_single_file(self, media_info: Dict, download_dir: Path) -> Optional[Path]:
        """ë‹¨ì¼ ë¯¸ë””ì–´ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        url = media_info['original_url']
        filename = media_info['filename']
        
        async with self.download_semaphore:
            for attempt in range(MEDIA_CONFIG['retry_attempts']):
                try:
                    # HEAD ìš”ì²­ìœ¼ë¡œ íŒŒì¼ í¬ê¸° í™•ì¸
                    async with self.session.head(url) as response:
                        if response.status != 200:
                            logger.warning(f"âš ï¸ File access failed: {url} (status: {response.status})")
                            return None
                        
                        content_length = response.headers.get('content-length')
                        if content_length:
                            file_size = int(content_length)
                            file_size_mb = file_size / (1024 * 1024)
                            
                            # ê°œë³„ íŒŒì¼ í¬ê¸° ì œí•œ
                            if file_size_mb > MEDIA_CONFIG['max_file_size_mb']:
                                logger.warning(f"âš ï¸ File size exceeded: {filename} ({file_size_mb:.1f}MB)")
                                return None
                            
                            # ì „ì²´ ë‹¤ìš´ë¡œë“œ í¬ê¸° ì œí•œ
                            if self.downloaded_size + file_size > self.max_total_size:
                                logger.warning(f"âš ï¸ Total download size limit reached: {filename}")
                                return None
                    
                    # ì‹¤ì œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                    async with self.session.get(url) as response:
                        if response.status == 200:
                            file_path = download_dir / filename
                            
                            # íŒŒì¼ëª… ì¤‘ë³µ ì²˜ë¦¬
                            counter = 1
                            original_path = file_path
                            while file_path.exists():
                                name_parts = original_path.stem, counter, original_path.suffix
                                file_path = original_path.parent / f"{name_parts[0]}_{name_parts[1]}{name_parts[2]}"
                                counter += 1
                            
                            async with aiofiles.open(file_path, 'wb') as f:
                                async for chunk in response.content.iter_chunked(8192):
                                    await f.write(chunk)
                            
                            # ë‹¤ìš´ë¡œë“œëœ í¬ê¸° ì¶”ê°€
                            actual_size = file_path.stat().st_size
                            self.downloaded_size += actual_size
                            
                            logger.debug(f"âœ… Download completed: {file_path.name}")
                            return file_path
                        else:
                            logger.warning(f"âš ï¸ Download failed: {url} (status: {response.status})")
                            
                except asyncio.TimeoutError:
                    logger.warning(f"âš ï¸ Download timeout: {url} (attempt {attempt + 1}/{MEDIA_CONFIG['retry_attempts']})")
                except Exception as e:
                    logger.warning(f"âš ï¸ Download error: {url} - {e} (attempt {attempt + 1}/{MEDIA_CONFIG['retry_attempts']})")
                
                if attempt < MEDIA_CONFIG['retry_attempts'] - 1:
                    await asyncio.sleep(1)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
        
        return None
    
    async def create_zip_file(
        self, 
        file_paths: List[Path], 
        download_dir: Path, 
        timestamp: str
    ) -> Path:
        """ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ë“¤ì„ ZIPìœ¼ë¡œ ì••ì¶•"""
        zip_filename = f"pickpost_media_{timestamp}.zip"
        zip_path = TEMP_DOWNLOAD_DIR / zip_filename
        
        def create_zip():
            try:
                with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED, compresslevel=6) as zipf:
                    for file_path in file_paths:
                        if file_path.exists():
                            # ZIP ë‚´ë¶€ì—ì„œì˜ íŒŒì¼ëª…
                            zipf.write(file_path, file_path.name)
            except Exception as e:
                logger.error(f"âŒ ZIP creation error: {e}")
                raise
        
        # CPU ì§‘ì•½ì ì¸ ZIP ìƒì„±ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(None, create_zip)
        
        logger.info(f"ğŸ“ ZIP compression complete: {zip_path.name} ({len(file_paths)} files)")
        return zip_path
    
    def get_manager_info(self) -> Dict[str, Any]:
        """ë§¤ë‹ˆì € ì •ë³´ ë°˜í™˜"""
        return {
            'config': MEDIA_CONFIG,
            'temp_dir': str(TEMP_DOWNLOAD_DIR),
            'language': self.user_lang,
            'supported_extensions': {
                'images': list(SUPPORTED_IMAGE_EXTENSIONS),
                'videos': list(SUPPORTED_VIDEO_EXTENSIONS),
                'audio': list(SUPPORTED_AUDIO_EXTENSIONS)
            }
        }

# ==================== ì •ë¦¬ í•¨ìˆ˜ ====================

def cleanup_old_downloads_with_logging(max_age_hours: int = 4) -> int:
    """ë¡œê¹…ê³¼ í•¨ê»˜ ì˜¤ë˜ëœ ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì •ë¦¬"""
    try:
        cleaned_count = cleanup_old_downloads(max_age_hours)
        return cleaned_count
    except Exception as e:
        logger.error(f"âŒ Error during file cleanup: {e}")
        return 0

# ==================== ëª¨ë“ˆ ì´ˆê¸°í™” ====================

# ëª¨ë“ˆ ë¡œë“œ ì‹œ ì •ë¦¬ ì‘ì—… ìˆ˜í–‰ - 4ì‹œê°„ìœ¼ë¡œ ë³€ê²½
cleanup_old_downloads(max_age_hours=4)

logger.info("ğŸ“¦ Simple media download module loaded successfully")
logger.info(f"ğŸ“ Temp directory: {TEMP_DOWNLOAD_DIR}")
logger.info(f"âš™ï¸ Config: Max {MEDIA_CONFIG['max_file_size_mb']}MB/file, {MEDIA_CONFIG['max_concurrent_downloads']} concurrent downloads")
logger.info("ğŸ”„ Direct URL extraction from crawled posts")
logger.info("ğŸ§¹ Temp file cleanup: Auto-delete after 4 hours")

# ì „ì—­ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (main.pyì—ì„œ ì‚¬ìš©)
_global_media_manager = None

def get_media_download_manager(user_lang: str = "en") -> MediaDownloadManager:
    """ì „ì—­ ë¯¸ë””ì–´ ë‹¤ìš´ë¡œë“œ ë§¤ë‹ˆì € ë°˜í™˜ (ì–¸ì–´ ì„¤ì • í¬í•¨)"""
    global _global_media_manager
    if _global_media_manager is None:
        _global_media_manager = MediaDownloadManager(user_lang=user_lang)
        logger.info("ğŸ¯ Media extraction: Direct from post fields")
    else:
        # ì–¸ì–´ ì„¤ì • ì—…ë°ì´íŠ¸
        _global_media_manager.user_lang = user_lang
    return _global_media_manager