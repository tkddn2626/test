# auto_crawler.py ê°œì„  - Universal í¬ë¡¤ëŸ¬ì— ì´ë¯¸ì§€/ì˜ìƒ ì¶”ì¶œ ê¸°ëŠ¥ ì¶”ê°€

import requests
import re
import json
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple, Any
import asyncio
import logging
from urllib.parse import urlparse, urljoin, quote, unquote
import time
from dataclasses import dataclass
import hashlib
from pathlib import Path
import os

# ğŸ”¥ ì–¸ì–´íŒ© ì‹œìŠ¤í…œ import ì¶”ê°€
from core.messages import create_localized_message

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# ================================
# ğŸ”¥ Universal ë¯¸ë””ì–´ ì¶”ì¶œ ì„¤ì •
# ================================

# ì§€ì›í•˜ëŠ” ë¯¸ë””ì–´ íŒŒì¼ í™•ì¥ì
SUPPORTED_IMAGE_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp', '.svg', '.tiff', '.avif'}
SUPPORTED_VIDEO_EXTENSIONS = {'.mp4', '.webm', '.mov', '.avi', '.mkv', '.flv', '.wmv', '.m4v', '.ogv'}
SUPPORTED_AUDIO_EXTENSIONS = {'.mp3', '.wav', '.ogg', '.m4a', '.flac', '.aac', '.wma'}

# ë¯¸ë””ì–´ í˜¸ìŠ¤íŒ… ë„ë©”ì¸ë“¤
MEDIA_HOSTING_DOMAINS = {
    # ì´ë¯¸ì§€ í˜¸ìŠ¤íŒ…
    'imgur.com', 'i.imgur.com', 'gyazo.com', 'i.gyazo.com',
    'prnt.sc', 'postimg.cc', 'i.postimg.cc', 'imgbb.com', 'i.imgbb.com',
    'flickr.com', 'farm1.staticflickr.com', 'farm2.staticflickr.com',
    'photobucket.com', 'tinypic.com', 'imageshack.us',
    
    # ì†Œì…œ ë¯¸ë””ì–´ ì´ë¯¸ì§€
    'pbs.twimg.com', 'scontent.cdninstagram.com', 'i.redd.it',
    'external-preview.redd.it', 'preview.redd.it',
    
    # ë¹„ë””ì˜¤ í˜¸ìŠ¤íŒ…
    'youtube.com', 'youtu.be', 'vimeo.com', 'dailymotion.com',
    'v.redd.it', 'gfycat.com', 'streamable.com', 'streamja.com',
    
    # CDN ë° ì¼ë°˜ í˜¸ìŠ¤íŒ…
    'cdn.discordapp.com', 'media.discordapp.net', 'i.4cdn.org',
    'files.catbox.moe', 'cdn.jsdelivr.net', 'raw.githubusercontent.com'
}

# ì´ë¯¸ì§€/ì˜ìƒì„ ìì£¼ í¬í•¨í•˜ëŠ” HTML ì…€ë ‰í„°ë“¤
MEDIA_SELECTORS = [
    'img[src]',                          # ì§ì ‘ ì´ë¯¸ì§€
    'video[src]', 'video source[src]',   # ë¹„ë””ì˜¤
    'audio[src]', 'audio source[src]',   # ì˜¤ë””ì˜¤
    'a[href*=".jpg"]', 'a[href*=".jpeg"]', 'a[href*=".png"]',  # ì´ë¯¸ì§€ ë§í¬
    'a[href*=".gif"]', 'a[href*=".webp"]', 'a[href*=".mp4"]',
    'a[href*=".webm"]', 'a[href*=".mov"]',
    '[data-src]',                        # ì§€ì—° ë¡œë”© ì´ë¯¸ì§€
    '[style*="background-image"]',       # CSS ë°°ê²½ ì´ë¯¸ì§€
    '.image img', '.photo img', '.picture img',  # í´ë˜ìŠ¤ ê¸°ë°˜
    '.gallery img', '.media img', '.attachment img',
    '[class*="image"] img', '[class*="photo"] img',
    '[class*="picture"] img', '[class*="media"] img'
]

class UniversalMediaExtractor:
    """Universal ì‚¬ì´íŠ¸ì—ì„œ ì´ë¯¸ì§€/ì˜ìƒ ë§í¬ë¥¼ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def extract_media_from_post(self, post: Dict) -> List[Dict]:
        """
        ê²Œì‹œë¬¼ì—ì„œ ë¯¸ë””ì–´ ì •ë³´ ì¶”ì¶œ (4chan ìŠ¤íƒ€ì¼)
        
        Args:
            post: í¬ë¡¤ë§ëœ ê²Œì‹œë¬¼ ì •ë³´
            
        Returns:
            ë¯¸ë””ì–´ ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{'type': 'image/video/audio', 'original_url': '', 'thumbnail_url': '', 'filename': ''}]
        """
        media_list = []
        
        try:
            # 1. ê²Œì‹œë¬¼ì—ì„œ ì´ë¯¸ ì¶”ì¶œëœ ë¯¸ë””ì–´ URL í™•ì¸
            existing_media = self._extract_existing_media_urls(post)
            media_list.extend(existing_media)
            
            # 2. ê²Œì‹œë¬¼ ë§í¬ì—ì„œ ì¶”ê°€ ë¯¸ë””ì–´ ì¶”ì¶œ
            post_url = post.get('ë§í¬', '') or post.get('ì›ë¬¸URL', '')
            if post_url:
                page_media = self._extract_media_from_page(post_url)
                media_list.extend(page_media)
            
            # 3. ë³¸ë¬¸ì—ì„œ ë¯¸ë””ì–´ URL íŒ¨í„´ ë§¤ì¹­
            content = post.get('ë³¸ë¬¸', '')
            if content:
                content_media = self._extract_media_from_content(content)
                media_list.extend(content_media)
            
            # 4. ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
            unique_media = self._deduplicate_media(media_list)
            
            logger.debug(f"ğŸ“¸ ì¶”ì¶œëœ ë¯¸ë””ì–´: {len(unique_media)}ê°œ (ê²Œì‹œë¬¼: {post.get('ì›ì œëª©', 'Unknown')[:50]}...)")
            
            return unique_media
            
        except Exception as e:
            logger.error(f"âŒ ë¯¸ë””ì–´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def _extract_existing_media_urls(self, post: Dict) -> List[Dict]:
        """ê²Œì‹œë¬¼ì—ì„œ ì´ë¯¸ ì¶”ì¶œëœ ë¯¸ë””ì–´ URLë“¤ ì²˜ë¦¬"""
        media_list = []
        
        # ì¸ë„¤ì¼ URL (ê¸°ì¡´)
        thumbnail_url = post.get('ì¸ë„¤ì¼ URL', '')
        if thumbnail_url and thumbnail_url.startswith('http'):
            media_info = self._create_media_info(thumbnail_url, 'thumbnail')
            if media_info:
                media_list.append(media_info)
        
        # ì´ë¯¸ì§€ URL (4chan ìŠ¤íƒ€ì¼)
        image_url = post.get('ì´ë¯¸ì§€ URL', '')
        if image_url and image_url.startswith('http'):
            media_info = self._create_media_info(image_url, 'original')
            if media_info:
                media_list.append(media_info)
        
        return media_list
    
    def _extract_media_from_page(self, url: str) -> List[Dict]:
        """ì›¹í˜ì´ì§€ì—ì„œ ë¯¸ë””ì–´ ì¶”ì¶œ (HTML íŒŒì‹±)"""
        media_list = []
        
        try:
            response = self.session.get(url, timeout=10)
            if response.status_code != 200:
                return media_list
            
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ê° ë¯¸ë””ì–´ ì…€ë ‰í„°ë¡œ ìš”ì†Œ ì°¾ê¸°
            for selector in MEDIA_SELECTORS:
                try:
                    elements = soup.select(selector)
                    for element in elements:
                        media_info = self._extract_media_from_element(element, url)
                        if media_info:
                            media_list.append(media_info)
                except Exception as e:
                    logger.debug(f"ì…€ë ‰í„° {selector} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            # Open Graph ì´ë¯¸ì§€ ì¶”ì¶œ
            og_media = self._extract_opengraph_media(soup, url)
            media_list.extend(og_media)
            
            # JSON-LD ë¯¸ë””ì–´ ì¶”ì¶œ
            jsonld_media = self._extract_jsonld_media(soup, url)
            media_list.extend(jsonld_media)
            
        except Exception as e:
            logger.debug(f"í˜ì´ì§€ ë¯¸ë””ì–´ ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
        
        return media_list
    
    def _extract_media_from_element(self, element, base_url: str) -> Optional[Dict]:
        """HTML ìš”ì†Œì—ì„œ ë¯¸ë””ì–´ ì •ë³´ ì¶”ì¶œ"""
        try:
            media_url = None
            
            # src ì†ì„±
            if element.has_attr('src'):
                media_url = element['src']
            
            # href ì†ì„± (ë§í¬)
            elif element.has_attr('href'):
                media_url = element['href']
            
            # data-src ì†ì„± (ì§€ì—° ë¡œë”©)
            elif element.has_attr('data-src'):
                media_url = element['data-src']
            
            # style ì†ì„±ì—ì„œ background-image ì¶”ì¶œ
            elif element.has_attr('style'):
                style = element['style']
                bg_match = re.search(r'background-image:\s*url\(["\']?([^"\')]+)["\']?\)', style)
                if bg_match:
                    media_url = bg_match.group(1)
            
            if media_url:
                # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                absolute_url = urljoin(base_url, media_url)
                return self._create_media_info(absolute_url, 'page_extracted')
            
        except Exception as e:
            logger.debug(f"ìš”ì†Œ ë¯¸ë””ì–´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return None
    
    def _extract_media_from_content(self, content: str) -> List[Dict]:
        """í…ìŠ¤íŠ¸ ë³¸ë¬¸ì—ì„œ ë¯¸ë””ì–´ URL íŒ¨í„´ ë§¤ì¹­"""
        media_list = []
        
        # URL íŒ¨í„´ë“¤
        url_patterns = [
            # ì§ì ‘ ë¯¸ë””ì–´ íŒŒì¼ URL
            r'https?://[^\s]+\.(?:jpg|jpeg|png|gif|webp|bmp|svg|mp4|webm|mov|avi|mp3|wav|ogg)\b',
            
            # ë¯¸ë””ì–´ í˜¸ìŠ¤íŒ… ë„ë©”ì¸
            r'https?://(?:i\.)?imgur\.com/[^\s]+',
            r'https?://gyazo\.com/[^\s]+',
            r'https?://[^\s]*\.(?:twimg|redd|imgur)\.com/[^\s]+',
            
            # ì¼ë°˜ì ì¸ ì´ë¯¸ì§€ URL íŒ¨í„´
            r'https?://[^\s]+/[^\s]*(?:image|img|photo|picture|media)[^\s]*\.[a-z]{2,4}',
        ]
        
        for pattern in url_patterns:
            matches = re.finditer(pattern, content, re.IGNORECASE)
            for match in matches:
                url = match.group(0)
                media_info = self._create_media_info(url, 'content_extracted')
                if media_info:
                    media_list.append(media_info)
        
        return media_list
    
    def _extract_opengraph_media(self, soup, base_url: str) -> List[Dict]:
        """Open Graph ë©”íƒ€íƒœê·¸ì—ì„œ ë¯¸ë””ì–´ ì¶”ì¶œ"""
        media_list = []
        
        # Open Graph ì´ë¯¸ì§€
        og_image = soup.find('meta', property='og:image')
        if og_image and og_image.get('content'):
            url = urljoin(base_url, og_image['content'])
            media_info = self._create_media_info(url, 'og_image')
            if media_info:
                media_list.append(media_info)
        
        # Open Graph ë¹„ë””ì˜¤
        og_video = soup.find('meta', property='og:video')
        if og_video and og_video.get('content'):
            url = urljoin(base_url, og_video['content'])
            media_info = self._create_media_info(url, 'og_video')
            if media_info:
                media_list.append(media_info)
        
        # Twitter Card ì´ë¯¸ì§€
        twitter_image = soup.find('meta', attrs={'name': 'twitter:image'})
        if twitter_image and twitter_image.get('content'):
            url = urljoin(base_url, twitter_image['content'])
            media_info = self._create_media_info(url, 'twitter_image')
            if media_info:
                media_list.append(media_info)
        
        return media_list
    
    def _extract_jsonld_media(self, soup, base_url: str) -> List[Dict]:
        """JSON-LD êµ¬ì¡°í™” ë°ì´í„°ì—ì„œ ë¯¸ë””ì–´ ì¶”ì¶œ"""
        media_list = []
        
        jsonld_scripts = soup.find_all('script', type='application/ld+json')
        for script in jsonld_scripts:
            try:
                data = json.loads(script.string)
                
                # ì´ë¯¸ì§€ ì¶”ì¶œ
                images = []
                if isinstance(data, dict):
                    if 'image' in data:
                        if isinstance(data['image'], list):
                            images.extend(data['image'])
                        else:
                            images.append(data['image'])
                    
                    # ì¤‘ì²©ëœ êµ¬ì¡°ì—ì„œë„ ì´ë¯¸ì§€ ì°¾ê¸°
                    if 'mainEntity' in data and isinstance(data['mainEntity'], dict):
                        if 'image' in data['mainEntity']:
                            images.append(data['mainEntity']['image'])
                
                for img in images:
                    img_url = img if isinstance(img, str) else img.get('url', '') if isinstance(img, dict) else ''
                    if img_url:
                        url = urljoin(base_url, img_url)
                        media_info = self._create_media_info(url, 'jsonld')
                        if media_info:
                            media_list.append(media_info)
                            
            except (json.JSONDecodeError, AttributeError):
                continue
        
        return media_list
    
    def _create_media_info(self, url: str, source_type: str) -> Optional[Dict]:
        """URLì—ì„œ ë¯¸ë””ì–´ ì •ë³´ ê°ì²´ ìƒì„± (4chan ìŠ¤íƒ€ì¼)"""
        try:
            if not url or not url.startswith('http'):
                return None
            
            # URL ì •ë¦¬
            clean_url = url.strip().split('?')[0]  # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°
            
            # íŒŒì¼ í™•ì¥ì ì¶”ì¶œ
            parsed = urlparse(clean_url)
            path = unquote(parsed.path)
            extension = Path(path).suffix.lower()
            
            # ë¯¸ë””ì–´ íƒ€ì… ê²°ì •
            media_type = self._get_media_type(extension, clean_url)
            if media_type == 'unknown':
                # í™•ì¥ìê°€ ì—†ì–´ë„ ë¯¸ë””ì–´ í˜¸ìŠ¤íŒ… ë„ë©”ì¸ì´ë©´ í—ˆìš©
                if not any(domain in parsed.netloc.lower() for domain in MEDIA_HOSTING_DOMAINS):
                    return None
                media_type = 'image'  # ê¸°ë³¸ê°’ìœ¼ë¡œ ì´ë¯¸ì§€ ì·¨ê¸‰
            
            # íŒŒì¼ëª… ìƒì„±
            filename = self._generate_filename(url, extension, media_type)
            
            # ì¸ë„¤ì¼ URL ìƒì„± (ì›ë³¸ê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •, 4chan ìŠ¤íƒ€ì¼)
            thumbnail_url = self._generate_thumbnail_url(clean_url, media_type)
            
            return {
                'type': media_type,
                'original_url': clean_url,
                'thumbnail_url': thumbnail_url,
                'filename': filename,
                'source': source_type,
                'domain': parsed.netloc,
                'extension': extension
            }
            
        except Exception as e:
            logger.debug(f"ë¯¸ë””ì–´ ì •ë³´ ìƒì„± ì‹¤íŒ¨ ({url}): {e}")
            return None
    
    def _get_media_type(self, extension: str, url: str) -> str:
        """í™•ì¥ìì™€ URLë¡œ ë¯¸ë””ì–´ íƒ€ì… ê²°ì •"""
        if extension in SUPPORTED_IMAGE_EXTENSIONS:
            return 'image'
        elif extension in SUPPORTED_VIDEO_EXTENSIONS:
            return 'video'
        elif extension in SUPPORTED_AUDIO_EXTENSIONS:
            return 'audio'
        
        # í™•ì¥ìê°€ ì—†ëŠ” ê²½ìš° URL íŒ¨í„´ìœ¼ë¡œ ì¶”ì •
        url_lower = url.lower()
        if any(keyword in url_lower for keyword in ['image', 'img', 'photo', 'picture', 'thumbnail']):
            return 'image'
        elif any(keyword in url_lower for keyword in ['video', 'vid', 'movie', 'film']):
            return 'video'
        elif any(keyword in url_lower for keyword in ['audio', 'sound', 'music', 'song']):
            return 'audio'
        
        return 'unknown'
    
    def _generate_filename(self, url: str, extension: str, media_type: str) -> str:
        """ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±"""
        try:
            parsed = urlparse(url)
            path = unquote(parsed.path)
            
            # ê¸°ì¡´ íŒŒì¼ëª…ì´ ìˆìœ¼ë©´ ì‚¬ìš©
            if path and path != '/':
                filename = Path(path).name
                if filename and len(filename) > 1:
                    return self._sanitize_filename(filename)
            
            # URL í•´ì‹œë¥¼ ì´ìš©í•œ ê³ ìœ  íŒŒì¼ëª… ìƒì„±
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            
            # í™•ì¥ì ê²°ì •
            if not extension:
                if media_type == 'image':
                    extension = '.jpg'
                elif media_type == 'video':
                    extension = '.mp4'
                elif media_type == 'audio':
                    extension = '.mp3'
                else:
                    extension = '.bin'
            
            return f"{media_type}_{url_hash}{extension}"
            
        except Exception:
            return f"media_{int(time.time())}.bin"
    
    def _generate_thumbnail_url(self, original_url: str, media_type: str) -> str:
        """ì¸ë„¤ì¼ URL ìƒì„± (4chan ìŠ¤íƒ€ì¼ - ì›ë³¸ê³¼ ë™ì¼)"""
        # 4chan ìŠ¤íƒ€ì¼: ì¸ë„¤ì¼ URLì„ ì›ë³¸ ì´ë¯¸ì§€ì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
        return original_url
    
    def _sanitize_filename(self, filename: str) -> str:
        """íŒŒì¼ëª…ì—ì„œ ìœ„í—˜í•œ ë¬¸ì ì œê±°"""
        # ìœ„í—˜í•œ ë¬¸ìë“¤ ì œê±°
        unsafe_chars = r'[<>:"/\\|?*\x00-\x1f]'
        safe_filename = re.sub(unsafe_chars, '_', filename)
        
        # ê¸¸ì´ ì œí•œ
        if len(safe_filename) > 100:
            name, ext = os.path.splitext(safe_filename)
            safe_filename = name[:95] + ext
        
        return safe_filename
    
    def _deduplicate_media(self, media_list: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ ë¯¸ë””ì–´ ì œê±°"""
        seen_urls = set()
        unique_media = []
        
        for media in media_list:
            url = media.get('original_url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_media.append(media)
        
        return unique_media

# ================================
# ğŸ”¥ AutoCrawler í´ë˜ìŠ¤ í™•ì¥
# ================================

class AutoCrawler:
    """ğŸ”¥ í†µí•© ìë™ í¬ë¡¤ëŸ¬ (ë¯¸ë””ì–´ ì¶”ì¶œ ê¸°ëŠ¥ ê°•í™”)"""
    
    def __init__(self):
        # ê¸°ì¡´ ì´ˆê¸°í™” ì½”ë“œ...
        self.site_detector = None
        self.unified_crawler = None
        self._crawlers_cache = {}
        self._initialization_attempted = False
        
        # ğŸ”¥ Universal ë¯¸ë””ì–´ ì¶”ì¶œê¸° ì¶”ê°€
        self.universal_media_extractor = UniversalMediaExtractor()
        
        # ğŸ”¥ ì§€ì›í•˜ëŠ” ì‚¬ì´íŠ¸ ëª©ë¡ì— 'x' ì¶”ê°€
        self.supported_sites = [
            'reddit', 'lemmy', 'dcinside', 'blind', 'bbc', 'x', '4chan','universal'
        ]
        
        # ì‚¬ì´íŠ¸ë³„ ë§¤ê°œë³€ìˆ˜ ë§¤í•‘ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
        self.site_param_mapping = {
            'reddit': {
                'target_param': 'subreddit_name',
                'module': 'crawlers.reddit',
                'function': 'fetch_posts',
                'supported_params': [
                    'limit', 'sort', 'time_filter', 'websocket',
                    'min_views', 'min_likes', 'start_date', 'end_date',
                    'enforce_date_limit', 'start_index', 'end_index'
                ],
                'unsupported_params': ['min_comments']
            },
            'lemmy': {
                'target_param': 'community_input',
                'module': 'crawlers.lemmy',
                'function': 'crawl_lemmy_board',
                'supported_params': [
                    'limit', 'sort', 'min_views', 'min_likes',
                    'time_filter', 'start_date', 'end_date', 'websocket',
                    'enforce_date_limit', 'start_index', 'end_index'
                ],
                'unsupported_params': ['min_comments']
            },
            'dcinside': {
                'target_param': 'board_name',
                'module': 'crawlers.dcinside',
                'function': 'crawl_dcinside_board',
                'supported_params': [
                    'limit', 'sort', 'min_views', 'min_likes', 'min_comments',
                    'time_filter', 'start_date', 'end_date', 'websocket',
                    'enforce_date_limit', 'start_index', 'end_index'
                ],
                'unsupported_params': []
            },
            'blind': {
                'target_param': 'board_name',
                'module': 'crawlers.blind',
                'function': 'crawl_blind_board',
                'supported_params': [
                    'limit', 'sort', 'min_views', 'min_likes', 'min_comments',
                    'time_filter', 'start_date', 'end_date', 'websocket',
                    'enforce_date_limit', 'start_index', 'end_index'
                ],
                'unsupported_params': []
            },
            'bbc': {
                'target_param': 'board_name',
                'module': 'crawlers.bbc',
                'function': 'crawl_bbc_board',
                'supported_params': [
                    'limit', 'sort', 'min_views', 'min_likes',
                    'time_filter', 'start_date', 'end_date', 'websocket',
                    'enforce_date_limit', 'start_index', 'end_index'
                ],
                'unsupported_params': ['min_comments']
            },
            'x': {
                'target_param': 'board_name',
                'module': 'crawlers.x',
                'function': 'crawl_x_board',
                'supported_params': [
                    'limit', 'sort', 'min_views', 'min_likes', 'min_comments',
                    'time_filter', 'start_date', 'end_date', 'websocket',
                    'enforce_date_limit', 'start_index', 'end_index'
                ],
                'unsupported_params': []
            },
            '4chan': {
                'target_param': 'board_input',
                'module': 'crawlers.4chan',
                'function': 'crawl_4chan_board',
                'supported_params': [
                    'limit', 'sort', 'min_views', 'min_likes', 'min_comments',
                    'time_filter', 'start_date', 'end_date', 'websocket',
                    'enforce_date_limit', 'start_index', 'end_index'
                ],
                'unsupported_params': []
            },
            'universal': {
                'target_param': 'input_data',
                'module': 'core.auto_crawler',
                'function': 'crawl',
                'supported_params': [
                    'limit', 'sort', 'min_views', 'min_likes', 'min_comments',
                    'time_filter', 'start_date', 'end_date', 'websocket',
                    'enforce_date_limit', 'start_index', 'end_index',
                    'include_media', 'include_images', 'include_videos'  # ğŸ”¥ ë¯¸ë””ì–´ ê´€ë ¨ ë§¤ê°œë³€ìˆ˜ ì¶”ê°€
                ],
                'unsupported_params': []
            }
        }

    async def crawl(self, input_data: str, **config) -> List[Dict]:
        """
        í†µí•© í¬ë¡¤ë§ ë©”ì¸ ë©”ì„œë“œ
        
        Args:
            input_data: í¬ë¡¤ë§ ëŒ€ìƒ (ê²Œì‹œíŒëª…, URL)
            **config: í¬ë¡¤ë§ ì„¤ì • (force_site_type í¬í•¨ ê°€ëŠ¥)
        
        Returns:
            í¬ë¡¤ë§ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        start_time = datetime.now()
        
        try:
            # ì˜ì¡´ì„± ì´ˆê¸°í™”
            self._initialize_dependencies()
            
            # 1. ì‚¬ì´íŠ¸ ê°ì§€ - force_site_typeì´ ìˆìœ¼ë©´ ì‚¬ìš© (ì¬ê°ì§€ ë°©ì§€)
            force_applied = False
            if 'force_site_type' in config:
                site_type = config.pop('force_site_type')
                force_applied = True
                logger.info(f"ğŸ¯ ê°•ì œ ì§€ì •ëœ ì‚¬ì´íŠ¸: {site_type}")
            else:
                site_type = await self._detect_site_type(input_data)
                logger.info(f"ğŸ” ìë™ ê°ì§€ëœ ì‚¬ì´íŠ¸: {site_type}")
            
            # 2. ê²Œì‹œíŒ ì‹ë³„ì ì¶”ì¶œ
            board_identifier = self._extract_board_identifier(input_data, site_type)
            logger.info(f"ğŸ“‹ ê²Œì‹œíŒ ì‹ë³„ì: {board_identifier}")
            
            # 3. í¬ë¡¤ë§ ì„¤ì • ì¤€ë¹„
            crawl_config = self._prepare_crawl_config(site_type, board_identifier, **config)
            
            # 4. í¬ë¡¤ë§ ì‹¤í–‰
            try:
                logger.info(f"ğŸš€ AutoCrawler í¬ë¡¤ë§ ì‹¤í–‰: {site_type}")
                results = await self._execute_crawl(site_type, **crawl_config)
            except Exception as e:
                if force_applied:
                    logger.error(f"âŒ force_site_type={site_type} í¬ë¡¤ë§ ì‹¤íŒ¨, í´ë°± ê¸ˆì§€")
                    raise e
                
                # ê¸°ì¡´ ë¡œì§: ìë™ ê°ì§€ëœ ê²½ìš°ì—ë§Œ í´ë°± í—ˆìš©
                logger.warning(f"AutoCrawler ì‹¤íŒ¨, í†µí•© í¬ë¡¤ëŸ¬ë¡œ í´ë°±: {e}")
                if self.unified_crawler:
                    logger.info(f"ğŸš€ í†µí•© í¬ë¡¤ë§ í´ë°± ì‹¤í–‰: {site_type}")
                    results = await self.unified_crawler.unified_crawl(
                        site_type, 
                        board_identifier, 
                        **crawl_config
                    )
                else:
                    raise e
            
            # 5. ê²°ê³¼ í›„ì²˜ë¦¬
            processed_results = self._post_process_results(results, site_type, config)
            
            elapsed = (datetime.now() - start_time).total_seconds()
            logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(processed_results)}ê°œ ê²°ê³¼ ({elapsed:.2f}ì´ˆ)")
            
            return processed_results
                
        except Exception as e:
            elapsed = (datetime.now() - start_time).total_seconds()
            logger.error(f"âŒ AutoCrawler ì˜¤ë¥˜ ({elapsed:.2f}ì´ˆ): {e}")
            raise

    def _initialize_dependencies(self):
        """ì˜ì¡´ì„±ë“¤ì„ ì§€ì—° ë¡œë“œ (í•œ ë²ˆë§Œ ì‹œë„)"""
        if self._initialization_attempted:
            return
        
        self._initialization_attempted = True
        
        try:
            # SiteDetector ë¡œë“œ
            from .site_detector import SiteDetector
            self.site_detector = SiteDetector()
            logger.info("âœ… SiteDetector ë¡œë“œ ì„±ê³µ")
        except ImportError as e:
            logger.warning(f"âš ï¸ SiteDetector ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.site_detector = None
        
        try:
            # UnifiedCrawler ë¡œë“œ (ì„ íƒì )
            from .unified_crawler import unified_crawler
            self.unified_crawler = unified_crawler
            logger.info("âœ… UnifiedCrawler ë¡œë“œ ì„±ê³µ")
        except ImportError as e:
            logger.warning(f"âš ï¸ UnifiedCrawler ë¡œë“œ ì‹¤íŒ¨ (í´ë°± ì‚¬ìš©): {e}")
            self.unified_crawler = None

    def _is_dynamic_site(self, url: str) -> bool:
        """ë™ì  ì‚¬ì´íŠ¸ ê°ì§€ (JavaScript ê¸°ë°˜)"""
        dynamic_patterns = [
            'imginn.com', 'picuki.com', 'instagram.com',
            'tiktok.com', 'twitter.com', 'x.com',
            'youtube.com', 'facebook.com', 'pinterest.com'
        ]
        return any(pattern in url.lower() for pattern in dynamic_patterns)
    
    async def _try_dynamic_crawling(self, url: str) -> List[Dict]:
        """ë™ì  í¬ë¡¤ë§ ì‹œë„ (Playwright ìš°ì„ , Selenium í´ë°±)"""
        
        # 1. Playwright ì‹œë„
        try:
            from playwright.async_api import async_playwright
            
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                
                await page.goto(url, wait_until='domcontentloaded')
                await page.wait_for_timeout(5000)  # 5ì´ˆ ëŒ€ê¸°
                
                # ì´ë¯¸ì§€ ì¶”ì¶œ
                images = await page.query_selector_all('img[src]')
                results = []
                
                for i, img in enumerate(images[:20]):  # ìµœëŒ€ 20ê°œ
                    src = await img.get_attribute('src')
                    alt = await img.get_attribute('alt') or f'Image {i+1}'
                    
                    if src and src.startswith('http'):
                        results.append({
                            'ë²ˆí˜¸': str(i + 1),
                            'ì›ì œëª©': alt,
                            'ë²ˆì—­ì œëª©': '',
                            'ë§í¬': url,
                            'ì›ë¬¸URL': url,
                            'ì´ë¯¸ì§€ URL': src,
                            'ì¸ë„¤ì¼ URL': src,
                            'ë³¸ë¬¸': alt,
                            'ì¡°íšŒìˆ˜': 0, 'ì¶”ì²œìˆ˜': 0, 'ëŒ“ê¸€ìˆ˜': 0,
                            'ì‘ì„±ì¼': '', 'ì‘ì„±ì': '',
                            'ì‚¬ì´íŠ¸': urlparse(url).netloc,
                            'ë¯¸ë””ì–´íƒ€ì…': 'image',
                            'ë¯¸ë””ì–´ê°œìˆ˜': 1,
                            'í¬ë¡¤ë§ë°©ì‹': 'AutoCrawler-Dynamic-Playwright',
                            'í”Œë«í¼': 'universal'
                        })
                
                await browser.close()
                logger.info(f"âœ… Playwright ë™ì  í¬ë¡¤ë§ ì„±ê³µ: {len(results)}ê°œ")
                return results
                
        except ImportError:
            logger.debug("Playwright ì—†ìŒ, Selenium ì‹œë„")
        except Exception as e:
            logger.debug(f"Playwright ì‹¤íŒ¨: {e}")
        
        # 2. Selenium ì‹œë„
        try:
            from selenium import webdriver
            from selenium.webdriver.chrome.options import Options
            from selenium.webdriver.common.by import By
            import time
            
            options = Options()
            options.add_argument('--headless')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            
            driver = webdriver.Chrome(options=options)
            driver.get(url)
            time.sleep(5)  # 5ì´ˆ ëŒ€ê¸°
            
            images = driver.find_elements(By.TAG_NAME, "img")
            results = []
            
            for i, img in enumerate(images[:20]):
                src = img.get_attribute('src')
                alt = img.get_attribute('alt') or f'Image {i+1}'
                
                if src and src.startswith('http'):
                    results.append({
                        'ë²ˆí˜¸': str(i + 1),
                        'ì›ì œëª©': alt,
                        'ë²ˆì—­ì œëª©': '',
                        'ë§í¬': url,
                        'ì›ë¬¸URL': url,
                        'ì´ë¯¸ì§€ URL': src,
                        'ì¸ë„¤ì¼ URL': src,
                        'ë³¸ë¬¸': alt,
                        'ì¡°íšŒìˆ˜': 0, 'ì¶”ì²œìˆ˜': 0, 'ëŒ“ê¸€ìˆ˜': 0,
                        'ì‘ì„±ì¼': '', 'ì‘ì„±ì': '',
                        'ì‚¬ì´íŠ¸': urlparse(url).netloc,
                        'ë¯¸ë””ì–´íƒ€ì…': 'image',
                        'ë¯¸ë””ì–´ê°œìˆ˜': 1,
                        'í¬ë¡¤ë§ë°©ì‹': 'AutoCrawler-Dynamic-Selenium',
                        'í”Œë«í¼': 'universal'
                    })
            
            driver.quit()
            logger.info(f"âœ… Selenium ë™ì  í¬ë¡¤ë§ ì„±ê³µ: {len(results)}ê°œ")
            return results
            
        except ImportError:
            logger.debug("Selenium ì—†ìŒ")
        except Exception as e:
            logger.debug(f"Selenium ì‹¤íŒ¨: {e}")
        
        return []
    
    async def _detect_site_type(self, input_data: str) -> str:
        """ì‚¬ì´íŠ¸ íƒ€ì… ê°ì§€"""
        if self.site_detector:
            try:
                return await self.site_detector.detect_site_type(input_data)
            except Exception as e:
                logger.warning(f"SiteDetector ì˜¤ë¥˜, í´ë°± ì‚¬ìš©: {e}")
        
        # í´ë°± ì‚¬ì´íŠ¸ ê°ì§€
        return self._fallback_site_detection(input_data)
    
    def _extract_board_identifier(self, input_data: str, site_type: str) -> str:
        """ê²Œì‹œíŒ ì‹ë³„ì ì¶”ì¶œ"""
        if self.site_detector:
            try:
                return self.site_detector.extract_board_identifier(input_data, site_type)
            except Exception as e:
                logger.warning(f"ì‹ë³„ì ì¶”ì¶œ ì˜¤ë¥˜, ì›ë³¸ ì‚¬ìš©: {e}")
        
        # í´ë°±: ê°„ë‹¨í•œ ì‹ë³„ì ì¶”ì¶œ
        return self._fallback_extract_identifier(input_data, site_type)
    
    def _fallback_site_detection(self, input_data: str) -> str:
        """í´ë°± ì‚¬ì´íŠ¸ ê°ì§€"""
        input_lower = input_data.lower()
        
        # URL ê¸°ë°˜ ê°ì§€
        if input_data.startswith('http'):
            parsed = urlparse(input_data)
            domain = parsed.netloc.lower()
            
            if 'reddit.com' in domain:
                return 'reddit'
            elif 'x.com' in domain or 'twitter.com' in domain:
                return 'x'
            elif any(chan_domain in domain for chan_domain in ['4chan.org', '4channel.org', 'boards.4chan']):
                return '4chan'
            elif 'dcinside.com' in domain:
                return 'dcinside'
            elif 'teamblind.com' in domain or 'blind.com' in domain:
                return 'blind'
            elif 'bbc.com' in domain or 'bbc.co.uk' in domain:
                return 'bbc'
            elif any(lemmy_domain in domain for lemmy_domain in ['lemmy.', 'beehaw.', 'sh.itjust.works']):
                return 'lemmy'
            else:
                logger.info(f"ğŸŒ Universal ì‚¬ì´íŠ¸ë¡œ ê°ì§€: {input_data}")
                return 'universal'
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì§€
        if any(word in input_lower for word in ['reddit', 'subreddit']):
            return 'reddit'
        elif any(word in input_lower for word in ['lemmy', 'ë ˆë¯¸']):
            return 'lemmy'
        elif any(word in input_lower for word in ['dcinside', 'dc', 'ë””ì‹œ', 'ê°¤ëŸ¬ë¦¬']):
            return 'dcinside'
        elif any(word in input_lower for word in ['blind', 'ë¸”ë¼ì¸ë“œ']):
            return 'blind'
        elif any(word in input_lower for word in ['bbc', 'british']):
            return 'bbc'
        elif any(word in input_lower for word in ['x.com', 'twitter.com', 'tweet', '@']) or input_lower == 'x':
            return 'x'
        elif any(word in input_lower for word in ['4chan', '4channel', 'imageboard', '/g/', '/v/', '/a/', '/pol/']):
            return '4chan'
        else:
            logger.info(f"ğŸŒ í‚¤ì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨, Universalë¡œ ì²˜ë¦¬: {input_data}")
            return 'universal'
    
    def _fallback_extract_identifier(self, input_data: str, site_type: str) -> str:
        """í´ë°± ì‹ë³„ì ì¶”ì¶œ"""
        if site_type == 'reddit' and '/r/' in input_data:
            import re
            match = re.search(r'/r/([^/]+)', input_data)
            return match.group(1) if match else input_data
        elif site_type == 'lemmy' and '/c/' in input_data:
            parts = input_data.split('/c/')
            if len(parts) > 1:
                from urllib.parse import urlparse
                try:
                    domain = urlparse(input_data).netloc
                    community = parts[1].split('/')[0]
                    return f"{community}@{domain}"
                except:
                    pass
        elif site_type == 'dcinside' and '?id=' in input_data:
            import re
            match = re.search(r'[?&]id=([^&]+)', input_data)
            return match.group(1) if match else input_data
        
        elif site_type == '4chan':
            import re
            # https://boards.4chan.org/a/ â†’ a
            # https://boards.4chan.org/g/thread/12345 â†’ g
            match = re.search(r'(?:4chan\.org|4channel\.org)/([a-z0-9]+)', input_data)
            if match:
                return match.group(1)  # ê²Œì‹œíŒëª… (a, g, v ë“±)
            else:
                return input_data  # ê²Œì‹œíŒëª…ë§Œ ì…ë ¥ëœ ê²½ìš°
        
        return input_data
    
    def _prepare_crawl_config(self, site_type: str, board_identifier: str, **config) -> Dict[str, Any]:
        """ì‚¬ì´íŠ¸ë³„ í¬ë¡¤ë§ ì„¤ì • ì¤€ë¹„"""
        if site_type not in self.site_param_mapping:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸: {site_type}")
        
        site_config = self.site_param_mapping[site_type]
        
        # ê¸°ë³¸ ì„¤ì •
        crawl_config = {
            site_config['target_param']: board_identifier
        }
        
        # ì§€ì›í•˜ëŠ” ë§¤ê°œë³€ìˆ˜ë§Œ ì¶”ê°€
        for param in site_config['supported_params']:
            if param in config and config[param] is not None:
                crawl_config[param] = config[param]
        
        # ê³µí†µ ë§¤ê°œë³€ìˆ˜ ë§¤í•‘
        common_mappings = {
            'start': 'start_index',
            'end': 'end_index',
            'board': site_config['target_param'],
            'input': site_config['target_param'],
            'board_identifier': site_config['target_param']
        }
        
        for source, target in common_mappings.items():
            if source in config and target in site_config['supported_params']:
                crawl_config[target] = config[source]
        
        # ì‚¬ì´íŠ¸ë³„ íŠ¹ìˆ˜ ì²˜ë¦¬
        crawl_config = self._apply_site_specific_processing(site_type, crawl_config, **config)
        
        # ì§€ì›í•˜ì§€ ì•ŠëŠ” ë§¤ê°œë³€ìˆ˜ ì œê±° ë° ê²½ê³ 
        unsupported = site_config['unsupported_params']
        for param in unsupported:
            if param in crawl_config:
                removed_value = crawl_config.pop(param)
                logger.warning(f"âš ï¸ {site_type}ì—ì„œ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë§¤ê°œë³€ìˆ˜ ì œê±°: {param}={removed_value}")
        
        # None ê°’ ì œê±°
        crawl_config = {k: v for k, v in crawl_config.items() if v is not None}
        
        logger.debug(f"í¬ë¡¤ë§ ì„¤ì • ì¤€ë¹„ ì™„ë£Œ ({site_type}): {list(crawl_config.keys())}")
        return crawl_config
    
    def _apply_site_specific_processing(self, site_type: str, config: Dict, **original_config) -> Dict:
        """ì‚¬ì´íŠ¸ë³„ íŠ¹ìˆ˜ ì²˜ë¦¬"""
        
        if site_type == 'reddit':
            # Reddit ì •ë ¬ ë°©ì‹ ë§¤í•‘
            sort_mapping = {
                "popular": "hot", "recommend": "top", "recent": "new",
                "comments": "top"
            }
            if 'sort' in config and config['sort'] in sort_mapping:
                config['sort'] = sort_mapping[config['sort']]
                
            # Redditì€ subreddit ì´ë¦„ì—ì„œ /r/ ì œê±°
            if 'subreddit_name' in config:
                subreddit = config['subreddit_name']
                if subreddit.startswith('/r/'):
                    config['subreddit_name'] = subreddit[3:]
                elif subreddit.startswith('r/'):
                    config['subreddit_name'] = subreddit[2:]
        
        elif site_type == 'lemmy':
            # Lemmy ì»¤ë®¤ë‹ˆí‹° í˜•ì‹ ì²˜ë¦¬
            if 'community_input' in config:
                community = config['community_input']
                if '@' not in community and not community.startswith('http'):
                    # ì¸ìŠ¤í„´ìŠ¤ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì¶”ê°€
                    config['community_input'] = f"{community}@lemmy.world"
        
        elif site_type == 'bbc':
            # BBCëŠ” board_nameì´ ë¹ˆ ë¬¸ìì—´ë¡œ í•„ìš”í•  ìˆ˜ ìˆìŒ
            if 'board_name' not in config:
                config['board_name'] = ""
        
        elif site_type == 'universal':
            # Universalë„ board_name ì²˜ë¦¬
            if 'board_name' not in config:
                config['board_name'] = ""
        
        return config
    
    async def _execute_crawl(self, site_type: str, **config) -> List[Dict]:
        """í¬ë¡¤ë§ ì‹¤í–‰"""
        if site_type == 'universal':
            # Universal í¬ë¡¤ë§ì€ AutoCrawler ë‚´ë¶€ì—ì„œ ì§ì ‘ ì²˜ë¦¬
            return await self._crawl_universal_internal(**config)
        else:
            # ë‹¤ë¥¸ ì‚¬ì´íŠ¸ëŠ” ì§ì ‘ í¬ë¡¤ëŸ¬ í˜¸ì¶œ
            return await self._direct_crawl(site_type, **config)
    
    async def _crawl_universal_internal(self, **config) -> List[Dict]:
        """Universal í¬ë¡¤ë§ ë‚´ë¶€ êµ¬í˜„ (ë™ì  ì‚¬ì´íŠ¸ ì§€ì› ì¶”ê°€)"""
        board_url = config.get('input_data', '') or config.get('board_url', '')
        include_media = config.get('include_media', True)
        
        if not board_url:
            logger.warning("Universal í¬ë¡¤ë§: URLì´ ì œê³µë˜ì§€ ì•ŠìŒ")
            return []
        
        # URL ì •ê·œí™”
        if not board_url.startswith('http'):
            board_url = 'https://' + board_url
        
        logger.info(f"ğŸŒ Universal í¬ë¡¤ë§ ì‹œì‘: {board_url}")
        
        # ğŸ”¥ ë™ì  ì‚¬ì´íŠ¸ ê°ì§€ ë° ì²˜ë¦¬
        if self._is_dynamic_site(board_url):
            logger.info(f"ğŸ¯ ë™ì  ì‚¬ì´íŠ¸ ê°ì§€: {urlparse(board_url).netloc}")
            
            # ë™ì  í¬ë¡¤ë§ ì‹œë„
            try:
                dynamic_results = await self._try_dynamic_crawling(board_url)
                if dynamic_results:
                    return dynamic_results
                else:
                    logger.warning("âš ï¸ ë™ì  í¬ë¡¤ë§ ì‹¤íŒ¨, ì •ì  ë°©ì‹ìœ¼ë¡œ í´ë°±")
            except Exception as e:
                logger.warning(f"âš ï¸ ë™ì  í¬ë¡¤ë§ ì˜¤ë¥˜: {e}, ì •ì  ë°©ì‹ìœ¼ë¡œ í´ë°±")
        
        try:
            # ê¸°ì¡´ ì •ì  í¬ë¡¤ë§ ë¡œì§
            import requests
            from bs4 import BeautifulSoup
            import re
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            logger.info(f"ğŸ“¡ ì •ì  í¬ë¡¤ë§ ìš”ì²­: {board_url}")
            
            response = requests.get(board_url, headers=headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì¼ë°˜ì ì¸ ë§í¬ íŒ¨í„´ ì°¾ê¸°
            results = []
            
            # ë‹¤ì–‘í•œ ì…€ë ‰í„°ë¡œ ë§í¬ ì°¾ê¸°
            selectors = [
                'a[href]',  # ëª¨ë“  ë§í¬
                'h1 a, h2 a, h3 a, h4 a',  # ì œëª© ë§í¬
                '.title a, .headline a, .article-title a',  # í´ë˜ìŠ¤ ê¸°ë°˜
                '[class*="title"] a, [class*="headline"] a'  # ë¶€ë¶„ í´ë˜ìŠ¤ ë§¤ì¹­
            ]
            
            all_links = []
            for selector in selectors:
                links = soup.select(selector)
                all_links.extend(links)
            
            # ì¤‘ë³µ ì œê±° (href ê¸°ì¤€)
            seen_hrefs = set()
            unique_links = []
            for link in all_links:
                href = link.get('href')
                if href and href not in seen_hrefs:
                    seen_hrefs.add(href)
                    unique_links.append(link)
            
            logger.info(f"ğŸ”— ë°œê²¬ëœ ê³ ìœ  ë§í¬: {len(unique_links)}ê°œ")
            
            # ì œëª©ì´ ë  ìˆ˜ ìˆëŠ” ìš”ì†Œë“¤ ì²˜ë¦¬
            for i, link in enumerate(unique_links[:config.get('limit', 20)]):
                href = link.get('href')
                if not href:
                    continue
                    
                # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                full_url = urljoin(board_url, href)
                
                # ë§í¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                title = link.get_text(strip=True)
                
                # ì œëª©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ì˜ë¯¸ì—†ëŠ” ê²½ìš° ìŠ¤í‚µ
                if not title or len(title) < 5:
                    continue
                
                # ê³µí†µì ìœ¼ë¡œ ì œì™¸í•  í…ìŠ¤íŠ¸ë“¤
                skip_patterns = ['more', 'read more', 'ë”ë³´ê¸°', 'click here', 'í´ë¦­', 'home', 'menu']
                if any(pattern in title.lower() for pattern in skip_patterns):
                    continue
                
                # ğŸ”¥ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ ë° ë¯¸ë””ì–´ ì¶”ì¶œ
                result = {
                    'ë²ˆí˜¸': str(i + 1),
                    'ì›ì œëª©': title,
                    'ë²ˆì—­ì œëª©': '',
                    'ë§í¬': full_url,
                    'ì›ë¬¸URL': full_url,
                    'ë³¸ë¬¸': '',
                    'ì¡°íšŒìˆ˜': 0,
                    'ì¶”ì²œìˆ˜': 0,
                    'ëŒ“ê¸€ìˆ˜': 0,
                    'ì‘ì„±ì¼': '',
                    'ì‘ì„±ì': '',
                    'ì‚¬ì´íŠ¸': urlparse(board_url).netloc,
                    'í¬ë¡¤ë§ë°©ì‹': 'AutoCrawler-Universal-Enhanced',
                    'í”Œë«í¼': 'universal'
                }
                
                # ğŸ”¥ ë¯¸ë””ì–´ ì¶”ì¶œ ë° ì¶”ê°€ (4chan ìŠ¤íƒ€ì¼)
                if include_media:
                    try:
                        media_list = self.universal_media_extractor.extract_media_from_post(result)
                        if media_list:
                            # ì²« ë²ˆì§¸ ë¯¸ë””ì–´ë¥¼ ëŒ€í‘œ ì´ë¯¸ì§€ë¡œ ì„¤ì •
                            first_media = media_list[0]
                            result['ì´ë¯¸ì§€ URL'] = first_media['original_url']
                            result['ì¸ë„¤ì¼ URL'] = first_media['thumbnail_url']
                            result['íŒŒì¼ëª…'] = first_media['filename']
                            result['ë¯¸ë””ì–´íƒ€ì…'] = first_media['type']
                            result['ë¯¸ë””ì–´ê°œìˆ˜'] = len(media_list)
                            
                            logger.debug(f"ğŸ“¸ ë¯¸ë””ì–´ ì¶”ì¶œ ì™„ë£Œ: {len(media_list)}ê°œ ({title[:30]}...)")
                        else:
                            # ë¯¸ë””ì–´ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’
                            result['ì´ë¯¸ì§€ URL'] = ""
                            result['ì¸ë„¤ì¼ URL'] = ""
                            result['íŒŒì¼ëª…'] = ""
                            result['ë¯¸ë””ì–´íƒ€ì…'] = ""
                            result['ë¯¸ë””ì–´ê°œìˆ˜'] = 0
                    except Exception as e:
                        logger.warning(f"âš ï¸ ë¯¸ë””ì–´ ì¶”ì¶œ ì‹¤íŒ¨ ({title[:30]}...): {e}")
                        result['ì´ë¯¸ì§€ URL'] = ""
                        result['ì¸ë„¤ì¼ URL'] = ""
                        result['ë¯¸ë””ì–´ê°œìˆ˜'] = 0
                
                results.append(result)
            
            logger.info(f"âœ… Universal í¬ë¡¤ë§ ì™„ë£Œ: {len(results)}ê°œ ë§í¬")
            
            if include_media:
                media_count = sum(1 for r in results if r.get('ë¯¸ë””ì–´ê°œìˆ˜', 0) > 0)
                logger.info(f"ğŸ–¼ï¸ ë¯¸ë””ì–´ í¬í•¨ ê²Œì‹œë¬¼: {media_count}/{len(results)}ê°œ")
            
            if not results:
                logger.warning(f"âš ï¸ {board_url}ì—ì„œ ìœ íš¨í•œ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            return results
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ Universal í¬ë¡¤ë§ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
            return []
        except Exception as e:
            logger.error(f"âŒ Universal í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return []

    async def _direct_crawl(self, site_type: str, **config) -> List[Dict]:
        """ì§ì ‘ í¬ë¡¤ëŸ¬ í˜¸ì¶œ (í´ë°±)"""
        site_config = self.site_param_mapping[site_type]
        module_name = site_config['module']
        function_name = site_config['function']
        
        if not module_name:
            raise ValueError(f"ëª¨ë“ˆì´ ì§€ì •ë˜ì§€ ì•Šì€ ì‚¬ì´íŠ¸: {site_type}")
        
        try:
            # í¬ë¡¤ëŸ¬ ëª¨ë“ˆì„ ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒˆë¡œ ë¡œë“œ
            if module_name not in self._crawlers_cache:
                crawler_module = __import__(module_name, fromlist=[function_name])
                crawler_function = getattr(crawler_module, function_name)
                self._crawlers_cache[module_name] = crawler_function
                logger.debug(f"í¬ë¡¤ëŸ¬ ëª¨ë“ˆ ë¡œë“œ: {module_name}.{function_name}")
            
            crawler_function = self._crawlers_cache[module_name]
            
            # í¬ë¡¤ë§ ì‹¤í–‰
            if asyncio.iscoroutinefunction(crawler_function):
                result = await crawler_function(**config)
            else:
                result = crawler_function(**config)
            
            return result or []
                
        except ImportError as e:
            logger.error(f"í¬ë¡¤ëŸ¬ ëª¨ë“ˆ import ì‹¤íŒ¨ ({site_type}): {e}")
            return []
        except Exception as e:
            logger.error(f"ì§ì ‘ í¬ë¡¤ë§ ì˜¤ë¥˜ ({site_type}): {e}")
            raise
    
    def _post_process_results(self, results: List[Dict], site_type: str, config: Dict) -> List[Dict]:
        """ê²°ê³¼ í›„ì²˜ë¦¬"""
        if not results:
            return []
        
        processed_results = []
        
        for result in results:
            # ê¸°ë³¸ í•„ë“œ ì •ê·œí™”
            normalized_result = self._normalize_result_fields(result, site_type)
            
            # ì¶”ê°€ í›„ì²˜ë¦¬
            if config.get('translate', False):
                # ë²ˆì—­ ê´€ë ¨ ì²˜ë¦¬ëŠ” ì—¬ê¸°ì„œ ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ (ìƒìœ„ ë ˆë²¨ì—ì„œ ì²˜ë¦¬)
                pass
            
            processed_results.append(normalized_result)
        
        # ì •ë ¬ ë° í•„í„°ë§
        processed_results = self._apply_final_filters(processed_results, config)
        
        return processed_results
    
    def _normalize_result_fields(self, result: Dict, site_type: str) -> Dict:
        """ê²°ê³¼ í•„ë“œ ì •ê·œí™”"""
        # ì´ë¯¸ ì •ê·œí™”ëœ ê²°ê³¼ë¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if all(key in result for key in ['ì›ì œëª©', 'ë§í¬', 'ì‘ì„±ì¼']):
            return result
        
        # ì‚¬ì´íŠ¸ë³„ í•„ë“œ ë§¤í•‘ (í•„ìš”í•œ ê²½ìš°)
        normalized = result.copy()
        
        # ê¸°ë³¸ í•„ë“œë“¤ì´ ì—†ëŠ” ê²½ìš° ë¹ˆ ê°’ìœ¼ë¡œ ì„¤ì •
        default_fields = {
            'ë²ˆí˜¸': '',
            'ì›ì œëª©': '',
            'ë²ˆì—­ì œëª©': '',
            'ë§í¬': '',
            'ë³¸ë¬¸': '',
            'ì¡°íšŒìˆ˜': 0,
            'ì¶”ì²œìˆ˜': 0,
            'ëŒ“ê¸€ìˆ˜': 0,
            'ì‘ì„±ì¼': ''
        }
        
        for field, default_value in default_fields.items():
            if field not in normalized:
                normalized[field] = default_value
        
        return normalized
    
    def _apply_final_filters(self, results: List[Dict], config: Dict) -> List[Dict]:
        """ìµœì¢… í•„í„°ë§ ë° ì •ë ¬"""
        filtered_results = results
        
        # ë²”ìœ„ í•„í„°ë§
        start_index = config.get('start_index', config.get('start', 1))
        end_index = config.get('end_index', config.get('end', len(results)))
        
        if start_index > 1 or end_index < len(results):
            # 1-based indexë¥¼ 0-basedë¡œ ë³€í™˜
            start_idx = max(0, start_index - 1)
            end_idx = min(len(results), end_index)
            filtered_results = filtered_results[start_idx:end_idx]
        
        return filtered_results

    def get_universal_media_extractor_info(self) -> Dict[str, Any]:
        """Universal ë¯¸ë””ì–´ ì¶”ì¶œê¸° ì •ë³´ ë°˜í™˜"""
        return {
            'supported_image_extensions': list(SUPPORTED_IMAGE_EXTENSIONS),
            'supported_video_extensions': list(SUPPORTED_VIDEO_EXTENSIONS),
            'supported_audio_extensions': list(SUPPORTED_AUDIO_EXTENSIONS),
            'media_hosting_domains': list(MEDIA_HOSTING_DOMAINS),
            'media_selectors': MEDIA_SELECTORS,
            'extraction_methods': [
                'existing_urls',  # ê²Œì‹œë¬¼ì— ì´ë¯¸ ìˆëŠ” URL
                'page_parsing',   # HTML í˜ì´ì§€ íŒŒì‹±
                'content_regex',  # ë³¸ë¬¸ ì •ê·œí‘œí˜„ì‹ ë§¤ì¹­
                'opengraph',      # Open Graph ë©”íƒ€íƒœê·¸
                'jsonld',         # JSON-LD êµ¬ì¡°í™” ë°ì´í„°
                'css_background'  # CSS ë°°ê²½ ì´ë¯¸ì§€
            ],
            'features': [
                '4chan_style_image_extraction',
                'automatic_thumbnail_generation', 
                'multiple_media_types_support',
                'deduplication',
                'safe_filename_generation'
            ]
        }

# ================================
# ğŸ”¥ Universal í¬ë¡¤ëŸ¬ë¥¼ ìœ„í•œ ë¯¸ë””ì–´ ì¶”ì¶œ í•¨ìˆ˜ (media_download.py í˜¸í™˜)
# ================================

def extract_universal_media(post: Dict) -> List[Dict]:
    """
    Universal í¬ë¡¤ëŸ¬ ê²Œì‹œë¬¼ì—ì„œ ë¯¸ë””ì–´ ì¶”ì¶œ
    media_download.pyì˜ ë™ì  ì¶”ì¶œê¸° ì‹œìŠ¤í…œê³¼ í˜¸í™˜ë˜ëŠ” í•¨ìˆ˜
    
    Args:
        post: í¬ë¡¤ë§ëœ ê²Œì‹œë¬¼ ì •ë³´
        
    Returns:
        ë¯¸ë””ì–´ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (media_download.py í˜•ì‹)
    """
    try:
        # UniversalMediaExtractor ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        extractor = UniversalMediaExtractor()
        
        # ë¯¸ë””ì–´ ì¶”ì¶œ
        media_list = extractor.extract_media_from_post(post)
        
        # media_download.py í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        converted_media = []
        for media in media_list:
            converted = {
                'type': media['type'],
                'original_url': media['original_url'],
                'thumbnail_url': media['thumbnail_url'],
                'filename': media['filename'],
                'source': f"universal_{media['source']}",
                'title': post.get('ì›ì œëª©', ''),
                'post_url': post.get('ë§í¬', ''),
                'domain': media.get('domain', ''),
                'extension': media.get('extension', ''),
                'post_number': post.get('ë²ˆí˜¸', ''),
                'site_type': 'universal'
            }
            converted_media.append(converted)
        
        logger.debug(f"ğŸ“¸ Universal ë¯¸ë””ì–´ ì¶”ì¶œ: {len(converted_media)}ê°œ")
        return converted_media
        
    except Exception as e:
        logger.error(f"âŒ Universal ë¯¸ë””ì–´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return []

# ================================
# ğŸ”¥ ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜ ì—…ë°ì´íŠ¸
# ================================

async def crawl_universal_with_media(input_data: str, limit: int = 50, sort: str = "recent",
                                   min_views: int = 0, min_likes: int = 0, min_comments: int = 0,
                                   start_date: str = None, end_date: str = None,
                                   include_media: bool = True, include_images: bool = True, 
                                   include_videos: bool = True, include_audio: bool = False,
                                   time_filter: str = None,
                                   websocket=None, start_index: int = 1, end_index: int = 20, 
                                   user_lang: str = "en") -> List[Dict]:
    """
    Universal í¬ë¡¤ë§ + ë¯¸ë””ì–´ ì¶”ì¶œ í†µí•© í•¨ìˆ˜
    
    Args:
        input_data: í¬ë¡¤ë§í•  URL
        include_media: ë¯¸ë””ì–´ ì¶”ì¶œ ì—¬ë¶€
        include_images: ì´ë¯¸ì§€ í¬í•¨ ì—¬ë¶€
        include_videos: ë¹„ë””ì˜¤ í¬í•¨ ì—¬ë¶€  
        include_audio: ì˜¤ë””ì˜¤ í¬í•¨ ì—¬ë¶€
        ... (ê¸°íƒ€ ë§¤ê°œë³€ìˆ˜ë“¤)
    
    Returns:
        ë¯¸ë””ì–´ ì •ë³´ê°€ í¬í•¨ëœ ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸
    """
    
    crawler = AutoCrawler()
    
    try:
        logger.info(f"ğŸŒ Universal ë¯¸ë””ì–´ í¬ë¡¤ë§ ì‹œì‘: {input_data}")
        
        if websocket:
            message = create_localized_message(
                progress=20,
                status_key="crawlingProgress.site_connecting",
                lang=user_lang,
                status_data={"site": "Universal"}
            )
            await websocket.send_json(message)
        
        # í¬ë¡¤ë§ ì„¤ì •
        config = {
            'input_data': input_data,
            'limit': max(end_index + 10, limit),
            'include_media': include_media,
            'include_images': include_images,
            'include_videos': include_videos,
            'include_audio': include_audio
        }
        
        # Universal í¬ë¡¤ë§ ì‹¤í–‰
        posts = await crawler._crawl_universal_internal(**config)
        
        if not posts:
            error_msg = f"""
Universal í¬ë¡¤ë§ì—ì„œ ê²Œì‹œë¬¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_data}

ê°€ëŠ¥í•œ ì›ì¸:
1. ì›¹ì‚¬ì´íŠ¸ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŒ
2. ë§í¬ë‚˜ ì½˜í…ì¸ ê°€ ì—†ëŠ” í˜ì´ì§€
3. ë™ì  ë¡œë”©ì´ë‚˜ JavaScriptê°€ í•„ìš”í•œ ì‚¬ì´íŠ¸
4. ë¡œë´‡ ì°¨ë‹¨ (robots.txt, Cloudflare ë“±)

ê¶Œì¥ì‚¬í•­:
â€¢ ì™„ì „í•œ URLì„ ì…ë ¥í•˜ì„¸ìš” (https:// í¬í•¨)
â€¢ ì •ì  ì½˜í…ì¸ ê°€ ìˆëŠ” ì‚¬ì´íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”
â€¢ í¬ëŸ¼, ë¸”ë¡œê·¸, ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ë“±ì´ ì í•©í•©ë‹ˆë‹¤
            """
            raise Exception(error_msg.strip())
        
        if websocket:
            message = create_localized_message(
                progress=80,
                status_key="crawlingProgress.media_processing",
                lang=user_lang
            )
            await websocket.send_json(message)
        
        # ë¯¸ë””ì–´ í•„í„°ë§
        if include_media:
            filtered_posts = []
            for post in posts:
                media_count = post.get('ë¯¸ë””ì–´ê°œìˆ˜', 0)
                media_type = post.get('ë¯¸ë””ì–´íƒ€ì…', '')
                
                # ë¯¸ë””ì–´ íƒ€ì…ë³„ í•„í„°ë§
                include_post = True
                if media_count > 0:
                    if media_type == 'image' and not include_images:
                        include_post = False
                    elif media_type == 'video' and not include_videos:
                        include_post = False
                    elif media_type == 'audio' and not include_audio:
                        include_post = False
                
                if include_post:
                    filtered_posts.append(post)
            
            posts = filtered_posts
        
        # ë²”ìœ„ ì ìš©
        if posts and len(posts) >= end_index:
            posts = posts[start_index-1:end_index]
            
            # ë²ˆí˜¸ ì¬ë¶€ì—¬
            for idx, post in enumerate(posts):
                post['ë²ˆí˜¸'] = start_index + idx
        
        logger.info(f"âœ… Universal ë¯¸ë””ì–´ í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ê²Œì‹œë¬¼")
        
        if include_media:
            media_posts = sum(1 for p in posts if p.get('ë¯¸ë””ì–´ê°œìˆ˜', 0) > 0)
            logger.info(f"ğŸ–¼ï¸ ë¯¸ë””ì–´ í¬í•¨ ê²Œì‹œë¬¼: {media_posts}/{len(posts)}ê°œ")
        
        return posts
        
    except Exception as e:
        logger.error(f"âŒ Universal ë¯¸ë””ì–´ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        raise

# ================================
# ğŸ”¥ ê¸°ì¡´ í˜¸í™˜ì„± í•¨ìˆ˜ë“¤ (í”„ë¡œì íŠ¸ íŒŒì¼ê³¼ í˜¸í™˜)
# ================================

async def validate_crawl_request(input_data: str, **config) -> Tuple[bool, List[str]]:
    """í¬ë¡¤ë§ ìš”ì²­ ìœ íš¨ì„± ê²€ì‚¬ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜)"""
    crawler = AutoCrawler()
    return await crawler.validate_crawl_request(input_data, **config)

def get_site_help_info(site_type: str) -> Dict[str, Any]:
    """ì‚¬ì´íŠ¸ë³„ ë„ì›€ë§ ì •ë³´ ë°˜í™˜ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜)"""
    crawler = AutoCrawler()
    return crawler.get_site_help_info(site_type)

def get_supported_sites() -> List[str]:
    """ì§€ì›í•˜ëŠ” ì‚¬ì´íŠ¸ ëª©ë¡ ë°˜í™˜ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜)"""
    return AutoCrawler().supported_sites

# ================================
# ğŸ”¥ ì¶”ê°€ëœ ìœ íš¨ì„± ê²€ì‚¬ ë©”ì„œë“œë“¤ (ëˆ„ë½ëœ ì½”ë“œ ë³µì›)
# ================================

async def validate_crawl_request(self, input_data: str, **config) -> Tuple[bool, List[str]]:
    """í¬ë¡¤ë§ ìš”ì²­ ìœ íš¨ì„± ê²€ì‚¬"""
    errors = []
    
    try:
        # 1. ì…ë ¥ ë°ì´í„° ê²€ì¦
        if not input_data or not input_data.strip():
            errors.append("í¬ë¡¤ë§ ëŒ€ìƒì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return False, errors
        
        # 2. ì‚¬ì´íŠ¸ ê°ì§€
        site_type = await self._detect_site_type(input_data)
        if site_type not in self.supported_sites:
            errors.append(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸ì…ë‹ˆë‹¤: {site_type}")
            return False, errors
        
        # 3. ë§¤ê°œë³€ìˆ˜ ê²€ì¦
        is_valid, param_errors = self._validate_parameters(site_type, **config)
        if not is_valid:
            errors.extend(param_errors)
        
        # 4. ë²”ìœ„ ê²€ì¦
        start_index = config.get('start_index', config.get('start', 1))
        end_index = config.get('end_index', config.get('end', 20))
        
        if start_index < 1:
            errors.append("ì‹œì‘ ì¸ë±ìŠ¤ëŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤")
        if end_index < start_index:
            errors.append("ë ì¸ë±ìŠ¤ëŠ” ì‹œì‘ ì¸ë±ìŠ¤ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤")
        if end_index - start_index > 100:
            errors.append("í•œ ë²ˆì— ìµœëŒ€ 100ê°œê¹Œì§€ë§Œ í¬ë¡¤ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        
        return len(errors) == 0, errors
        
    except Exception as e:
        errors.append(f"ìœ íš¨ì„± ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return False, errors

def _validate_parameters(self, site_type: str, **config) -> Tuple[bool, List[str]]:
    """ë§¤ê°œë³€ìˆ˜ ìœ íš¨ì„± ê²€ì‚¬"""
    errors = []
    
    if site_type not in self.site_param_mapping:
        errors.append(f"ë§¤ê°œë³€ìˆ˜ ì •ë³´ê°€ ì—†ëŠ” ì‚¬ì´íŠ¸: {site_type}")
        return False, errors
    
    site_config = self.site_param_mapping[site_type]
    unsupported_params = site_config['unsupported_params']
    
    # ì§€ì›í•˜ì§€ ì•ŠëŠ” ë§¤ê°œë³€ìˆ˜ ê²€ì‚¬
    for param in unsupported_params:
        if param in config and config[param] is not None:
            errors.append(f"{site_type}ì—ì„œ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë§¤ê°œë³€ìˆ˜: {param}")
    
    # ê°’ ë²”ìœ„ ê²€ì‚¬
    if 'limit' in config:
        limit = config['limit']
        if not isinstance(limit, int) or limit < 1 or limit > 200:
            errors.append("limitì€ 1-200 ì‚¬ì´ì˜ ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤")
    
    # ë‚ ì§œ í˜•ì‹ ê²€ì‚¬
    for date_param in ['start_date', 'end_date']:
        if date_param in config and config[date_param]:
            try:
                datetime.strptime(config[date_param], "%Y-%m-%d")
            except ValueError:
                errors.append(f"{date_param}ëŠ” YYYY-MM-DD í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤")
    
    return len(errors) == 0, errors

def get_site_help_info(self, site_type: str) -> Dict[str, Any]:
    """ì‚¬ì´íŠ¸ë³„ ë„ì›€ë§ ì •ë³´ ë°˜í™˜"""
    if site_type not in self.supported_sites:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸: {site_type}")
    
    site_config = self.site_param_mapping[site_type]
    
    help_info = {
        'reddit': {
            'format': 'subreddit_name or /r/subreddit_name or full URL',
            'examples': ['python', '/r/programming', 'https://reddit.com/r/askreddit'],
            'notes': 'Reddit ì„œë¸Œë ˆë”§ ì´ë¦„ ë˜ëŠ” URLì„ ì…ë ¥í•˜ì„¸ìš”'
        },
        'lemmy': {
            'format': 'community@instance or full URL',
            'examples': ['technology@lemmy.world', 'asklemmy@lemmy.ml', 'https://lemmy.world/c/technology'],
            'notes': 'ì¸ìŠ¤í„´ìŠ¤ê°€ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ @lemmy.worldê°€ ì¶”ê°€ë©ë‹ˆë‹¤'
        },
        'dcinside': {
            'format': 'gallery_name or gallery_id or full URL',
            'examples': ['programming', 'í”„ë¡œê·¸ë˜ë°', 'https://gall.dcinside.com/board/lists/?id=programming'],
            'notes': 'ê°¤ëŸ¬ë¦¬ ID ë˜ëŠ” ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”'
        },
        'blind': {
            'format': 'topic_name or full URL',
            'examples': ['íšŒì‚¬ìƒí™œ', 'ê°œë°œì', 'https://www.teamblind.com/kr/topics/íšŒì‚¬ìƒí™œ'],
            'notes': 'í† í”½ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”'
        },
        'bbc': {
            'format': 'section_url',
            'examples': ['https://www.bbc.com/news', 'https://www.bbc.com/technology'],
            'notes': 'BBC ì„¹ì…˜ URLì„ ì…ë ¥í•˜ì„¸ìš”'
        },
        'x': {
            'format': 'username or hashtag or full URL',
            'examples': ['@username', '#hashtag', 'https://x.com/username'],
            'notes': 'X(Twitter) ì‚¬ìš©ìëª…ì´ë‚˜ í•´ì‹œíƒœê·¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”'
        },
        '4chan': {
            'format': 'board_name or full URL',
            'examples': ['g', 'a', 'pol', 'https://boards.4chan.org/g/'],
            'notes': '4chan ê²Œì‹œíŒ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: g, a, v, pol)'
        },
        'universal': {
            'format': 'full_url',
            'examples': ['https://example.com/forum', 'https://news.site.com'],
            'notes': 'ì™„ì „í•œ URLì„ ì…ë ¥í•˜ì„¸ìš”. AutoCrawlerê°€ ìë™ìœ¼ë¡œ ë§í¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.'
        }
    }
    
    base_help = help_info.get(site_type, {
        'format': 'site_specific_input',
        'examples': ['example'],
        'notes': 'ì‚¬ì´íŠ¸ë³„ í˜•ì‹ì„ í™•ì¸í•˜ì„¸ìš”'
    })
    
    # ë§¤ê°œë³€ìˆ˜ ì •ë³´ ì¶”ê°€
    base_help.update({
        'target_parameter': site_config['target_param'],
        'supported_parameters': site_config['supported_params'],
        'unsupported_parameters': site_config['unsupported_params'],
        'module_info': {
            'module': site_config['module'] or 'AutoCrawler Internal',
            'function': site_config['function']
        }
    })
    
    return base_help

# ================================
# ğŸ”¥ ëª¨ë“ˆ ë©”íƒ€ë°ì´í„° (media_download.py ë™ì  íƒì§€ìš©)
# ================================

# Universal í¬ë¡¤ëŸ¬ ë¯¸ë””ì–´ ì¶”ì¶œê¸° ì •ë³´
UNIVERSAL_MEDIA_EXTRACTOR_INFO = {
    'name': 'Universal Media Extractor',
    'version': '1.0.0',
    'description': 'Universal ì‚¬ì´íŠ¸ ë¯¸ë””ì–´ ì¶”ì¶œê¸° (4chan ìŠ¤íƒ€ì¼)',
    'supported_sites': ['universal'],
    'supported_media_types': ['image', 'video', 'audio'],
    'extraction_function': extract_universal_media,
    'features': [
        'html_parsing',
        'opengraph_extraction', 
        'jsonld_extraction',
        'css_background_extraction',
        'content_regex_matching',
        'automatic_deduplication'
    ]
}

# media_download.pyê°€ ì´ í•¨ìˆ˜ë¥¼ ìë™ìœ¼ë¡œ ì°¾ì„ ìˆ˜ ìˆë„ë¡ í•¨ìˆ˜ëª… í‘œì¤€í™”
def get_media_from_post(post: Dict) -> List[Dict]:
    """media_download.py í‘œì¤€ ì¸í„°í˜ì´ìŠ¤"""
    return extract_universal_media(post)

# ================================
# ğŸ”¥ ë©”ì¸ ì‹¤í–‰ë¶€
# ================================

if __name__ == "__main__":
    import asyncio
    
    # ë¡œê¹… ë ˆë²¨ ì„¤ì •
    logging.getLogger().setLevel(logging.INFO)
    
    print("ğŸ”¥ Universal ë¯¸ë””ì–´ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("ğŸ“¸ 4chan ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€ ì¶”ì¶œ ê¸°ëŠ¥ í¬í•¨")
    print("ğŸŒ media_download.py í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤ ì œê³µ")
    print("-" * 60)
    
# ëª¨ë“ˆ ë¡œë“œ í™•ì¸
logger.info("ğŸ”¥ Universal ë¯¸ë””ì–´ í¬ë¡¤ëŸ¬ Enhanced v1.0 ë¡œë“œ ì™„ë£Œ")
logger.info(f"ğŸ“Š ì§€ì› ë¯¸ë””ì–´ íƒ€ì…: {len(SUPPORTED_IMAGE_EXTENSIONS | SUPPORTED_VIDEO_EXTENSIONS | SUPPORTED_AUDIO_EXTENSIONS)}ê°œ")
logger.info(f"ğŸ¯ ë¯¸ë””ì–´ í˜¸ìŠ¤íŒ… ë„ë©”ì¸: {len(MEDIA_HOSTING_DOMAINS)}ê°œ")
logger.info(f"ğŸ” HTML ì…€ë ‰í„°: {len(MEDIA_SELECTORS)}ê°œ")
logger.info("ğŸ–¼ï¸ 4chan ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€ ì¶”ì¶œ ë°©ì‹ ì ìš©")
logger.info("âš™ï¸ media_download.py ë™ì  ì¶”ì¶œê¸° ì‹œìŠ¤í…œ í˜¸í™˜")